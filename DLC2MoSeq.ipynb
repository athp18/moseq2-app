{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to DLC2MoSeq\n",
    "\n",
    "Model and analyze your DLC-tracked keypoint data using MoSeq's modeling and analysis pipeline.\n",
    "\n",
    "__Note: This is notebook is still in development and has not been fully tested on many use cases yet.__\n",
    "\n",
    "For now, the DLC2Moseq functions will remain separate from the MoSeq codebase until it has been debugged properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare DeepLabCut Projects\n",
    "\n",
    "Assuming that you have already acquired some data, run each project through the DeepLabCut pipeline such that each project trained model.\n",
    "\n",
    "Once your model is trained, use the `create_labeled_video()` DLC function to create an h5 file outlining the keypoint coordinates for each frame, as well as a video for your reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Sessions and Create Index File \n",
    "\n",
    "The index file will be used to designate and separate the experimental group datasets during the arhmm modeling.\n",
    "\n",
    "After performing data acquisition, store all of your session folders under a parent directory (shown below) to access them in this notebook. \n",
    "\n",
    "```\n",
    ".\n",
    "└── Base_Directory/\n",
    "    ├── session_1/ **\n",
    "    ├   ├── dlc-models/\n",
    "    ├   ├── evaluation-results/\n",
    "    ├   ├── labeled-data/\n",
    "    ├   ├── training-datasets\n",
    "    ├   ├── videoName_modelName_sessionName_iterNum.h5 # this is the dataset to model\n",
    "    ├   └── videos/\n",
    "    ...\n",
    "    ├── session_2/ **\n",
    "    ├   ├── dlc-models/\n",
    "    ├   ├── evaluation-results/\n",
    "    ├   ├── labeled-data/\n",
    "    ├   ├── training-datasets\n",
    "    ├   ├── videoName_modelName_sessionName_iterNum.h5 # this is the dataset to model\n",
    "    └── └── videos/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dlc_utils.util import generate_index\n",
    "base_dir = '/'\n",
    "\n",
    "index_filepath = os.path.join(base_dir, 'moseq2dlc-index.yaml')\n",
    "\n",
    "generate_index(base_dir, index_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Groups\n",
    "### What are groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoSeq using groups in the `moseq2-index.yaml` file to indicate whether your collected sessions are representing a single experimental group, or many different groups that you would like to compare while modeling and visuslizing\n",
    "\n",
    "The index file requires that all your sessions have a metadata.json file in order to successfully assign each recorded subject or session to a group.\n",
    "\n",
    "Once a cell is run, it will display your current indexing structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View your Current Group Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlc_utils.util import get_groups_command, set_group\n",
    "\n",
    "get_groups_command(index_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Groups to Model\n",
    "\n",
    "Set the groups by inputting the respective index of the session with the desired group name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_group(int -> sessionIndex, str-> groupName, str->path_to_index)\n",
    "set_group(1, 'group1', index_filepath)\n",
    "get_groups_command(index_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load H5 Files To Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your DLC modeling results (labeled video) that is stored in a `.h5` file in each respective sessions' base directory. This cell will return dataframes that contain the coordinates of each labeled body part for the whole video. This is the training data you will use to train the ARHMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlc_utils.preprocess import load_dlc_modeling_data, pack_data\n",
    "\n",
    "# Load dataset and coordinate key list\n",
    "data_coords, coords = load_dlc_modeling_data(index_filepath)\n",
    "train_data = pack_data(index_filepath, data_coords, coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Crop Coordinates to Center and Orient Mouse Facing East"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a cropped version of your video containing only the mouse segmented from the background. This is only for used to compute grid movies of the mouse syllables post modeling. The cropped video is NOT included in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dlc_utils.preprocess import get_crop_rotated\n",
    "\n",
    "# body part label names (declared in each DLC session config file, under skeleton)\n",
    "front_pt = 'nose' \n",
    "rear_pt = 'tail'\n",
    "cropped_videos = get_crop_rotated(index_filepath, data_coords, front_pt, rear_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Model-Free Changepoints\n",
    "\n",
    "This is an optional step used to aid in determining model-free syllable lengths; which are general approximations of the duration of respective body language syllables. Computing Model-Free Changepoints can be useful for determining the prior variable for syllable duration, denoted as `kappa`, in the ARHMM modeling step.\n",
    "\n",
    "A good Changepoint graph should show a smooth left-skewed bell-curve representing changepoint durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlc_utils.analysis import compute_changepoints\n",
    "from dlc_utils.viz import plot_changepoints\n",
    "\n",
    "cps = compute_changepoints(train_data)\n",
    "fig, ax = plot_changepoints(cps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Labeled Keypoints Using ARHMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train or Load Previously Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from dlc_utils.analysis import model_train_pbb, parse_modeling_results\n",
    "from dlc_utils.viz import plot_training_lls\n",
    "\n",
    "model_path = os.path.join(base_dir, 'arhmm.p.gz')\n",
    "if not os.path.exists(model_path):\n",
    "    model = model_train_pbb(train_data, \n",
    "                                index_filepath,\n",
    "                                model_type=\"arhmm\",\n",
    "                                num_procs=1,\n",
    "                                test_size=0, # hold out any data?\n",
    "                                iters=100, # usually we do 300-400 iterations for final model fits\n",
    "                                kappa=None, # stickiness\n",
    "                                separate_trans=True, # model **multiple** groups separately?\n",
    "                                empirical_bayes=False)\n",
    "    joblib.dump(model, model_path, compress=3)\n",
    "    results = parse_modeling_results(index_filepath, model)\n",
    "else:\n",
    "    model = joblib.load(model_path)\n",
    "    results = parse_modeling_results(index_filepath, model)\n",
    "    \n",
    "lls, _ = plot_training_lls(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Model vs. Model-free Changepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlc_utils.viz import plot_model_cp_diff\n",
    "diff, _ = plot_model_cp_diff(results, cps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Grid Movies\n",
    "\n",
    "This function allows you to create grid videos of the mouse performing the same syllable at different timestamps and durations. Select the respective video index from your `cropped_videos` array to compute it's respective labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dlc_utils.viz import make_grid_movies\n",
    "\n",
    "crowd_dir = os.path.join(base_dir, 'crowd_movies/')\n",
    "video_index = 0\n",
    "# select session index\n",
    "labels = results['labels'][video_index]\n",
    "make_grid_movies(labels, cropped_videos[video_index], output_dir=crowd_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View your generated crowd movies below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Video\n",
    "from glob import glob\n",
    "\n",
    "videos = sorted(glob(os.path.join(crowd_dir, '*.mp4')))\n",
    "vids = [Video(vid, embed=True) for vid in videos]\n",
    "for vid, vp in zip(vids, videos):\n",
    "    print(vp.split('/')[-1])\n",
    "    display(vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Transition Graph\n",
    "\n",
    "Use the following command to generate a syllable transition graph. The graph will be comprised of nodes labelled by syllable, and edges depicting a probable transition, with edge thickness depicting the weight of the transition edge.\n",
    "\n",
    "For multiple groups, there will be a transition graph for each group, as well as a difference-graph with different colors to identify the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.model.util import get_transition_matrix, get_syllable_statistics\n",
    "from moseq2_viz.viz import graph_transition_matrix\n",
    "\n",
    "labels = results['labels']\n",
    "groups = [] # array of declared groups (for multiple sessions)\n",
    "trans_mats = []\n",
    "max_syllables, nexamples = 30, 30\n",
    "\n",
    "if len(groups) > 0:\n",
    "    for group in groups:\n",
    "        use_labels = [lbl for lbl, g in zip(labels, groups) if g==group]\n",
    "        trans_mats.append(get_transition_matrix(use_labels, normalize=True, combine=True, max_syllable=max_syllables))\n",
    "else:\n",
    "    trans_mats = [get_transition_matrix(labels, normalize=True, combine=True, max_syllable=max_syllables)]\n",
    "\n",
    "\n",
    "plt, _, _ = graph_transition_matrix(trans_mats, edge_threshold=.0025, anchor=0, usage_threshold=1,\n",
    "                            edge_width_scale=.2, edge_color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Syllable Usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlc_utils.viz import plot_usages\n",
    "plot_usages(index_filepath, labels, max_syllables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
