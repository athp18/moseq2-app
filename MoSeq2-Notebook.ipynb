{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to MoSeq2-Notebook\n",
    "\n",
    "### Run all of the MoSeq2 tools in a self-contained notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>MoSeq2 Introduction</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Cps4eniKXpoKwSjFGC4R7S1JSeG4bVJg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoSeq2 software toolkit for unsupervised characterization of animal behavior. Moseq takes depth recordings of single behaving animals as input, and outputs a rich labeling of postural dynamics in terms of reused motifs or 'syllables'. \n",
    "\n",
    "This notebook begins with compressed depth recordings (see 'Data Acquisiting Overview' below) and transforms this data through the steps of:\n",
    "- **Extraction**: The animal is segmented from the background and its position and heading direction are aligned across frames.\n",
    "- **Dimensionality reduction**: Raw video is de-noised and transformed to low-dimensional pose trajectories using principal component analysis (PCA).\n",
    "- **Model training**: Pose trajectories are modeled using an autoregressive hidden Markov model (AR-HMM), producing a sequence of syllable labels.\n",
    "- **Analysis**: Model output is reported through visualization and statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "Below are a list of publications and links to the individual github tool wikis for your convenience.\n",
    "- Publications\n",
    "    - [Mapping Sub-Second Structure in Mouse Behavior](http://datta.hms.harvard.edu/wp-content/uploads/2018/01/pub_23.pdf)\n",
    "    - [The Striatum Organizes 3D Behavior via Moment-to-Moment Action Selection](http://datta.hms.harvard.edu/wp-content/uploads/2019/06/Markowitz.final_.pdf)\n",
    "    - [Q&A: Understanding the composition of behavior](http://datta.hms.harvard.edu/wp-content/uploads/2019/06/Datta-QA.pdf)\n",
    "- Wikis\n",
    "    - [Extract](https://github.com/dattalab/moseq2-extract/wiki)\n",
    "    - [PCA](https://github.com/dattalab/moseq2-pca/wiki)\n",
    "    - [Model](https://github.com/dattalab/moseq2-model/wiki)\n",
    "    - [Viz](https://github.com/dattalab/moseq2-viz/wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moseq2 takes animal depth recordings as input. We we have developed a [data acquisition pipeline](https://github.com/dattalab/moseq2-docs/wiki/Setup:-acquisition-software) for the Xbox Kinect depth camera. We suggest following our [data acquisiting tutorial](https://github.com/dattalab/moseq2-docs/wiki/Acquisition) for doing recordings. \n",
    "\n",
    "__It is recommended that MoSeq2 users collect over 10 hours of depth videos (nframes >= ~1 million frames) for the best analysis results.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure you are running the python version located in your corresponding conda environment.\n",
    "\n",
    "Remember: The anaconda environment must be activated prior to launching this jupyter notebook in order to use the specified python version.\n",
    "\n",
    "For example, if your anaconda environment is called moseq2, then your output would look like: ```/Users/username/anaconda3/envs/moseq2/bin/python```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Notebook Setup</h1></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the requirements for Moseq2 by following the [README file](http://localhost:8888/notebooks/MoSeq2_Step_0.ipynb) (if you have not done so already). Then input your desired data directory __absolute path__ in the cell below to get started.\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Rs-LYyYIHueyE3x60dKk_fTtNMkCzqSb\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Folder Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After acquiring some data, an individual session folder should contain the following files:\n",
    "\n",
    "```\n",
    "├── session_1/ **\n",
    "├   ├── depth.dat **        # depth data\n",
    "├   ├── depth_ts.txt **     # timestamps\n",
    "└─  └── metadata.json **    # metadata\n",
    "```\n",
    "***\n",
    "\n",
    "- `depth.dat`: compressed file version of depth video (to be extracted)\n",
    "- `depth_ts.txt`: timestamp file used for metadata analysis (can be used to check dropped fram e rate). Check the depth_ts.txt file to be sure that the dropped frame rate is less than 1%. If you look at the difference between neighboring timestamps, it should be ~30 (as in 30 milliseconds).\n",
    "- `metadata.json`: JSON file containing recording session information.\n",
    "\n",
    "__Keep each session recording folder separate in order to preserve the correct data parings.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data file organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be able to access your acquired data from any directory in your filesystem within this notebook.\n",
    "\n",
    "To ensure that your Moseq2-Notebook runs smoothely, ensure your data directories follow the sample structure below:\n",
    "- Organize your directories such that you have 1 parent directory that contains all of your individual session recording subdirectories (as shown below)\n",
    "\n",
    "```\n",
    ".\n",
    "└── Data_Directory/\n",
    "    ├── session_1/ **\n",
    "    ├   ├── depth.dat        # depth data\n",
    "    ├   ├── depth_ts.txt     # timestamps\n",
    "    ├   └── metadata.json    # metadata\n",
    "    ...\n",
    "    ├── session_2/ **\n",
    "    ├   ├── depth.dat\n",
    "    ├   ├── depth_ts.txt\n",
    "    └── └── metadata.json\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Progress File\n",
    "\n",
    "In case that your notebook kernel is interrupted, crashes, etc. The MoSeq2-Notebook comes with a progress file that will store all of your variables within each session in case you need to restore them at a later time (without performing all the computations again).\n",
    "\n",
    "The variables values will be written to the progress yaml file as you run each cell, such that when you return to your analysis session, you can run the following cell and load all your found variables. \n",
    "\n",
    "__The first cell of each section will be a convenience cell that will restore your notebook variables when run. You only need to run them if you have an uninitialized variable that has been previously computed in a previous kernel.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Your Recording Directories and Create The Progress File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input the absolute path to your desired parent directory containing your recorded session subdirectories in the path variable in the cell below.\n",
    "\n",
    "__Note: In order to correctly find your inputted directory, ensure to input the absolute path. The absolute path is the complete path to your file/folder in your current filesystem. To obtain it, you can run `pwd` in your bash terminal in your desired directory.__\n",
    "\n",
    "The following cell will recursively search through your inputted directory and return a list of the found `.dat` files, and your base working directory path. Leaving the path empty will result in searching your current directory (where the notebook is located)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import get_found_sessions\n",
    "from glob import glob\n",
    "import ruamel.yaml as yaml\n",
    "import os, sys\n",
    "\n",
    "data_path = '/Users/aymanzeine/Desktop/Playground/test_sessions/'\n",
    "\n",
    "base_dir, found_sessions = get_found_sessions(data_path)\n",
    "progress_filepath = os.path.join(base_dir, 'test_progress.yaml')\n",
    "\n",
    "print('Number of found sessions to analyze:', found_sessions)\n",
    "print('Your base directory is:', base_dir)\n",
    "\n",
    "if os.path.exists(progress_filepath):\n",
    "    with open(progress_filepath, 'r') as f:\n",
    "        progress_vars = yaml.safe_load(f)\n",
    "    f.close()\n",
    "    \n",
    "    print('Progress file found, listing initialized variables...\\n')\n",
    "    \n",
    "    for k,v in progress_vars.items():\n",
    "        if v != 'TBD':\n",
    "            print(f'{k}: {v}')\n",
    "    \n",
    "    restore = input('Would you like to restore the above listed notebook variables? [Y -> yes, else -> exit]')\n",
    "    \n",
    "    if restore != \"Y\":\n",
    "        sys.exit()\n",
    "\n",
    "    print('Updating notebook variables...')\n",
    "\n",
    "    config_filepath = progress_vars['config_file']\n",
    "    index_filepath = progress_vars['index_file']\n",
    "    pca_dirname = progress_vars['pca_dirname']\n",
    "    scores_filename = progress_vars['scores_filename']\n",
    "    model_path = progress_vars['model_path']\n",
    "    scores_file = progress_vars['scores_path']\n",
    "    crowd_dir = progress_vars['crowd_dir']\n",
    "\n",
    "else:\n",
    "    print('Progress file not found, creating new one.')\n",
    "    progress_vars = {'base_dir': base_dir, 'config_file': 'TBD', 'index_file': 'TBD', 'pca_dirname': 'TBD',\n",
    "                    'scores_filename': 'TBD', 'scores_path': 'TBD', 'model_path': 'TBD', 'crowd_dir': 'TBD'}\n",
    "    \n",
    "    with open(progress_filepath, 'w') as f:\n",
    "        yaml.safe_dump(progress_vars, f)\n",
    "    f.close()\n",
    "\n",
    "    print('\\nProgress file created, listing initialized variables...')\n",
    "    for k,v in progress_vars.items():\n",
    "        if v != 'TBD':\n",
    "            print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from moseq2_extract.gui import generate_config_command, update_progress\n",
    "\n",
    "config_filepath = os.path.join(base_dir, 'config.yaml')\n",
    "\n",
    "print(f'generating file in path: {config_filepath}')\n",
    "generate_config_command(config_filepath)\n",
    "\n",
    "update_progress(progress_filepath, 'config_file', config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A configuration file has been created in your base directory (depicted below).\n",
    "\n",
    "```\n",
    ".\n",
    "└── Data_Directory/\n",
    "    ├── config.yaml **\n",
    "    ├── session_1/ \n",
    "    ├   ├── depth.dat        \n",
    "    ├   ├── depth_ts.txt     \n",
    "    ├   └── metadata.json    \n",
    "    ...\n",
    "    ├── session_2/ \n",
    "    ├   ├── depth.dat\n",
    "    ├   ├── depth_ts.txt\n",
    "    └── └── metadata.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a Flip File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure your extraction is smooth and invariant to the mouse's orientation, we recommend using a flip-classifier to aid keeping the mouse oriented throughout the extraction.\n",
    "Note: MoSeq2-Notebook currently only supports flip correction for Adult male c57 mice. (You may skip this step if your mice are very different)\n",
    "\n",
    "The following cell will also output the path where your flip classifier has been saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import download_flip_command\n",
    "\n",
    "download_flip_command(base_dir, config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Raw Data Extraction</h1></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the MoSeq2-Extract module to convert your raw data files to human-readable/parseable formats such as mp4 videos, and YAML/HDF5 metadata files. These metadata files are used to then train your PCA model, while the mp4 file is primarily used to ensure that the session was extracted correctly with no defects or unwanted artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the extraction step, begin by testing your detected ROIs with the default parameters. If your arena has been correctly detected, continue into the to the test extraction step.\n",
    "\n",
    "The first two steps are meant to debug possible extraction errors you may encounter before performing an extraction on your full dataset.\n",
    "\n",
    "This step is important because it will determine the integrity of your data going into the analysis steps. If the resulting extraction contains artifacts, or is too noisy, etc. the PCA and ARHMM modeling results will not be reliable.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1YBBpvLTq8xc6wzvQqrenGA9B6a01SH4L\">\n",
    "\n",
    "Once testing is done, you can then proceed to extract all the session files found by your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Extraction Data Quality Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing a full extraction on your recordings, ensure your Regions of Interest (ROIs) are properly found. This will bring more clarity as to what to expect after a complete extraction of your data.\n",
    "\n",
    "The resulting calculated ROIs correspond to the detected arena floor, the background subtracted from the detected arena, and the first frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test ensures that your whole background area is properly captured without any artifacts that may interfere with the mouse video extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurable Parameter Descriptions\n",
    "- `bg_roi_dilate`: the detected floor mask is dilated with a kernel whose (width, height) is specified by this parameter\n",
    "- `bg_roi_depth_range`: min/max depth values in which the ROI detection algorithm will search for a flat surface.\n",
    "- `bg_roi_gradient_threshold`: threshold value to decide which areas to include in the background roi.\n",
    "\n",
    "***\n",
    "\n",
    "### Possible ROI Pathologies\n",
    "\n",
    "__Pathology__: Resulting background ROI contains holes, or walls are not crisp/well defined.\n",
    "\n",
    "__Reasons__: \n",
    "- The recording environment may have been too bright, causing the infrared depth calculations to be skewed.\n",
    "- The background depth range may be inaccurate to that of the real-life recording scenario. Always ensure that the inputted heights and depth ranges are accurate to that of the recorded session.\n",
    "\n",
    "__Solutions__: \n",
    "- Assuming the data acquisition went okay, try adjusting the background dilation parameter to a larger kernel in the case of an incomplete background, and smaller in the case of ROIs with holes in the arena background.\n",
    "- Similarly, check if the inputted depth range is accurate to that of the real-life recording camera height from the bucket floor.\n",
    "- If the walls are not being captured correctly, or there are holes in the background, try __slightly__  decreasing the background gradient threshold (by increments of [50-100]) to try to capture more of the environment. \n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will extract the first frame, ROI, and background ROI for your reference before continuing into the extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "from moseq2_extract.gui import find_roi_command\n",
    "\n",
    "with open(config_filepath, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "f.close()\n",
    "\n",
    "# Relevant ROI parameters you may need to configure\n",
    "config_data['bg_roi_dilate'] = (10, 10) # Size of the mask dilation (to include environment walls)\n",
    "config_data['bg_roi_depth_range'] = (650, 750) # Range to search for floor of arena (in mm)\n",
    "config_data['bg_roi_gradient_threshold'] = 3000 # Threshold at which ROI boundaries are placed on the input frame\n",
    "\n",
    "with open(config_filepath, 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "f.close()\n",
    "\n",
    "rois_dir = find_roi_command(base_dir, config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, you can expect the following directory structure:\n",
    "\n",
    "```\n",
    ".\n",
    "└── Data_Directory/\n",
    "    ├── config.yaml \n",
    "    ├── session_1/ \n",
    "    ├   ├── proc/ **\n",
    "    ├   ├   ├── bground.png & bground.tiff **\n",
    "    ├   ├   ├── first_frame.png & first_frame.tiff **\n",
    "    ├   ├   └── roi.png & roi.tiff ** \n",
    "    ├   ├── depth.dat        \n",
    "    ├   ├── depth_ts.txt     \n",
    "    ├   └── metadata.json    \n",
    "    ...\n",
    "    └── session_2/ \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display your calculated ROI images below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "from glob import glob\n",
    "\n",
    "images = glob(os.path.join(rois_dir, '*.png'))\n",
    "ims = [Image(im) for im in images]\n",
    "for im, ip in zip(ims,images):\n",
    "    print(ip.split('/')[-1])\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Test Extraction \n",
    "\n",
    "Now that the ROIs have been successfully detected, you can now test whether the mouse is segmented from the arena and oriented correctly during the extraction process.\n",
    "\n",
    "The extracted version of the mouse will appear on the top left-hand corner of the generated video. If the mouse is consistently facing rightward, then the extraction test is successful and you can proceed to extract your dataset with the set parameters.\n",
    "\n",
    "Otherwise, ensure that your environmental parameters are correct and an appropriate amount of spatial and temporal filtering has been applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurable Parameter Descriptions\n",
    "- `min_height`: The shortest possible height that the mouse can be in your recordings. Important factor to include in order to properly estimate the minimum depth value during video construction.\n",
    "- `max_height`: The tallest possible height your mouse can be in your recordings. Important factor to include in order to properly estimate the maximum depth value during video construction.\n",
    "- `spatial_filter_size`: Median spatial filter applied to the raw video pre-extraction in order to get crisp mp4 files. The larger the kernel, the more granular your video will become. Be aware not to set it too high as to not lose video clarity. (Must be ODD)\n",
    "- `temporal_filter_size`: Median temporal filter applied to the raw video pre-extraction in order to handle any frame drops or time irregularities in the compressed data. Only use if your videos appear to be laggy or have a noticable amount of frames dropped. (Must be ODD)\n",
    "\n",
    "***\n",
    "\n",
    "### Possible Extraction Pathologies\n",
    "__Pathology__: The extracted mouse image is too grainy causing the depth contour boundaries to not be well defined.\n",
    "\n",
    "__Reasons__: \n",
    "- The inputted minimum and maximum mouse heights were inaccurate, causing the depth estimations to be incorrect.\n",
    "- Spatial filter kernel size may be too small, underprocessing each frame.\n",
    "- Temporal filter kernel size may be too large, overprocessing the mouse boundaries.\n",
    "\n",
    "__Solutions__: \n",
    "- Ensure your mouse measurements are accurate to your recording environment.\n",
    "- Try increasing the applied spatial kernel size. Increasing the kernel size may help make the mouse image crisp without losing definition in the body contours in the extracted image.\n",
    "- Decreasing any applied temporal filtering is also recommended. However, note that there is a balance of temporal and spatial filtering to achieve a good extraction.\n",
    "\n",
    "***\n",
    "\n",
    "__Pathology__: Extracted mouse video is too jittery.\n",
    "\n",
    "\n",
    "__Reasons__: \n",
    "- This is can be due to the lack of temporal filtering. \n",
    "- There may be too much noise in the image skewing the mouse ellipse fit, causing the centroid calculation to be inconsistent over frame ranges. \n",
    "\n",
    "__Solutions__: \n",
    "- Temporal filtering helps smoothen the transitions of the frames over time, simplifying the mouse ellipse-fit task, and therefore stabilizing the extracted video.\n",
    "- Using an appropriate amount of spatial and temporal filtering will help remedy this issue.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "__Pathology__: Extracted mouse orientation is not consistently correct. (Mouse is facing wrong direction)\n",
    "\n",
    "\n",
    "__Reasons__: \n",
    "- Using too large of a tail filter size, or overfiltering the image to remove the mouse tail can also ambiguate the mouse ellipse fit, skewing orientation detection.\n",
    "- Underfiltering the temporal domain can also lead to mouse orientation inaccuracies due to pixel jitter cause by the kinect on-line depth estimation. \n",
    "- Overfiltering the spatial domain can also cause inaccurate orientation flip detection due to the loss in detail of the image, namely around the edges of the mouse.\n",
    "\n",
    "\n",
    "__Solutions__: \n",
    "- Ensure your flip classifier path is found in your `config.yaml` file in order for it to be applied.\n",
    "- Temporal filtering helps keep the fitted ellipse consistent throughout the image processing, increasing orientation-flip precision.\n",
    "- Once again, experiment to try to achieve the appropriate balance between temporal and spatial filtering.\n",
    "\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "from moseq2_extract.gui import sample_extract_command\n",
    "\n",
    "nframes = 200 # number of frames to extract from raw to preview\n",
    "\n",
    "with open(config_filepath, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "f.close()\n",
    "\n",
    "# Extraction parameters you may need to configure\n",
    "config_data['min_height'] = 10 # Min mouse height from floor (mm)\n",
    "config_data['max_height'] = 100 # Max mouse height from floor (mm)\n",
    "config_data['spatial_filter_size'] = [3] # Space prefilter kernel (median filter, must be odd)\n",
    "config_data['temporal_filter_size'] = [0] # Time prefilter kernel (median filter, must be odd)\n",
    "\n",
    "with open(config_filepath, 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "f.close()\n",
    "\n",
    "sample_ext_dir = sample_extract_command(base_dir, config_filepath, nframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After an extraction, you can expect the following directory structure:\n",
    "\n",
    "```\n",
    ".\n",
    "├── config.yaml\n",
    "├── session_1/\n",
    "├   ├── sample_proc/ **\n",
    "├   ├   ├── bground.png & bground.tiff **\n",
    "├   ├   ├── first_frame.png & first_frame.tiff **\n",
    "├   ├   ├── results_00.mp4 **\n",
    "├   ├   ├── results_00.h5 **\n",
    "├   ├   ├── results_00.yaml **\n",
    "├   ├   └── roi.png & roi.tiff ** \n",
    "├   ├── depth.dat\n",
    "├   ├── depth_ts.txt\n",
    "├   └── metadata.json\n",
    "└── session_2/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view your sample extraction below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Video\n",
    "\n",
    "vid = Video(os.path.join(sample_ext_dir, 'results_00.mp4'), embed=True)\n",
    "\n",
    "display(vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are happy with your sample extraction, continue to extracting your full dataset. Otherwise, consider adjusting some of your ROI or extraction parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Session(s)\n",
    "\n",
    "The cell will prompt you to choose whether you would like to extract individual sessions, or all of them. Enter your selection, and then wait for the extraction to complete to preview them.\n",
    "\n",
    "__WARNING: If you are extracting files of size X GB, where X much larger than the amount of available RAM GB, you may experience an interruption in the extraction due to a memory overflow problem.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import extract_found_sessions\n",
    "\n",
    "# depth files to recursively search for that have been partially extracted or not yet extracted \n",
    "ext = '.dat'\n",
    "\n",
    "commands = extract_found_sessions(base_dir, config_filepath, ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what your directory structure should look like once the process is complete:\n",
    "\n",
    "```\n",
    ".\n",
    "├── config.yaml\n",
    "├── session_1/\n",
    "├   ...\n",
    "├   └── proc/ **\n",
    "├   ├   ├── roi.tiff\n",
    "├   ├   ...\n",
    "├   ├   ├── results.yaml **\n",
    "├   └   └── results.h5 **\n",
    "└── session_2/\n",
    "├   ...\n",
    "├   └── proc/ **\n",
    "├   ├   ├── roi.tiff\n",
    "├   ├   ...\n",
    "├   ├   ├── results.yaml **\n",
    "└   └   └── results.h5 **\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate your results into one folder and generate an index file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will search through your base directory for the `proc/` folders in each session, and copy them all in a single directory. \n",
    "\n",
    "Then it will generate the index file by searching for all the metadata found in the `results_00.h5`/`results_00.yaml` files, and consolidate all that information in one file, assigning each session to a `default` group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurable Parameter Descriptions\n",
    "- `recording_format`: the start_time, session_name, and subject_name parameters are variable names that are read from each `metadata.json` file. The names of the resulting files will have the inputted format.\n",
    "- `aggregate_results_dirname`: directory name that contains all of your extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import aggregate_extract_results_command, update_progress\n",
    "\n",
    "recording_format = '{start_time}_{session_name}_{subject_name}' # filename formats for the copied extracted data files\n",
    "aggregate_results_dirname = 'aggregate_results/' # directory name to save all metadata+extracted videos to with above respective name format\n",
    "\n",
    "index_filepath = aggregate_extract_results_command(base_dir, recording_format, aggregate_results_dirname)\n",
    "\n",
    "update_progress(progress_filepath, 'index_file', index_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregate results folder will be saved in your base directory,\n",
    "resulting in the following directory (sample) structure where the base directory contains the notebook:\n",
    "\n",
    "```\n",
    ".\n",
    "├── aggregate_results/ **\n",
    "├   ├── session_1_results.h5 ** # session 1 metadata\n",
    "├   ├── session_1_results.yaml **\n",
    "├   ├── session_1_results.mp4 ** # session 1 extracted video\n",
    "├   ├── session_2_results.h5 ** # session 2 metadata\n",
    "├   ├── session_2_results.yaml **\n",
    "├   └── session_2_results.mp4 ** # session 2 extracted video\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml ** # index file\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n",
    "\n",
    "__Notice your index file has been generated in your base directory.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View all of your extracted videos below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Video\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "extractions = glob(os.path.join(base_dir, aggregate_results_dirname, '*.mp4'))\n",
    "vids = [Video(vid, embed=True) for vid in extractions]\n",
    "for vid, ext in zip(vids, extractions):\n",
    "    print(ext.split('/')[-1])\n",
    "    display(vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Principal Component Analysis (PCA)</h1></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been extracted, implement a Principal Component Analysis on your metadata (specifically h5 files) to compute the principal components of your mouse's body in order to subsequently classify its behavior in the ARHMM model.\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1I1WcfEwzpfwIxNYStX7swLAIvjQEVApy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A good example of what you should expect from your PCA Components and Scree plot are shown below:__\n",
    "\n",
    "<center>Components</center> | <center>Scree Plot</center>\n",
    "- | - \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1dX5Gpd3PKL4vfVviLeP0CqBrz9PW37Au\" width=400 height=400> | <img src=\"https://drive.google.com/uc?export=view&id=12uqsBYuWCjpUQ6QrAjo35MnwYDzHqnge\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurable Parameter Descriptions\n",
    "- `gaussfilter_space`: Kernel size for performing a gaussian spatial filter on your processed mouse video before performing PCA. This helps identify crisper, more informative principal components.\n",
    "- `gaussfilter_time`: Kernel size for performing a gaussian temporal filter\n",
    "- `medfilter_space`: Same as gauss filter kernel but uses Median Filtering instead. (Typically use one or the other)\n",
    "    - Both spatial filters are used for when the principal components do not appear to have crisp boundaries, or are all too similar to each other to be considered reliable components.\n",
    "- `medfilter_time`: Same as gauss filter kernel but uses Median Filtering instead. (Typically use one or the other)\n",
    "- `missing_data`: If you have missing/dropped frames in your videos, set this to true.\n",
    "- `missing_data_iters`: Number of times to iterate over missing data during PCA to fill in missing gaps appropriately.\n",
    "- `recon_pcs`: Number of principal components to reconstruct from missing data.\n",
    "\n",
    "***\n",
    "\n",
    "### Possible PCA Pathologies\n",
    "__Pathology__: Cannot achieve a explained variance of over 90% from less than 15 Principal Components (PCs).\n",
    "\n",
    "__Reasons__: \n",
    "- The crop size may be too large, resulting in large amounts of extraneous noise being included in the PCA model.\n",
    "- The input frames may be too noisy; this could be noise stemming from overall image quality, over/under filtering, or non-consitstent mouse orientation or centroid.\n",
    "- If the input frames are overly spatially smoothed, then the PCA will not be able to accurately separate each principal component.\n",
    "- If the training set is overly temporally smoothed, then that could attribute to some overfitting principal components. This means that causing it to incorrectly explain a larger variance for a PC due to it being extentuated over more frames as a result of the temporal filter.\n",
    "- Too few frames were inputted to derive proper PCs.\n",
    "\n",
    "__Solutions__: \n",
    "- Check if the crop size is too large, if so, decrease it and re-extract your data.\n",
    "- Try (incrementally) adjusting the spatial and temporal filtering kernel sizes in the PCA step. This will aid the PCA in constructing PCs that more accurately explain large enough pose trajectory portions to cover over 90% of the data's explained variance.\n",
    "- Acquire and extract more data, then try with more data.\n",
    "\n",
    "***\n",
    "\n",
    "__Pathology__: Graphed PCs look overprocessed, or are not representative of realistic mouse body regions.\n",
    "\n",
    "\n",
    "__Reasons__: \n",
    "- Lack of spatial filtering, and/or a large amount of pixel noise can attribute to this type of issue. Further, If the training set is too small for the large amount of features, then the PCA may try to overestimate some mouse body regions, causing inaccurate PC selection.\n",
    "- Inaccurately inputted minimum and maximum mouse inputted heights could contribute to the over/underestimation of the depth contours’ explained variance, therefore segmenting the mouse’s body with very coarse boundaries.\n",
    "- An excess of the combination of temporal and/or spatial filtering can also cause overprocessing of PCs due to the features not only have lost definition in the spatial domain, but also in the time domain.\n",
    "\n",
    "__Solutions__: \n",
    "- Ensure that an appropriate amount of spatial and temporal filtering is applied.\n",
    "- If there are  missing frames, apply and appropriate amount of temporal filtering.\n",
    "- Ensure that if there are missing frames, a proper amount of PCs are being reconstructed (`recon_pcs` is set correctly).\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Convenience) Restore Progress Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "Javascript(\"Jupyter.notebook.execute_cells([18])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_pca.gui import train_pca_command\n",
    "from moseq2_extract.gui import update_progress\n",
    "import ruamel.yaml as yaml\n",
    "\n",
    "pca_filename = 'pca' # Name of your PCA model h5 file to be saved\n",
    "pca_dirname = '_pca/' # Directory to save your computed PCA results\n",
    "\n",
    "with open(config_filepath, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "f.close()\n",
    "\n",
    "# PCA parameters you may need to configure\n",
    "config_data['gaussfilter_space'] = (1.5, 1) # Spatial filter for data (Gaussian)\n",
    "config_data['medfilter_space'] = [0] # Median spatial filter\n",
    "config_data['recon_pcs'] = 10 # Number of PCs to use for missing data reconstruction\n",
    "config_data['missing_data'] = False # Use missing data PCA\n",
    "config_data['missing_data_iters'] = 10 # Number of times to iterate over missing data during PCA\n",
    "\n",
    "\n",
    "with open(config_filepath, 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "f.close()\n",
    "\n",
    "train_pca_command(base_dir, config_filepath, pca_dirname, pca_filename)\n",
    "\n",
    "update_progress(progress_filepath, 'pca_dirname', pca_dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, you can expect your relative directory structure to look something like this:\n",
    "```\n",
    ".\n",
    "├── _pca/ **\n",
    "├   ├── pca.h5 ** # pca model compressed file\n",
    "├   ├── pca.yaml  ** # pca model YAML metadata file\n",
    "├   ├── pca_components.png **\n",
    "├   └── pca_scree.png **\n",
    "├── aggregate_results/\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "\n",
    "```\n",
    "\n",
    "View your `computed PCs` and `scree plot` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "images = [os.path.join(base_dir, pca_dirname, 'pca_components.png'), \n",
    "          os.path.join(base_dir, pca_dirname, 'pca_scree.png')]\n",
    "for im in images:\n",
    "    display(Image(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Principal Component Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply your trained PCA model using your computed principal components to compute your PC Scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from moseq2_pca.gui import apply_pca_command\n",
    "from moseq2_extract.gui import update_progress\n",
    "\n",
    "scores_filename = 'pca_scores' # name of the scores file to compute and save\n",
    "index_filepath = os.path.join(base_dir, 'moseq2-index.yaml') # path to your auto-generated (possibly modified) index file\n",
    "\n",
    "apply_pca_command(base_dir, index_filepath, config_filepath, pca_dirname, scores_filename)\n",
    "\n",
    "update_progress(progress_filepath, 'scores_filename', scores_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, you will have a pca_scores file saved in your pca directory. (Example shown below)\n",
    "```\n",
    ".\n",
    "├── _pca/\n",
    "├   ├── pca.h5\n",
    "├   ├── pca.yaml\n",
    "├   ├── pca_scores.h5  ** # scores file\n",
    "├   ├── pca_components.png\n",
    "├   └── pca_scree.png\n",
    "├── aggregate_results/\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Computing Model-Free Syllable Changepoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an optional step used to aid in determining model-free syllable lengths; which are general approximations of the duration of respective body language syllables. Computing Model-Free Changepoints can be useful for determining the prior variable for syllable duration, denoted as `kappa`, in the ARHMM modeling step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A good example of a Changepoints Distance plot is shown below__\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1sMkSB34bGbOimumN6Gg1-zV2Hk98v2Zy\" width=400 height=400>\n",
    "\n",
    "\n",
    "Measure syllable block duration distances between detected syllables using your PCA model or computed PC scores below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Warning: These parameters have been pre-tuned to accomodate for C57 Mice, and those of the like. Therefore, we do not recommend changing the changepoint calculation parameters. However, if you decide to do so, it is at your own risk.__\n",
    "\n",
    "### Configurable Parameter Descriptions\n",
    "- `threshold`: Computed value used to determine the \"peak\"/transition point from one syllable to the other\n",
    "- `dims`: Number of random projections to use in order to compare the computed principal components with, and determine a distribution for the block durations.\n",
    "\n",
    "***\n",
    "\n",
    "### Possible Model-Free Changepoints Pathologies\n",
    "__Pathology__: Model-free syllable changepoint distances distribution is skewed incorrectly.\n",
    "\n",
    "__Reasons__: \n",
    "- Considering that this a plot generated using pre-computed principal components, errors that appear here may be direcly correlated with a poorly fitted PCA.\n",
    "- This can occur if PCA model was trained on input data with many missing frames that were not reconstructed using the correct PCs, iterated over too many or too few times, or did not use a proper mask configuration to properly reconstruct the missing frames.\n",
    "- This can be attributed to an excess of temporal filtering, causing the computed principal components to be either misinterpreted or overgeneralized over long time scales, skewing the distance distribution.\n",
    "\n",
    "__Solutions__: \n",
    "- Try retraining the PCA with less temporal filtering.\n",
    "- Ensure your extracted data is correct. If the extraction version of the mouse is too noisy, then the PC trajectories cannot be accurately applied to the data.  \n",
    "- Get more data and try again.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_pca.gui import compute_changepoints_command\n",
    "import ruamel.yaml as yaml\n",
    "changepoints_filename = 'changepoints' # name of the changepoints images to generate\n",
    "with open(config_filepath, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "f.close()\n",
    "\n",
    "# Changepoint computation parameters you may want to configure\n",
    "config_data['threshold'] = 0.5 # Peak threshold to use for changepoints\n",
    "config_data['dims'] = 300 # Number of random projections to use\n",
    "\n",
    "with open(config_filepath, 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "f.close()\n",
    "\n",
    "compute_changepoints_command(base_dir, config_filepath, pca_dirname, changepoints_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changepoints plot will be generated and saved in the pca directory (example below).\n",
    "\n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├   ├── pca.h5\n",
    "├   ├── pca_scores.h5\n",
    "├   ...\n",
    "├   └── changepoints_dist.png **\n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View your changepoints distance plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(os.path.join(base_dir, pca_dirname, changepoints_filename+'_dist.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>ARHMM Modeling</h1></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train your ARHMM (Auto-Regressive Hidden Markov Model), you will use your computed PC scores as your input data, and specify whether you are modeling a single experimental group for observational research, or modeling multiple different groups (e.g. control vs. experimental groups) for comparative analysis.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1V2n5Jg61Pr86m0groyTX_qJ40bSm5LG7\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Specify Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoSeq using groups in the `moseq2-index.yaml` file to indicate whether your collected sessions are representing a single experimental group, or many different groups that you would like to compare while modeling and visuslizing\n",
    "\n",
    "The index file requires that all your sessions have a metadata.json file in order to successfully assign each recorded subject or session to a group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, all the session recordings have the same group title: `'default'`. If you do not have 2 sessions that are different enough to separate to different groups for later comparison, you can skip this step.\n",
    "\n",
    "Otherwise, there are 3 ways you are able to specify your groups:\n",
    "1. Specify group by SessionName\n",
    "2. Specify group by SubjectName\n",
    "3. Manually edit index file\n",
    "\n",
    "Once a cell is run, it will display your current indexing structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Convenience) Restore Progress Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "Javascript(\"Jupyter.notebook.execute_cells([18])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Your Index File\n",
    "#### View Indexed Sessions\n",
    "Use this cell to view your sessions' information regarding their SessionNames, SubjectNames, and Groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import get_groups_command\n",
    "\n",
    "index_filepath = os.path.join(base_dir, 'moseq2-index.yaml')\n",
    "\n",
    "get_groups_command(index_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Specify Group by Session Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import add_group_by_session\n",
    "\n",
    "value = 'ayman_first_tethered_recording' # value of the corresponding key\n",
    "group = 'group2' # designated group name\n",
    "exact = True # Must be exact key-value match\n",
    "lowercase = False # change to lowercase\n",
    "negative = False # select opposite selection than key-value pair given\n",
    "\n",
    "add_group_by_session(index_filepath, value, group, exact, lowercase, negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Specify Group by Subject Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import add_group_by_subject\n",
    "\n",
    "value = 'blackStockOFA80GritSanded_012517' # value of the corresponding key\n",
    "group = 'group2' # designated group name\n",
    "exact = False # Must be exact key-value match\n",
    "lowercase = False # change to lowercase\n",
    "negative = False # select opposite selection than key-value pair given\n",
    "\n",
    "add_group_by_subject(index_filepath, value, group, exact, lowercase, negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Manually Edit Index File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply navigate to your `moseq2-index.yaml` file in your designated directory, and editing the group key-value pair to your specified name values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ARHMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurable Parameter Descriptions\n",
    "- `hold_out`: Boolean for whether to hold out data during the training process.\n",
    "- `hold_out_seed`: Integer used to reproduce the same hold out set for repeated testing.\n",
    "- `nfolds`: Number of data folds to hold out during training. (If used, nfolds <= nsessions)\n",
    "- `npcs`: Number of selected principal components, chosen in order as shown in the PC Components plot. If too few or too many PCs are selected, the ARHMM predictions will become unreliable.\n",
    "- `num_iter`: Number of time the model will iterate over your dataset, we recommend at least 100 starting out. This is modeling regularization parameter to ensure that your model is fitting appropriately to its given dataset.\n",
    "- `max_states`: Maximum number of states the ARHMM that the ARHMM can end up with at the end of training. This is modeling regularization parameter that indicates the complexity of the transitions that may be happening in your dataset. Therefore, if there are too few the model may not learn the actual behavior, and if there are too many, then the model will overfit to the dataset.\n",
    "- `separate-trans`: Boolean for whether to separate the modeling process for different groups. (Must set to true if number of unique groups > 1)\n",
    "- `kappa`: Prior probability distribution variable used to indicate average syllable length. Setting kappa to the number of frames is a good starting point to determining the proper expressed syllable durations. If kappa is too low, syllables will shorter in duration, and vice versa.\n",
    "- `checkpoint_freq`: Value indicating when to save model checkpoints per number of iterations passed. (If -1, do not checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from moseq2_model.gui import learn_model_command\n",
    "from moseq2_extract.gui import update_progress\n",
    "import os\n",
    "\n",
    "pca_dir = os.path.join(base_dir, pca_dirname)\n",
    "scores_file = os.path.join(pca_dir, scores_filename+'.h5') # path to input PC scores file to model\n",
    "model_path = os.path.join(base_dir, 'model.p') # path to save trained model\n",
    "\n",
    "# Advanced modeling parameters\n",
    "hold_out = False # boolean to hold out data during the training process\n",
    "hold_out_seed = 42 # integer to standardize the held out folds during training\n",
    "nfolds = 2 # number of folds to hold out during training (if hold_out==True)\n",
    "npcs = 10  # number of PCs being used\n",
    "\n",
    "num_iter = 100 # number of iterations to train model\n",
    "max_states = 100 # number of maximum states the ARHMM can end up with\n",
    "kappa = None # syllable length prior None for default=nframes\n",
    "robust = False # use robust-ARHMM with t-distribution\n",
    "\n",
    "separate_trans = True # separate group transition graphs; set to True if ngroups > 1\n",
    "\n",
    "checkpoint_freq = -1 # model saving freqency (in interations)\n",
    "\n",
    "learn_model_command(scores_file, model_path, config_filepath, index_filepath, hold_out, nfolds,\n",
    "                    num_iter, max_states, npcs, kappa,\n",
    "                    separate_trans, robust, checkpoint_freq)\n",
    "\n",
    "update_progress(progress_filepath, 'scores_path', scores_file)\n",
    "update_progress(progress_filepath, 'model_path', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, your model will be saved in your base directory (shown below). \n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── model.p **\n",
    "├── moseq2-index.yaml/\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n",
    "\n",
    "Now use the moseq2-viz module to produce crowd videos and a number of statistical analysis plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Visualize Analysis Results</h1></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a trained ARHMM, you can use it generate informative graphs and videos regarding the behavior syllables found, their usage frequency, and transition probabilities.\n",
    "\n",
    "The graph below shows the 4 operations that the MoSeq2-Viz module currently affords. They can also be computed in any order at this point in the notebook.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1K_PnJ6psGxmXkUScIpfBStzXbCTO6MB2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Crowd Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool allows you to create videos containing many overlayed clips of the mouse performing the same specified syllable at the moment a red dot appears on their body. The videos are sorted by most frequently expressed syllable to least.\n",
    "\n",
    "***\n",
    "\n",
    "### Possible Crowd Movie Pathologies\n",
    "\n",
    "__Pathology__: Generated crowd movies look all too similar.\n",
    "\n",
    "__Reasons__: \n",
    "- The input data size (number of frames) is too small, causing the ARHMM to overfit each state to a larger set of PCs.\n",
    "- The selected PCs may have been overfiltered, causing them to be fitted to a larger set of differently labelled syllables.\n",
    "- The ARHMM model may be too simple, meaning that either the selected PCs do not explain a large enough amount of variance in the set, or that the selected maximum amount of states is not enough to capture the full variance in the mouse’s behaviors.\n",
    "\n",
    "\n",
    "__Solutions__: \n",
    "- Ensure your PCs cover over 90% the data's explained variance, and that they are all included in the ARHMM training data.\n",
    "- Try increasing the number of `max_states` that the ARHMM has to increase the problem complexity, increasing variance in labels.\n",
    "- Try increasing the number of PCs being used in the ARHMM training.\n",
    "\n",
    "***\n",
    "\n",
    "__Pathology__: Generated crowd movies do not contain mice unanimously exhibiting the same syllable, or have large varying time-scales.\n",
    "\n",
    "\n",
    "__Reasons__: \n",
    "- The ARHMM model is underfitted; this could be due to too few model training iterations, or the max number of states could be too large, hence incorrectly predicting the mouse’s behaviors because the syllable likelihoods have not yet converged at the end of training.\n",
    "- The selected syllable duration prior probability distribution, `kappa`, denotes the syllable time series duration probability distribution. If it is too large, then the resulting syllables will span over large time scales, and vice versa.\n",
    "- The input frames may be too noisy, or the PC’s selected may be too complex/unrealistic to properly represent a consistent behavior syllable.\n",
    "- Too many PCs may have been selected, causing the dimensionality complexity to increase, resulting in overly explained syllables.\n",
    "- Too much additive noise was augmented during the training process, possibly also during the PCA training, overcomplicating the PC-to-time-series syllable mapping problem.\n",
    "- Improper temporal smoothing will lead to unreliable calculations of syllable prior distributions.\n",
    "\n",
    "\n",
    "__Solutions__: \n",
    "- Increase the number of model training iterations in order ensure that the syllable likelihoods are converging. \n",
    "- Try decreasing the complexity of your ARHMM. If the `max_states` is set too high, you may end up with many 'starved states' that will skew the posterior probability distributions for some syllables.\n",
    "- Try decreasing the amount of additive noise (if added), or add spatial filtering to your PC or frame data.\n",
    "- Acquire more data and try again.\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "__Note: ensure all of the paths listed the `moseq2-index.yaml` file are correct in order for the following cells to run successfully.__\n",
    "\n",
    "If the command stalls, restart the kernel, reload your variables and run it again.\n",
    "\n",
    "To create the crowd videos, run the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Convenience) Restore Notebook Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "Javascript(\"Jupyter.notebook.execute_cells([18])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from moseq2_viz.gui import make_crowd_movies_command\n",
    "from moseq2_extract.gui import update_progress\n",
    "\n",
    "model_path = os.path.join(base_dir, 'model.p') # path to save trained model\n",
    "index_filepath = os.path.join(base_dir, 'moseq2-index.yaml')\n",
    "crowd_dir = os.path.join(base_dir, 'crowd_movies/') # output directory to save all movies in\n",
    "\n",
    "max_syllables, max_examples = 20, 20 # maximum number of syllables, and examples of each syllable in a video respectively\n",
    "\n",
    "make_crowd_movies_command(index_filepath, model_path, config_filepath, crowd_dir, max_syllables, max_examples)\n",
    "\n",
    "update_progress(progress_filepath, 'crowd_dir', crowd_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once completed, you can find your crowd movies along with a metadata YAML file in your corresponding crowd directory. The metadata `info.yaml` file will contain model information pertaining to how these crowd videos were produced.\n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── crowd_movies/ **\n",
    "├   ├── info.yaml **\n",
    "├   ├── syllable_sorted_44 (usage).mp4 **\n",
    "├   ...\n",
    "├   └── syllable_sorted_12 (usage).mp4 **\n",
    "├── model.p \n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View your generated crowd movies below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Video\n",
    "from glob import glob\n",
    "\n",
    "videos = glob(os.path.join(crowd_dir, '*.mp4'))\n",
    "vids = [Video(vid, embed=True) for vid in videos]\n",
    "for vid, vp in zip(vids, videos):\n",
    "    print(vp.split('/')[-1])\n",
    "    display(vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Usage Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this command to compute the model-detected syllables usages sorted in descending order of usage. \n",
    "\n",
    "__Note for plotting multiple groups: remember to include all group names in the tuple to graph them. Additionally, the model must be already trained on the specified groups. (When training the model, it will first output the uuids that have been added with their specified group name for your reference).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import plot_usages_command\n",
    "\n",
    "model_path = os.path.join(base_dir, 'model.p')\n",
    "plot_path = os.path.join(base_dir, 'plots/')\n",
    "sort = True\n",
    "count = 'usage'\n",
    "max_used_syllable = max_syllables - 1 \n",
    "groups = ('group1', 'group2') #empty tuple for single group usage plot\n",
    "output_file = os.path.join(plot_path, 'usages')\n",
    "\n",
    "if not os.path.exists(plot_path):\n",
    "    os.makedirs(plot_path)\n",
    "\n",
    "plot_usages_command(index_filepath, model_path, sort, count, max_syllables, groups, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Scalar Summary and Tracking Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following command to compute some scalar summary information about your modeled groups, such as average velocity, height, etc.\n",
    "This command will also generate a tracking summary plot; depicting the path traveled by the mouse in your recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import plot_scalar_summary_command\n",
    "\n",
    "output_file = os.path.join(plot_path, 'scalars') # prefix name of the saved scalar position and summary graphs\n",
    "\n",
    "plot_scalar_summary_command(index_filepath, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph your output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "from glob import glob\n",
    "\n",
    "images = glob(os.path.join(plot_path, 'scalars_*.png'))\n",
    "ims = [Image(im) for im in images]\n",
    "for im in ims:\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Syllable Transition Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following command to generate a syllable transition graph. The graph will be comprised of nodes labelled by syllable, and edges depicting a probable transition, with edge thickness depicting the weight of the transition edge.\n",
    "\n",
    "***\n",
    "\n",
    "### Possible Syllable Transition Graph Pathologies\n",
    "\n",
    "__Pathology__: Too little syllables are being generated in the syllable transition graph.\n",
    "\n",
    "__Reasons__: \n",
    "- If the ARHMM is trained on too few states or principal components, then the syllable mapping problem becomes oversimplified, causing less syllables to be recognized.\n",
    "- If the syllables probability distribution is too sporatic, the model will have a high propensity to get stuck in local altimas during training, leading to underfitting.\n",
    "- The input data (frames and/or PCA) may have been excessively spatially filtered, losing the spatial syllable complexity, leading to the model overgeneralizing the mouse poses and detecting less syllables. Conversely, it could have a large amount of dropped frames, leading to the same problem.\n",
    "- The weights can be influenced by the `kappa` parameter, denoting how long a mouse will be exhibiting a given syllable. Inaccurate probability priors inputted during training can have catastrophic repercussions on model reliability.\n",
    "\n",
    "__Solutions__: \n",
    "- A good way to combat this issue is to use a Robust AR-HMM, which normalizes the syllable probability distributions to a t-distribution.\n",
    "- Ensure an appropriate amount of temporal filtering is applied to your extracted videos. If too much smoothing is applied then the transitions will become less apparent, skewing model predictions.\n",
    "- Try using different values of `kappa` by increasing it or decreasing it by factors of 10. (Note: we recommend using a minimum `kappa` value equal to the number of frames).\n",
    "- Try increasing the `max_states` variable to add more complexity to the model training problem.\n",
    "- Adjusting any spatial or temporal filtering applied in the Extract and/or PCA steps (depending on which appears to be more noisy).\n",
    "\n",
    "***\n",
    "\n",
    "__Pathology__: Too many syllables being generated in the syllable transition graph, all with skewed weights.\n",
    "\n",
    "__Reasons__: \n",
    "- The syllable transition probabilities may not have converged yet.\n",
    "- The model syllable-transition probability distribution is too sporatic; this can be caused by an underfitted model, too many emptied hidden states, or noisy input data (PCs and/or frames).\n",
    "- The input frames may either have too many dropped frames, or has been excessively temporally filtered, perturbing all the calculated syllable transition probability distributions.\n",
    "- The default `alpha` parameter, denoting syllable transition prior probability distributions, may not be representative of the real life data being sampled.\n",
    "\n",
    "__Solutions__: \n",
    "- Using a `robust` t-distribution model will aid in normalizing state posterior probability distributions.\n",
    "- Increase the number of training iterations.\n",
    "- Increasing `nfolds` and holding out data during training (1 fold per recording in the dataset). This will ensure that the model will not overfit by estimating validation log-likelihoods to compare to during training. It is recommended to hold out 1 fold per recording such that the validation set is properly representative of your training data.\n",
    "\n",
    "***\n",
    "\n",
    "For multiple groups, there will be a transition graph for each group, as well as a unified graph with different colors to identify the groups. __Note: remember to include all group names in the tuple to graph them.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import plot_transition_graph_command\n",
    "\n",
    "max_syllable = 20 # Maximum number of nodes in the transition graph\n",
    "groups = ('group1', 'group2') # Group(s) to graph, default graph if empty tuple\n",
    "output_filename = os.path.join(plot_path, 'transition') # name of the png file to be saved\n",
    "\n",
    "plot_transition_graph_command(index_filepath, model_path, config_filepath, max_syllable, groups, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Notebook End</h1></center>\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
