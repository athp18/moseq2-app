{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moseq2 App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1jCpb4AQKBasne2SoXw_S1kwU7R9esAF6\">\n",
    "\n",
    "MoSeq2 software toolkit for unsupervised modeling and characterization of animal behavior. Moseq transforms depth recordings of animals into a rich description of behavior as a series of reused and stereotyped motifs, also known as 'syllables'. \n",
    "\n",
    "This notebook begins with depth recordings (see the [data acquisition overview](#Data-Acquisition-Overview) below) and transforms this data through the steps of: \n",
    "\n",
    "- **[Extraction](#Raw-Data-Extraction)**: The animal is segmented from the background and its position and heading direction are aligned across frames.\n",
    "- **[Dimensionality reduction](#Principal-Component-Analysis-(PCA))**: Raw video is de-noised and transformed to low-dimensional pose trajectories using principal component analysis (PCA).\n",
    "- **[Model training](#ARHMM-Modeling)**: Pose trajectories are modeled using an autoregressive hidden Markov model (AR-HMM), producing a sequence of syllable labels.\n",
    "- **[Analysis](#Visualize-Analysis-Results)**: Model output is reported through visualization and statistical analysis.\n",
    "\n",
    "These are notebook shortcuts you can click on to navigate to that part of the notebook.\n",
    "\n",
    "### Resources\n",
    "We've provided links to the MoSeq documentation and recent publications that have used this software.\n",
    "- Documentation of all MoSeq functions (links to pdfs):\n",
    "    - [Wiki](https://github.com/dattalab/moseq2-app/wiki)\n",
    "    - [App](https://github.com/dattalab/moseq2-app/blob/release/Documentation.pdf)\n",
    "    - [Extract](https://github.com/dattalab/moseq2-extract/blob/release/Documentation.pdf)\n",
    "    - [PCA](https://github.com/dattalab/moseq2-pca/blob/release/Documentation.pdf)\n",
    "    - [Model](https://github.com/dattalab/moseq2-model/blob/release/Documentation.pdf)\n",
    "    - [Viz](https://github.com/dattalab/moseq2-viz/blob/release/Documentation.pdf)\n",
    "- Publications\n",
    "    - [Mapping Sub-Second Structure in Mouse Behavior](http://datta.hms.harvard.edu/wp-content/uploads/2018/01/pub_23.pdf)\n",
    "    - [The Striatum Organizes 3D Behavior via Moment-to-Moment Action Selection](http://datta.hms.harvard.edu/wp-content/uploads/2019/06/Markowitz.final_.pdf)\n",
    "    - [Revealing the structure of pharmacobehavioral space through motion sequencing](https://www.nature.com/articles/s41593-020-00706-3)\n",
    "    - [Q&A: Understanding the composition of behavior](http://datta.hms.harvard.edu/wp-content/uploads/2019/06/Datta-QA.pdf)\n",
    "    \n",
    "### Feedback\n",
    "\n",
    "If you would like to leave us feedback on how you liked or disliked this notebook,\n",
    "or if you want specific and new features, please fill out [this survey](https://forms.gle/FbtEN8E382y8jF3p6).\n",
    "    \n",
    "### Data Acquisition Overview\n",
    "MoSeq2 takes animal depth recordings as input. We we have developed a [data acquisition pipeline](https://github.com/dattalab/moseq2-app/wiki/Setup:-acquisition-software) for the second generation `Xbox Kinect` depth camera. We suggest following our [data acquisition tutorial](https://github.com/dattalab/moseq2-app/wiki/Acquisition) for doing recordings. \n",
    "\n",
    "MoSeq2 also accepts depth recordings from an `Azure Kinect` camera as well as the `Intel RealSense` depth camera. These cameras have their own means of acquiring data that is built-in to their respective Development Kits.\n",
    "\n",
    "**We recommend recording more than 10 hours of depth video (~1 million frames at 30 frames per second) to ensure quality MoSeq models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Zkd0tATi8r2ENHvN8OczIrEf4K8PFmhM\">\n",
    "\n",
    "### Check to see if you're running python from the correct conda enviornment\n",
    "\n",
    "If you performed the recommended installation, you should see the `sys.executable` path point to the python path within the `moseq2-app` environment. I.e.,\n",
    "```python\n",
    "import sys\n",
    "print(sys.executable)\n",
    "# /Users/wgillis/miniconda3/envs/moseq2-app/bin/python\n",
    "```\n",
    "\n",
    "### Check if the dependencies are found\n",
    "\n",
    "Run the following cell to check if `moseq2-app` is installed in your current conda kernel. The latest working version number is `0.1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import moseq2_app\n",
    "\n",
    "print('Python path:', sys.executable)\n",
    "print('MoSeq2 app version:', moseq2_app.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data file organization\n",
    "\n",
    "The currently accepted depth data extensions are `.dat`, `.tar.gz`, `.avi` and `.mkv`. `.dat` files are generated from our kinect2 data acquisition software, `.tar.gz` files are compressed depth files using the data acquisition software, `.avi` files are compressed `.dat` files using `moseq2-extract`, and `.mkv` files are generated from Microsoft's recording software for the Azure Kinect.\n",
    "\n",
    "After performing data acquisition, store all of your recording folders in the same folder (shown below). We recommend that you save a copy of this notebook in the same folder as your data to: (1) access them in this notebook, (2) have a unique MoSeq pipeline notebook for each project, and (3) enable the videos this notebook generates to load. \n",
    "\n",
    "```\n",
    ".\n",
    "└── Data_Directory/\n",
    "    ├── Main-MoSeq2-Notebook.ipynb (running)\n",
    "    ├── session_1/ ** - the folder containing all of a single session's data\n",
    "    ├   ├── depth.dat        # depth data - the recording itself\n",
    "    ├   ├── depth_ts.txt     # timestamps - csv/txt file of the frame timestamps\n",
    "    ├   └── metadata.json    # metadata - json file that contains the rodent's info (group, subjectName, etc.)\n",
    "    ...\n",
    "    ├── session_2/ **\n",
    "    ├   ├── depth.dat\n",
    "    ├   ├── depth_ts.txt\n",
    "    └── └── metadata.json\n",
    "\n",
    "```\n",
    "\n",
    "__Note: if your data was acquired using an Azure Kinect or Intel RealSense depth camera, you will not have `depth_ts.txt` or `metadata.json` in your session directories. MoSeq2-Extract will automatically generate metadata json files for all acquired sessions missing the file.__\n",
    "\n",
    "### Notebook Progress File\n",
    "\n",
    "This notebook generates a `progress.yaml` file that stores the filepaths to data generated from this notebook. For example it will contain paths to:\n",
    "- aggregated extractions\n",
    "- PC scores of the extractions\n",
    "- model results\n",
    "\n",
    "In the case that your notebook kernel is shutdown for any reason, you can load the progress file to 'restore' your progress. The progress file does **not** track MoSeq pipeline operations that were executed outside of this notebook (for example, if you were to run PCA using the CLI). You can manually modify the paths in the progress file or loaded dictionary to record filepaths for these external operations.\n",
    "\n",
    "__To restore previously computed variables, look for the cells following the `Restore Progress Variables` label.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Progress Variables\n",
    "\n",
    "- Use this cell to load your notebook analysis progress. We recommend running this notebook from the folder where your data is located so the generated media will display properly. In that case, you can specify the `base_dir` as `./` (or the current folder).\n",
    "\n",
    "`check_progress` will print progress bars for each pipeline step in the notebook. \n",
    "- The extraction progress bar indicates total the number of extracted sessions detected in the provided `base_dir` path.\n",
    "- It prints the session names that haven't been extracted. __Note: the progress does not reflect the contents of the aggregate_results/ folder.__\n",
    "- The remainder of the progress bars are derived from reading the paths in the `progress_paths` dictionary, filling up the bar if the included paths are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import check_progress\n",
    "\n",
    "base_dir = './' # Add the path to your data folder here.\n",
    "# We recommend that you run this notebook in the same folder as your data. In that case, you don't have to change base_dir\n",
    "progress_filepath = join(base_dir, 'progress.yaml')\n",
    "\n",
    "progress_paths = check_progress(base_dir, progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Configuration Files\n",
    "\n",
    "The `config.yaml` will be used to hold all configurable parameters for all steps in the MoSeq pipeline. The parameters used will be added to this file as you progress through the notebook. You can then use it to run an identical pipeline in future analyses, or directly configure parameters from there when debugging cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_extract.gui import generate_config_command\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "config_filepath = join(base_dir, 'config.yaml')\n",
    "\n",
    "print(f'generating file in path: {config_filepath}')\n",
    "generate_config_command(config_filepath)\n",
    "progress_paths = update_progress(progress_filepath, 'config_file', config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A configuration file has been created in the base directory (depicted below).\n",
    "\n",
    "```\n",
    ".\n",
    "└── Data_Directory/\n",
    "    ├── config.yaml **\n",
    "    ├── session_1/ \n",
    "    ├   ├── depth.dat        \n",
    "    ├   ├── depth_ts.txt     \n",
    "    ├   └── metadata.json    \n",
    "    ...\n",
    "    ├── session_2/ \n",
    "    ├   ├── depth.dat\n",
    "    ├   ├── depth_ts.txt\n",
    "    └── └── metadata.json\n",
    "```\n",
    "\n",
    "### Download a Flip File\n",
    "\n",
    "MoSeq2 uses a flip classifier to guarantee that the mouse is always oriented facing east in the extractions. The flip classifiers we provide __are trained for experiments run with C57BL/6 mice using with Kinect v2 depth cameras__.\n",
    "\n",
    "If your dataset does not work with these flip classifiers, consider training your own. Click [this link](https://github.com/dattalab/moseq2-app/tree/jupyter/) to view the flip-classifier training notebooks. Once you have it trained, add the path to the `config.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import download_flip_command\n",
    "# selection=0 - large mice with fibers (default)\n",
    "# selection=1 - adult male C57s\n",
    "# selection=2 - mice with Inscopix cables\n",
    "download_flip_command(base_dir, config_filepath, selection=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Extraction\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1vH3fs0Xu5c4gyzFChzV6r_5JGi6JReW2\">\n",
    "\n",
    "The MoSeq2-Extract module is used to segment the mouse from the background and create data files for dimensionality reduction and modeling. The resulting `.h5` and `.yaml` data files stored in a `proc` subfolder created in the session's folder by default.`.mp4` videos are also generated and primarily used for quality assurance after extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive ROI Detection Tool\n",
    "\n",
    "Use this interactive tool to detect your recordings' mouse extraction configuration parameters prior to extracting all of your data. This tool can also be used to catch possibly corrupted or inconsistent sessions, and handle diagnosing ROI detection/extraction errors and saving unique parameter sets for solved edge cases.\n",
    "\n",
    "<center><h3>Widget Guide</h3></center>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1kgMrz-Py4x1k-WwyoHS95cbPRMdJuwvy\">\n",
    "\n",
    "<h3>Instructions</h3>\n",
    "<br>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"width: 45%;\">\n",
    "            <ol>\n",
    "                <li style=\"text-align:left; font-size:15px\">\n",
    "                    Click on any row in the session selector to load that session's data to the view.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:15px\">\n",
    "                    If the indicator next to the session's name is green, then the session is considered ready for extraction. A red indicator can either mean the session has not been checked yet, or its unique parameter set is incorrect.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:15px\">\n",
    "                    Adjust the Depth Range Selector to include the depth range of the detected bucket floor distance (which can be found by hovering over the Background image with your mouse).\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:15px\">\n",
    "                    If the mouse seems to be cropped when at the bucket edge, increase the dilate iterations to enlarge the size of the included floor area.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:15px\">\n",
    "                    Use the Rodent Height Threshold Slider to remove any noise/speckle from the bucket floor or walls. \n",
    "                    <ul>\n",
    "                        <li style=\"text-align:left; font-size:15px\">\n",
    "                            Ensure the Min-Height is small enough to only filter out floor reflects and the mouse tail. (Filtering out the tail helps keep the cropped image oriented facing east). \n",
    "                        </li>\n",
    "                        <li style=\"text-align:left; font-size:15px\">\n",
    "                            Ensure the Max-Height is large enough to include the highest possible distance the mouse can rear up the bucket without including any possible extrema anomalies from the bucket walls.\n",
    "                        </li>\n",
    "                        <li style=\"text-align:left; font-size:15px\">\n",
    "                            To explore the session's mouse heights, hover over either of the bottom 2 plots to view the mouse height.\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:15px\">\n",
    "                    Use the current frame selector slider to change the displayed session frame in the bottom 2 plots.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:15px\">\n",
    "                    Change the frame range slider values to adjust the segments of the video to extract, then click the \"Extract Sample\" button to trigger an extraction and view the results.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:15px\">\n",
    "                    Once you have found a parameter set that is satisfactory, click the \"Check All Sessions\" button to test the parameters on all of the found sessions, flagging any outliers.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:15px\">\n",
    "                    In the case that no sessions were flagged, click the \"Save Parameters\" button to save the currently displayed and configured parameters to the inputted config file, as well as the individual session config file.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:15px\">\n",
    "                    Otherwise, if a session is flagged, click on the session in the Session Selector, view the text indicator for the the error details, then adjust the parameters until the session passes, and finally save the parameters.\n",
    "                    <ul>\n",
    "                        <li style=\"text-align:left; font-size:15px\">\n",
    "                            If a session appears to have a passing ROI+extraction but is indicated as a flagged, you can use the \"Mark Passing\" button to manually accept the session's parameter set. This is the case when a session's ROI area pixel count is outside of the acceptance threshold when compared with the latest passing session's pixel count.\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ol>\n",
    "            <p style=\"text-align:left; font-size:15px\">\n",
    "                    Note that you can also manually edit the slider values by clicking on the numbers to activate keyboard editing.\n",
    "            </p>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img style=\"display:contents\" src=\"https://drive.google.com/uc?export=view&id=1jwLb1Tzpx0iAl89RF7z9bnI-sjFftTDh\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "__Notice: if cell seems to be running out of memory after first use, set `compute_all_bgs=False` to reduce the memory pressure.__\n",
    "\n",
    "If you are using an alternative `flip_classifier`, `crop_size`,  or would like to edit other extraction preprocessing parameters, use the following format:\n",
    "```\n",
    "# Read in the config file\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "    \n",
    "# Edit its contents\n",
    "config_data['crop_size'] = (100, 100) # new crop size\n",
    "config_data['flip_classifier'] = './alternative-flip-classifier.pkl' # updated flip classifier path\n",
    "...\n",
    "config_data['gaussfilter_space'] = [2.5, 2] # new spatial filtering kernel size\n",
    "config_data['medfilter_time'] = [3] # new temporal filtering kernel size\n",
    "\n",
    "# Filtering out head-fixed cables?\n",
    "config_data['cable_filter_iters'] = 10 # new number of cable filtering iterations\n",
    "config_data['cable_filter_size'] = (7, 7) # new spatial filter kernel size\n",
    "\n",
    "# Write the changes back to the config file before running the interactive_roi_detector command.\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import ruamel.yaml as yaml\n",
    "from moseq2_app.main import interactive_roi_detector\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "session_config_path = join(progress_paths['base_dir'], 'session_config.yaml')\n",
    "progress_paths = update_progress(progress_filepath, 'session_config', session_config_path)\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "config_data['camera_type'] = 'kinect' # 'kinect', 'azure' or 'realsense'\n",
    "config_data['crop_size'] = (80, 80)\n",
    "\n",
    "# if using azure or realsense, increase the noise_tolerance\n",
    "config_data['noise_tolerance'] = 30\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "interactive_roi_detector(base_dir, progress_paths, compute_all_bgs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Progress Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "base_dir = './' # User-defined absolute path\n",
    "\n",
    "progress_filepath = join(base_dir, 'progress.yaml')\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Session(s)\n",
    "\n",
    "- If `extract_all=False`, the cell will prompt you to choose whether you would like to extract individual sessions, (empty string to extract all of them). Enter your selection, and then wait for the extraction to complete to preview them.\n",
    "- If `skip_extracted=True`, the command will only search for (and list) sessions that have not been previously extracted.\n",
    "\n",
    "__Note: If sessions are not listed when running the cell, ensure your selected extension matches that of your depth files.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import extract_found_sessions\n",
    "\n",
    "# include the file extensions for the depth files you would like to search for and extract.\n",
    "extensions = ['.avi'] # and/or .dat, .mkv\n",
    "\n",
    "extract_found_sessions(base_dir, progress_paths['config_file'], extensions, extract_all=True, skip_extracted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what your directory structure should look like once the process is complete:\n",
    "\n",
    "```\n",
    ".\n",
    "├── config.yaml\n",
    "├── session_1/\n",
    "├   ...\n",
    "├   └── proc/ **\n",
    "├   ├   ├── roi.tiff\n",
    "├   ├   ...\n",
    "├   ├   ├── results_00.yaml+h5 ** (represents .h5 and .yaml files)\n",
    "├   └   └── results_00.mp4 ** (extracted video)\n",
    "└── session_2/\n",
    "├   ...\n",
    "├   └── proc/ **\n",
    "├   ├   ├── roi.tiff\n",
    "├   ├   ...\n",
    "├   ├   ├── results_00.yaml+h5 **\n",
    "└   └   └── results_00.mp4 **\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Extraction Validation Tests\n",
    "\n",
    "Once all the extractions are complete, run this cell to run some data validation tests. The tests will either emit an error or a warning. \n",
    "- An __error__ indicates that the session is corrupted in some way and should not be included in the following pipeline steps.\n",
    "- The __warning__ tests are meant to primarily give a better idea of how standarized the dataset is, with respect to the captured sizes and certain behavior metrics. Allowing you to make a more conscious decision on whether to include data in the subsequent steps.\n",
    "  - Depending on the experimental conditions, a warning can indicate that the session may need to be inspected prior to continuing into the PCA step. \n",
    "  - Experimenters using sedating or stimulating drugs (or experiments of the like) may ignore position and scalar related warning anomalies. Since the conditions may be expected to cause divergences in general motility, the warnings can therefore be interpretted as a confirmation that the captured experimental group data differs from that of the controls. \n",
    "\n",
    "Error raising tests: \n",
    "- Count Dropped Frames: an error is raised if a session is missing more than 5% of the frames, given the session's accompanying timestamp file exists.\n",
    "- Missing Mouse Check: raises an error if a mouse is missing from the video for any reason for >5% of the session's total frames.\n",
    "- Scalar Anomaly: raises an error if >5% of a session's computed scalar values are NaN.\n",
    "\n",
    "Warning raising tests:\n",
    "- Size Anomaly: Warning is raised when a mouse's captured body-area is less than 2 standard deviations from the mean size throughout the session.\n",
    "- Scalar Anomaly: Warning is raised with a list of specified scalars if some of the mean scalar values are outside of the 1st through 3rd quartile range. \n",
    "- Position Anomaly: There are two cases that raise warnings:\n",
    "    1. Mouse is stationary for >5% of the session.\n",
    "    2. Mouse's position PDF is at least 2 standard deviations away from the mean of all the sessions, measured using Kullback–Leibler divergence.\n",
    "     - This anomaly can indicate that a mouse has explored a much larger or smaller region of the arena compared to the remainder of the dataset.\n",
    "\n",
    "To diagnose certain scalar anomalies, use the [Scalar Summary Cell](#Compute-Scalar-Summary) below to graph any desired scalar value.\n",
    "   \n",
    "The following cell will run the tests and emit the warnings and errors if any are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import validate_extractions\n",
    "\n",
    "validate_extractions(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Extractions\n",
    "\n",
    "Run this cell to launch an interactive session selection widget to load and preview any extracted session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import preview_extractions\n",
    "\n",
    "preview_extractions(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate your results into one folder and generate an index file.\n",
    "\n",
    "The following cell will search through your base directory for the `proc/` folders in each session, and copy them all in a single directory. \n",
    "\n",
    "Then it will generate the `moseq2-index.yaml` file by searching for all the metadata found in the `results_00.h5`/`results_00.yaml` files, and consolidate all that information in one file, assigning each session to a `default` group.\n",
    "\n",
    "The `aggregate_results/` folder contains all the data you need to run the rest of the pipeline. The PCA will only train on data included in that folder, and same for the model.\n",
    "\n",
    "The `moseq2-index.yaml` file contains all the sessions+metadata that are included in `aggregate_results/`, it will also be heavily used in the visualization steps to plot different mouse and/or group statistics.\n",
    "\n",
    "__Important Note: The index file contains UUIDs for each session which are newly generated during the extraction step. These UUIDs are referenced throughout the pipeline, so if you re-extract a session, ensure that you re-aggregate your data to ensure all the UUIDs are up-to-date BEFORE the PCA step.__ Not updating the index file could cause `KeyError`s to occur when referencing the extracted data and/or the pca_scores with the model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "from moseq2_extract.gui import aggregate_extract_results_command\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "recording_format = '{start_time}_{session_name}_{subject_name}' # filename formats for the copied extracted data files\n",
    "\n",
    "# directory NAME to save all metadata+extracted videos to with above respective name format\n",
    "aggregate_results_dirname = 'aggregate_results/'\n",
    "\n",
    "train_data_dir = join(base_dir, aggregate_results_dirname)\n",
    "update_progress(progress_filepath, 'train_data_dir', train_data_dir)\n",
    "\n",
    "# the subpath indicates to only aggregate extracted session paths with that subpath, only change if aggregating data from a different location\n",
    "index_filepath = aggregate_extract_results_command(base_dir, recording_format, aggregate_results_dirname)\n",
    "progress_paths = update_progress(progress_filepath, 'index_file', index_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregate results folder will be saved in your base directory,\n",
    "resulting in the following directory (sample) structure where the base directory contains the notebook:\n",
    "\n",
    "```\n",
    ".\n",
    "├── aggregate_results/ **\n",
    "├   ├── session_1_results_00.h5+yaml ** # session 1 compressed extraction + metadata \n",
    "├   ├── session_1_results_00.mp4 ** # session 1 extracted video\n",
    "├   ├── session_2_results_00.h5+yaml ** # session 2 compressed extraction + metadata \n",
    "├   └── session_2_results_00.mp4 ** # session 2 extracted video\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml ** # index file\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n",
    "\n",
    "__Notice your index file has also been generated in your base directory, and it's general initial structure is shown below.__\n",
    "\n",
    "```\n",
    "files:\n",
    "- group: default\n",
    "  metadata:\n",
    "      ...\n",
    "      SubjectName: control_mouse_1\n",
    "      SessionName: day1\n",
    "  path: [/path/to/results_00.h5, /path/to/results_00.yaml]\n",
    "  uuid: 11dc6c26-0de6-4145-9bcc-a9ec200b667e\n",
    "- group: default\n",
    "  metadata:\n",
    "      ...\n",
    "      SubjectName: drug_mouse_1\n",
    "      SessionName: day1\n",
    "  path: [/path/to/results_00.h5, /path/to/results_00.yaml]\n",
    "  uuid: 16d76d24-35c3-4ca8-aedc-c12456abb4c4\n",
    "...\n",
    "pca_path: ./_pca/pca_scores.h5\n",
    "```\n",
    "\n",
    "## Specify Groups\n",
    "\n",
    "MoSeq using groups in the moseq2-index.yaml file to indicate whether your collected sessions are representing a single experimental group, or many different groups that you would like to compare while modeling and visualizing.\n",
    "\n",
    "Specifying groups also helps distinguishing the data points in the scalar and heatmap plots.\n",
    "\n",
    "The index file requires that all your sessions have a metadata.json file in order to successfully assign each recorded subject or session to a group.\n",
    "\n",
    "Use this GUI to input the group names associated with all the sessions. \n",
    "- You can click on the column names to sort the index file.\n",
    "- You can use your keyboard to select multiple rows.\n",
    "- Enter your desired group name in the text input and click `Set Group` to update all the associated session rows.\n",
    "- Once all your groups are set, click the `Update Index File` button to update the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import interactive_group_setting\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "interactive_group_setting(progress_paths['index_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Scalar Summary\n",
    "\n",
    "Use the following command to compute some scalar summary information about your modeled groups, such as average velocity, height, etc.\n",
    "\n",
    "This graph is meant to give you an idea of whether your extractions were consistent throughout the sessions. If you have a large standard deviation in mouse length/width when your mice are all the same size in reality, then there may have been an error in the extraction or acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "from moseq2_viz.gui import plot_scalar_summary_command\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "# Prefix name of the saved scalar position and summary graphs\n",
    "output_file = join(progress_paths['plot_path'], 'scalars') \n",
    "\n",
    "# Scalars to display\n",
    "show_scalars = ['velocity_2d_mm', 'velocity_3d_mm',\n",
    "                'height_ave_mm', 'width_mm', 'length_mm']\n",
    "\n",
    "colors = None # None for default colors; otherwise use list\n",
    "\n",
    "scalar_df = plot_scalar_summary_command(progress_paths['index_file'], output_file, \n",
    "                                        show_scalars=show_scalars, \n",
    "                                        colors=colors)\n",
    "\n",
    "# Graph the output\n",
    "display(Image(output_file+'_summary.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Position Heatmaps For Each Session\n",
    "Each heatmap will be titled with the session's subject name and group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "from moseq2_viz.gui import plot_verbose_position_heatmaps\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'session_heatmaps') \n",
    "plot_verbose_position_heatmaps(progress_paths['index_file'], output_file)\n",
    "\n",
    "# Graph the output\n",
    "display(Image(output_file+'.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Group Mean Position Summary\n",
    "\n",
    "These plots will give you a good idea of the general captured hyperactivity level and amount of area exploration in each of your experimental groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "from moseq2_viz.gui import plot_mean_group_position_heatmaps_command\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'group_heatmaps') \n",
    "plot_mean_group_position_heatmaps_command(progress_paths['index_file'], output_file)\n",
    "\n",
    "# Graph the output\n",
    "display(Image(output_file+'.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Principal Component Analysis (PCA)</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "Once the data has been extracted, compute Principal Components (PCs) of the data to perform dimensionality reduction on the data going into the modeling step.\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1I1WcfEwzpfwIxNYStX7swLAIvjQEVApy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Progress Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "base_dir = './' # User-defined absolute path\n",
    "\n",
    "progress_filepath = join(base_dir, 'progress.yaml')\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training PCA\n",
    "\n",
    "Train a PCA model on your extracted data to acquire the Principal Components that explain the largest possible variance in your dataset. If the resulting Principal Components look smooth with well defined regions, and your Scree plot shows an explained variance of 90% or above in less than 10 PCs then the PCA model is properly trained. Otherwise, consult the [pathologies below](#Possible-PCA-Pathologies) to solve any issues.\n",
    "\n",
    "- You can check your distributed data processing progress while the PCA operations are taking place by checking the [dask server](https://localhost:8787/). This is only meant for optimization or debugging.\n",
    "\n",
    "- If there are occlusions over the rodent in your extractions (shown below), set `config_data['missing_data'] = True` to recompute the missing data.\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1y9_aRzrE3PS34GC2LJe3zuEXvms04S90\">\n",
    "\n",
    "__Using SLURM?__ Add and edit the following config parameters to spawn dask workers according to your specifications:\n",
    "```\n",
    "# Read in the config file\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "    \n",
    "# Edit its contents\n",
    "config_data['cluster_type'] = 'slurm'\n",
    "\n",
    "config_data['nworkers'] = 8 # number of spawned jobs\n",
    "config_data['queue'] = 'short' # partition\n",
    "config_data['memory'] = '40GB' # amount of memory per worker\n",
    "config_data['cores'] = 1 # number of cores per worker\n",
    "config_data['wall_time'] = '01:00:00' # worker time limit\n",
    "\n",
    "# Write the changes back to the config file before running the interactive_roi_detector command.\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import ruamel.yaml as yaml\n",
    "from moseq2_pca.gui import train_pca_command\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "pca_filename = 'pca' # Name of your PCA model h5 file to be saved\n",
    "pca_dirname = '_pca/' # Directory to save your computed PCA results\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "# PCA parameters you may need to configure\n",
    "config_data['gaussfilter_space'] = (1.5, 1) # Spatial filter for data (Gaussian)\n",
    "config_data['medfilter_space'] = [0] # Median spatial filter\n",
    "config_data['medfilter_time'] = [0] # Median temporal filter\n",
    "\n",
    "# If dataset includes head-attached cables, set missing_data=True\n",
    "config_data['missing_data'] = False # Set True for dataset with missing/dropped frames to reconstruct respective PCs.\n",
    "config_data['missing_data_iters'] = 10 # Number of times to iterate over missing data during PCA\n",
    "config_data['recon_pcs'] = 10 # Number of PCs to use for missing data reconstruction\n",
    "\n",
    "# Dask Configuration\n",
    "config_data['dask_port'] = '8787' # port to access Dask Dashboard\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'pca_dirname', join(base_dir, pca_dirname))\n",
    "\n",
    "# will train on data in aggregate_results/\n",
    "train_pca_command(progress_paths, pca_dirname, pca_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, a new directory titled `_pca` will be created containing all your PCA data.\n",
    "```\n",
    ".\n",
    "├── _pca/ **\n",
    "├   ├── pca.h5 ** # pca model compressed file\n",
    "├   ├── pca.yaml  ** # pca model YAML metadata file\n",
    "├   ├── pca_components.png **\n",
    "├   └── pca_scree.png **\n",
    "├── aggregate_results/\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "\n",
    "```\n",
    "\n",
    "View your `computed PCs` and `scree plot` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "images = [join(progress_paths['pca_dirname'], 'pca_components.png'), \n",
    "          join(progress_paths['pca_dirname'], 'pca_scree.png')]\n",
    "for im in images:\n",
    "    display(Image(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible PCA Pathologies\n",
    "\n",
    "<table style=\"width: 100%;\">\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th></th>\n",
    "      <th>Good PCA Output Examples</th>\n",
    "      <th style=\"text-align:center;\">Bad Scree Plot Example</th>\n",
    "      <th style=\"text-align:center;\">Bad Principal Components Example</th>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Pathology Description</th>\n",
    "      <th style=\"text-align:center;\"></th>\n",
    "      <td style=\"text-align:center;\">Cannot achieve a explained variance of over 90% from less than 15 Principal Components (PCs).</td>\n",
    "      <td style=\"text-align:center;\">Graphed PCs look overprocessed, or are not representative of realistic mouse body regions.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Reference Examples</th>\n",
    "      <th style=\"text-align:center;\">\n",
    "        <ul>\n",
    "            <li>Components<br>\n",
    "                <img src=\"https://drive.google.com/uc?export=view&id=1dX5Gpd3PKL4vfVviLeP0CqBrz9PW37Au\" width=350 height=350></li><br><br>\n",
    "            <li>Scree Plot<br>\n",
    "                <img src=\"https://drive.google.com/uc?export=view&id=12uqsBYuWCjpUQ6QrAjo35MnwYDzHqnge\" width=350 height=350>\n",
    "            <br>\"90.65% in 7 PCs\"</li>\n",
    "        </ul>\n",
    "      </th>\n",
    "      <td><img src=\"https://drive.google.com/uc?export=view&id=14OwThgsf2GXnrl3-9TXEMvF3PDxmRsHE\" width=350 height=350></td>\n",
    "      <td><img src=\"https://drive.google.com/uc?export=view&id=1d35zKWiT7bkWbNNAon_JdSjKyVgcHHzi\" width=350 height=350></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Image Analysis Solutions</th>\n",
    "      <th style=\"text-align:center;\"></th>\n",
    "      <td>\n",
    "        <ul>\n",
    "          <li style=\"text-align:left;\">Check if the crop size is too large, if so, decrease it and re-extract your data.</li>\n",
    "          <li style=\"text-align:left;\">Try (incrementally) adjusting the spatial and temporal filtering kernel sizes in the PCA step. Generally, increasing temporal smoothing will aid in increasing explained variance, however overfiltering will hinder ARHMM reliability.</li>\n",
    "        </ul>\n",
    "      </td>\n",
    "      <td>\n",
    "          <ul>\n",
    "              <li style=\"text-align:left;\">Ensure that an appropriate amount of spatial and temporal filtering is applied.</li>\n",
    "              <li style=\"text-align:left;\">If there are  missing frames, apply and appropriate amount of temporal filtering, and a proper amount of PCs are being reconstructed (recon_pcs is set to the appropriate amount of PCs).</li>\n",
    "          </ul>\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">General Solutions</th>\n",
    "      <th style=\"text-align:center;\"></th>\n",
    "      <td style=\"text-align:center;\">\n",
    "          <ul>\n",
    "          <li style=\"text-align:left;\">Try setting missing_data=True. Using an iterative PCA to reconstruct the PCs can aid in increasing the explained variance ratio.</li>\n",
    "          <li style=\"text-align:left;\">Increase the size of your dataset. If your dataset is too small, it may contribute to overprocessing PCs as well.</li>\n",
    "        </ul>\n",
    "      </td> <!-- G -->\n",
    "      <td style=\"text-align:center;\">Acquire and extract more data, then try with more data.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Principal Component Scores\n",
    "\n",
    "Apply your trained PCA model using your computed principal components to compute your PC Scores. PC Scores are a result of multiplying the input videos by the transpose of the principal components. They indicate the \"score\" or explainability ratio (AKA Pose Trajectory) of each computed Principal Component in each frame of the inputted videos.\n",
    "\n",
    "These are the values that the ARHMM will train on, as the PCA Scores have a [positive semi-definite matrix](https://en.wikipedia.org/wiki/Definite_symmetric_matrix) property, which is required by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_pca.gui import apply_pca_command\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "scores_filename = 'pca_scores' # name of the scores file to compute and save\n",
    "\n",
    "scores_file = join(progress_paths['pca_dirname'], scores_filename+'.h5') # path to input PC scores file to model\n",
    "progress_paths = update_progress(progress_filepath, 'scores_path', scores_file)\n",
    "\n",
    "apply_pca_command(progress_paths, scores_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, you will have a pca_scores file saved in your pca directory. (Example shown below)\n",
    "```\n",
    ".\n",
    "├── _pca/\n",
    "├   ├── pca.h5\n",
    "├   ├── pca.yaml\n",
    "├   ├── pca_scores.h5  ** # scores file\n",
    "├   ├── pca_components.png\n",
    "├   └── pca_scree.png\n",
    "├── aggregate_results/\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "\n",
    "```\n",
    "\n",
    "## (Optional) Computing Model-Free Syllable Changepoints\n",
    "\n",
    "This is an optional step used to aid in determining model-free syllable lengths; which are general approximations of the duration of respective body language syllables. The Changepoint distribution is also meant to provide a clearer representation of the accuracy of the PCA-fit.\n",
    "\n",
    "A good Changepoint graph should show a smooth left-skewed histogram distribution of changepoint durations, with a CPE curve accurately fit to the histogram. Having a proper left-skewed distribution indicates that the Principal Components accurately represent your extracted data. If that is not the case, consult the [below pathologies](#Possible-Model-Free-Changepoints-Pathologies).\n",
    "\n",
    "The x-axis denotes the time taken to transition from one pose to another. The y-axis indicates the probability of a changepoint having the duration at each point on x. Ideally, the changepoint mode/curve-maximum should be roughly 0.3 seconds (300 ms).\n",
    "\n",
    "__Note: the parameters below have been preconfigured to best process C57 mouse data, and have not been tested for other species. Configure them at your own risk.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "from moseq2_pca.gui import compute_changepoints_command\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "changepoints_filename = 'changepoints' # name of the changepoints images to generate\n",
    "\n",
    "# Changepoint computation parameters you may want to configure\n",
    "config_data['threshold'] = 0.5 # Peak threshold to use for changepoints\n",
    "config_data['dims'] = 300 # Number of random projections to compare the computed principal components with\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'changepoints_path', changepoints_filename)\n",
    "compute_changepoints_command(progress_paths['train_data_dir'], progress_paths, changepoints_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changepoints plot will be generated and saved in the pca directory (example below).\n",
    "\n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├   ├── pca.h5\n",
    "├   ├── pca_scores.h5\n",
    "├   ...\n",
    "├   └── changepoints_dist.png **\n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n",
    "\n",
    "View your changepoints distance plot (if the text is too small, check the pdf file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(join(progress_paths['pca_dirname'], progress_paths['changepoints_path']+'_dist.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Model-Free Changepoints Pathologies\n",
    "\n",
    "<table style=\"width: 100%;\">\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th></th>\n",
    "      <th style=\"text-align:center;\">Good Changepoint Analysis Example</th>\n",
    "      <th style=\"text-align:center;\">Poor Changepoints Analysis Example</th>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Pathology Description</th>\n",
    "      <td style=\"text-align:center;\"></td>\n",
    "      <td style=\"text-align:center;\">Model-free syllable changepoint distances distribution is incorrectly skewed/too sparse and/or changepoint mode duration is less than 0.2s</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Reference Example</th>\n",
    "      <td><img src=\"https://drive.google.com/uc?export=view&id=1sMkSB34bGbOimumN6Gg1-zV2Hk98v2Zy\" width=350 height=350></td>\n",
    "      <td><img src=\"https://drive.google.com/uc?export=view&id=1S-ALkPmb8sBZGkKmJ7Q3-RdxAbfS0PWV\" width=350 height=350></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">General Solutions</th>\n",
    "      <td style=\"text-align:center;\"></td>\n",
    "      <td>\n",
    "          <ul>\n",
    "              <li style=\"text-align:left;\">Try retraining the PCA with adjusted spatial and temporal filtering kernel sizes.</li>\n",
    "              <li style=\"text-align:left;\">Ensure your extracted data is correct with minimal flips. If the extraction version of the mouse is too noisy, then the PC trajectories cannot be accurately applied to the data.</li>\n",
    "              <li style=\"text-align:left;\">Get more data and try again.</li>\n",
    "          </ul>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "***\n",
    "<center><h1>ARHMM Modeling</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "In order to train your ARHMM (Auto-Regressive Hidden Markov Model), you will use your computed PC scores as your input data, and specify whether you are modeling a single experimental group for observational research, or modeling multiple different groups (e.g. control vs. experimental groups) for comparative analysis.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=15uoeWDTn8fbcau2jErJEVFUt2iJyadmo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Progress Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "base_dir = './' # User-defined absolute path\n",
    "\n",
    "progress_filepath = join(base_dir, 'progress.yaml')\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ARHMM\n",
    "__Note: when loading a model checkpoint, ensure the all the inputted parameters (especially the selected groups) are identical to that of the checkpoint. Otherwise the model will not train.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import ruamel.yaml as yaml\n",
    "from moseq2_model.gui import learn_model_command\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "modeling_session_path = 'saline-amphetamine/'\n",
    "model_name = 'model.p'\n",
    "\n",
    "session_path = join(base_dir, modeling_session_path)\n",
    "model_path = join(session_path, model_name) # path to save trained model\n",
    "\n",
    "select_groups = False # select specific groups to model; if False, will model all data as is in moseq2-index.yaml\n",
    "\n",
    "# model saving freqency (in interations); will create a checkpoints/ directory containing checkpointed models\n",
    "checkpoint_freq = -1\n",
    "use_checkpoint = False # resume training from latest saved checkpoint\n",
    "\n",
    "# Advanced modeling parameters\n",
    "hold_out = False # boolean to hold out data subset during the training process\n",
    "nfolds = 2 # (if hold_out==True): number of folds to hold out during training; 1 fold per session\n",
    "\n",
    "npcs = 10  # number of PCs being used\n",
    "max_states = 100 # number of maximum states the ARHMM can end up with\n",
    "\n",
    "# use robust-ARHMM with t-distribution -> yields less states/syllables if True, \n",
    "# used to constrict accepted behavioral variability\n",
    "robust = True \n",
    "\n",
    "# separate group transition graphs; set to True if ngroups > 1\n",
    "separate_trans = True \n",
    "\n",
    "num_iter = 100 # number of iterations to train model\n",
    "\n",
    "# syllable length probability distribution prior; (None, int or 'scan'); if None, kappa=nframes\n",
    "kappa = None \n",
    "\n",
    "# if kappa == 'scan', optionally set bounds to scan kappa values between, in either a linear or log-scale.\n",
    "scan_scale = 'log' # or linear\n",
    "min_kappa = None\n",
    "max_kappa = None\n",
    "\n",
    "# total number of models to spool\n",
    "n_models = 5\n",
    "\n",
    "# Select platform to run models on\n",
    "cluster_type = 'local' # currently supported cluster_types = 'local' or 'slurm'\n",
    "run_cmd = False # if True, runs the commands via os.system(...)\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'model_path', model_path)\n",
    "progress_paths = update_progress(progress_filepath, 'model_session_path', session_path)\n",
    "\n",
    "learn_model_command(progress_paths, hold_out=hold_out, nfolds=nfolds, num_iter=num_iter, max_states=max_states,\n",
    "                    npcs=npcs, kappa=kappa, separate_trans=separate_trans, robust=robust,\n",
    "                    checkpoint_freq=checkpoint_freq, use_checkpoint=use_checkpoint, select_groups=select_groups,\n",
    "                    cluster_type=cluster_type, min_kappa=min_kappa, scan_scale=scan_scale,\n",
    "                    max_kappa=max_kappa, n_models=n_models, run_cmd=run_cmd, output_dir=modeling_session_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, your model will be saved in your base directory (shown below). \n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── modeling_session/ ***\n",
    "├   └── model.p ***\n",
    "├── moseq2-index.yaml/\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Notebook Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "base_dir = './' # User-defined absolute path\n",
    "\n",
    "progress_filepath = join(base_dir, 'progress.yaml')\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Best Model Fit\n",
    "\n",
    "Use this feature to determine whether the trained model has captured median syllable durations that match the principal components changepoints.\n",
    "\n",
    "This feature can also return the best model from a list of models found in the `progress_paths['model_session_path']`.\n",
    "\n",
    "Below are examples of some comparative distributions that you can expect when using this tool:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img height=400 width=400 src=\"https://drive.google.com/uc?export=view&id=1ENQVOFcM7moN_k6G_hVysIAaH-smRnEd\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img height=400 width=400 src=\"https://drive.google.com/uc?export=view&id=1rtfzkBGISuu8fpGNLNOTt9881Hgg_rXC\">\n",
    "        </td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "from moseq2_viz.gui import get_best_fit_model\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'model_vs_pc_changepoints')\n",
    "\n",
    "best_model_fit = get_best_fit_model(progress_paths, plot_all=True)\n",
    "progress_paths = update_progress(progress_filepath, 'model_path', best_model_fit)\n",
    "display(Image(output_file+'.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End and User Survey\n",
    "\n",
    "Please take some time to tell us your thoughts about this notebook:\n",
    "**[user feedback survey](https://forms.gle/FbtEN8E382y8jF3p6)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.993px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
