{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to MoSeq2-Notebook\n",
    "\n",
    "### Run all of the MoSeq tools in a containerized notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, run the following jupyter notebook to ensure MoSeq2 is installed and running smoothely on your machine.\n",
    "[SETUP NOTEBOOK](http://localhost:8889/notebooks/MoSeq2_Step_0.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, copy this notebook file to your recorded session directory. You will create a new copy of this notebook for each analysis session.\n",
    "\n",
    "You can use the cells below to ensure your sessions are found in this notebook, and the correct python is being used in your designated conda env (where your moseq2 tools are installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you are running the python version located in your corresponding conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the files and env are correct, run the cell below to generate an extraction configuration file to aid in your extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import *\n",
    "\n",
    "output_filepath = './config.yaml'\n",
    "\n",
    "generate_config_command(output_filepath, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a Flip file\n",
    "In order to ensure your extraction is smooth and invariant to the mouse's orientation, we recommend using a flip-classifier to aid keeping the mouse oriented throughout the extraction.\n",
    "\n",
    "The flip file indices are as follows:\n",
    "* [0] - Large mice with fibers.\n",
    "* [1] - Adult male c57s.\n",
    "* [2] - Mice with Inscopix cables.\n",
    "\n",
    "Enter your desired index in the variable assignment below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import *\n",
    "\n",
    "output_dir = './'\n",
    "selected_index = 1\n",
    "download_flip_command(output_dir, selected_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is done, be sure to copy the name of the flip file into the flip_classifier field in the `config.yaml` file. (You can find and edit any files using this Jupyter session. Simply click on the desired file in the Home Page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Extraction Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract your current session's depth file, input it's path below and run the cell.\n",
    "\n",
    "__Note__: In order to configure your extraction parameters, you must edit your config.yaml file prior to executing the extract command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import *\n",
    "\n",
    "input_file = 'depth.dat'\n",
    "output_dir = 'proc/'\n",
    "config_file = 'config.yaml'\n",
    "\n",
    "extract_command(input_file, output_dir, config_file)\n",
    "\n",
    "from IPython.display import Video, Image\n",
    "Video('proc/results_00.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all your data is extracted and saved in your desired proc/ directory, you are now able to perform the PCA step.\n",
    "\n",
    "### Training\n",
    "\n",
    "To train your PCA on your extracted data results, run the following command. It will recursively search within your current directory structure for your extracted results_xx.h5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_pca.gui import *\n",
    "\n",
    "input_dir = './'\n",
    "config_file = 'config.yaml'\n",
    "pca_filename = 'pca'\n",
    "output_dir = '_pca/'\n",
    "cluster_type = 'local'\n",
    "gaussfilter_space, gaussfilter_time = (1.5, 1), 0\n",
    "medfilter_space, medfilter_time = [0], [0]\n",
    "missing_data, missing_data_iters = True, 10\n",
    "mask_threshold, mask_height_threshold = -16, 5\n",
    "min_height, max_height = 10, 100\n",
    "tailfilter_size, tailfilter_shape = (9,9), 'ellipse'\n",
    "use_fft = False\n",
    "recon_pcs = 10\n",
    "rank = 50\n",
    "chunk_size = 4000\n",
    "local_processes = True\n",
    "queue = 'debug'\n",
    "nworkers, cores = 10, 1\n",
    "processes, memory = 1, '15GB'\n",
    "wall_time, timeout = '6:00:00', 5\n",
    "\n",
    "train_pca_command(input_dir, cluster_type, output_dir, gaussfilter_space, gaussfilter_time, medfilter_space, medfilter_time, missing_data, missing_data_iters, mask_threshold, mask_height_threshold, \n",
    "                  min_height, max_height, tailfilter_size, tailfilter_shape, use_fft, \n",
    "                  recon_pcs, rank, pca_filename, chunk_size, config_file, local_processes, \n",
    "                  queue, nworkers, cores, processes, memory, wall_time, timeout)\n",
    "\n",
    "from IPython.display import display, Image\n",
    "images = ['_pca/pca_components.png', '_pca/pca_scree.png']\n",
    "for im in images:\n",
    "    display(Image(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Principal Component Scores\n",
    "Once your PCA model has been trained, you can now apply your model using your extracted data amd computed principal components. To compute your PC Scores, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_pca.gui import *\n",
    "\n",
    "input_dir = './'\n",
    "config_file = 'config.yaml'\n",
    "scores_filename = 'pca_scores'\n",
    "output_dir = '_pca/'\n",
    "cluster_type = 'local'\n",
    "h5_path = './'\n",
    "h5_mask_path = './'\n",
    "pca_path = '/components'\n",
    "pca_file = '_pca/pca.h5'\n",
    "chunk_size = 4000\n",
    "fill_gaps = True\n",
    "fps = 30\n",
    "detrend_window = 0\n",
    "queue = 'debug'\n",
    "nworkers, cores = 1, 1\n",
    "processes, memory = 1, '15GB'\n",
    "wall_time, timeout = '6:00:00', 5\n",
    "\n",
    "apply_pca_command(input_dir, cluster_type, output_dir, scores_filename, h5_path, h5_mask_path, pca_path, pca_file,\n",
    "                  chunk_size, fill_gaps, fps, detrend_window, config_file,\n",
    "                  queue, nworkers, cores, processes, memory, wall_time, timeout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Model-free Syllable Changepoints\n",
    "To measure block duration distances between detected syllables using your PCA model or computed scores, you can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_pca.gui import *\n",
    "\n",
    "input_dir = './'\n",
    "config_file = 'config.yaml'\n",
    "scores_filename = '_pca/pca_scores.h5'\n",
    "output_dir = '_pca/'\n",
    "output_filename = 'changepoints'\n",
    "cluster_type = 'local'\n",
    "pca_file_components = None\n",
    "components_path = '/components'\n",
    "neighbors = 1\n",
    "threshold = .5\n",
    "klags = 6\n",
    "sigma = 3.5\n",
    "dims = 300\n",
    "fps = 30\n",
    "h5_path = './'\n",
    "h5_mask_path = './'\n",
    "chunk_size = 4000\n",
    "queue = 'debug'\n",
    "nworkers, cores = 1, 1\n",
    "processes, memory = 1, '15GB'\n",
    "wall_time, timeout = '6:00:00', 5\n",
    "\n",
    "compute_changepoints_command(input_dir, output_dir, output_filename, cluster_type, pca_file_components, scores_filename,\n",
    "                             components_path, neighbors, threshold, klags, sigma, dims, fps, h5_path, h5_mask_path, chunk_size,\n",
    "                             config_file, queue, nworkers, cores, processes, memory, wall_time, timeout)\n",
    "\n",
    "from IPython.display import display, Image\n",
    "display(Image('_pca/changepoints_dist.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ARHMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have computed your PCA Scores, you can now use this data as your input to train your Auto-Regressive Heuristic Markov Model (ARHMM).\n",
    "If you have multiple groups (for example, a control and experimental group) that you would like to model separately using the same model, use the ```--separate-trans``` flag in the command below.\n",
    "\n",
    "### Single Group Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_model.gui import *\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "scores_file = cwd+'/_pca/pca_scores.h5'\n",
    "dest_file = cwd+'/model.p'\n",
    "hold_out = False\n",
    "hold_out_seed = -1\n",
    "nfolds = 5\n",
    "ncpus = 0\n",
    "num_iter = 100\n",
    "var_name = 'scores'\n",
    "e_step = False\n",
    "save_every = -1\n",
    "save_model = True\n",
    "max_states = 50\n",
    "npcs = 10\n",
    "whiten = 'all'\n",
    "progressbar = True\n",
    "kappa = 100000 #nframes\n",
    "gamma = 1e3\n",
    "alpha = 5.7\n",
    "noise_level = 0\n",
    "nlags = 3\n",
    "separate_trans = False\n",
    "robust = False\n",
    "checkpoint_freq = -1\n",
    "index = \"\"\n",
    "default_group = 'n/a'\n",
    "\n",
    "learn_model_command(scores_file, dest_file, hold_out, hold_out_seed, nfolds, ncpus,\n",
    "                num_iter, var_name, e_step,\n",
    "                save_every, save_model, max_states, npcs, whiten, progressbar,\n",
    "                kappa, gamma, alpha, noise_level, nlags, separate_trans, robust,\n",
    "                checkpoint_freq, index, default_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Group Training\n",
    "In order to model multiple groups separately in your model, you must generate an index file to point to all your relevant paths, as well as indicate use the separate transition graphs flag.\n",
    "\n",
    "Begin by generating your index file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import *\n",
    "\n",
    "input_dir = './'\n",
    "pca_file = '_pca/pca_scores.h5'\n",
    "output_file = 'moseq2-index.yaml'\n",
    "filter_tup = ()\n",
    "all_uuids = False\n",
    "\n",
    "generate_index_command(input_dir, pca_file, output_file, filter_tup, all_uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell you can view which groups your subjects are associated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import *\n",
    "\n",
    "index_file = 'moseq2-index.yaml'\n",
    "key = ''\n",
    "value = ''\n",
    "group = ''\n",
    "exact = False\n",
    "lowercase = False\n",
    "negative = False\n",
    "\n",
    "add_group_command(index_file, key, value, group, exact, lowercase, negative)\n",
    "\n",
    "\n",
    "# Implement view groups function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train your model on multiple groups using your augmented index file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for multi groups with sep\n",
    "cwd = os.getcwd()\n",
    "scores_file = cwd+'/_pca/pca_scores.h5'\n",
    "dest_file = cwd+'/model.p'\n",
    "hold_out = False\n",
    "hold_out_seed = -1\n",
    "nfolds = 5\n",
    "ncpus = 0\n",
    "num_iter = 100\n",
    "var_name = 'scores'\n",
    "e_step = False\n",
    "save_every = -1\n",
    "save_model = True\n",
    "max_states = 50\n",
    "npcs = 10\n",
    "whiten = 'all'\n",
    "progressbar = True\n",
    "kappa = 100000 #nframes\n",
    "gamma = 1e3\n",
    "alpha = 5.7\n",
    "noise_level = 0\n",
    "nlags = 3\n",
    "separate_trans = True\n",
    "robust = False\n",
    "checkpoint_freq = -1\n",
    "index = \"\"\n",
    "default_group = 'n/a'\n",
    "\n",
    "learn_model_command(scores_file, dest_file, hold_out, hold_out_seed, nfolds, ncpus,\n",
    "                num_iter, var_name, e_step,\n",
    "                save_every, save_model, max_states, npcs, whiten, progressbar,\n",
    "                kappa, gamma, alpha, noise_level, nlags, separate_trans, robust,\n",
    "                checkpoint_freq, index, default_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a trained ARHMM, you can use the moseq2-viz module to produce crowd videos and a number of statistical analysis plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Ensure that you have a `moseq2-index.yaml` file generated using this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_model.gui import *\n",
    "from moseq2_viz.gui import *\n",
    "\n",
    "input_dir = './'\n",
    "pca_file = '_pca/pca_scores.h5'\n",
    "output_file = 'moseq2-index.yaml'\n",
    "filter_tup = None\n",
    "all_uuids = False\n",
    "\n",
    "generate_index_command(input_dir, pca_file, output_file, filter_tup, all_uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Crowd Videos\n",
    "This tool allows you to create videos containing many overlayed clips of the mouse performing the same specified syllable at the moment a red dot appears on their body. The videos are sorted by most frequently expressed syllable to least.\n",
    "To create the crowd videos, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import *\n",
    "\n",
    "index_filepath = 'moseq2-index.yaml'\n",
    "output_dir = './crowd_movies/'\n",
    "model_path = 'model.p'\n",
    "max_syllables, max_examples = 40, 40\n",
    "sort = True\n",
    "count = 'usage'\n",
    "gaussianfilter_space = (0, 0)\n",
    "medfilter_space = [0]\n",
    "min_height, max_height = 5, 80\n",
    "raw_size, scale = (512, 424), 1\n",
    "cmap = 'jet'\n",
    "dur_clip = 300\n",
    "legacy_jitter_fix = False\n",
    "\n",
    "make_crowd_movies_command(index_filepath, model_path, max_syllables, max_examples, sort, count, gaussianfilter_space,\n",
    "                          medfilter_space, output_dir, min_height, max_height, raw_size, scale, cmap, dur_clip, legacy_jitter_fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Usage Plots\n",
    "Use this command to compute the model-detected syllables usages sorted in descending order of usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import *\n",
    "\n",
    "index_filepath = 'moseq2-index.yaml'\n",
    "model_fit = 'model.p'\n",
    "sort = True\n",
    "count = 'usage'\n",
    "max_syllable = 40\n",
    "group = ''\n",
    "output_file = 'usages'\n",
    "\n",
    "plot_usages_command(index_filepath, model_fit, sort, count, max_syllable, group, output_file)\n",
    "from IPython.display import display, Image\n",
    "display(Image('usages.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Scalar Summary and Tracking Plots\n",
    "Use the following command to compute some scalar summary information about your modeled groups, such as average velocity, height, etc.\n",
    "This command will also generate a tracking summary plot; depicting the path traveled by the mouse in your recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import *\n",
    "\n",
    "index_filepath = 'moseq2-index.yaml'\n",
    "output_file = 'scalars'\n",
    "\n",
    "plot_scalar_summary_command(index_filepath, output_file)\n",
    "from IPython.display import display, Image\n",
    "display(Image('scalars_summary.png'))\n",
    "display(Image('scalars_position.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Syllable Transition Graph\n",
    "Use the following command to generate a syllable transition graph. The graph will be comprised of nodes labelled by syllable, and edges depicting a probable transition, with edge thickness depicting the weight of the transition edge.\n",
    "\n",
    "For multiple groups, there will be a transition graph for each group, as well as a unified graph with different colors to identify the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import *\n",
    "\n",
    "index_filepath = 'moseq2-index.yaml'\n",
    "model_fit = 'model.p'\n",
    "max_syllable = 40\n",
    "group = ''\n",
    "output_file = 'transition'\n",
    "normalize = 'bigram'\n",
    "edge_threshold, usage_threshold = .001, 0\n",
    "layout = 'spring'\n",
    "keep_orphans, orphan_weight = False, 0\n",
    "arrows = True\n",
    "sort = True\n",
    "count = 'usage'\n",
    "edge_scaling, node_scaling = 250, 1e4\n",
    "scale_node_by_usage = True\n",
    "width_per_group = 8\n",
    "\n",
    "plot_transition_graph_command(index_filepath, model_fit, max_syllable, group, output_file, normalize, edge_threshold,\n",
    "                              usage_threshold, layout, keep_orphans, orphan_weight, arrows, sort, count, edge_scaling, node_scaling,\n",
    "                              scale_node_by_usage, width_per_group)\n",
    "\n",
    "from IPython.display import display, Image\n",
    "display(Image('transition.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
