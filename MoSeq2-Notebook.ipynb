{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to MoSeq2-Notebook\n",
    "\n",
    "### Run all of the MoSeq tools in a containerized notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, run the following jupyter notebook to ensure MoSeq2 is installed and running smoothely on your machine.\n",
    "[SETUP NOTEBOOK](http://localhost:8889/notebooks/MoSeq2_Step_0.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, copy this notebook file to your recorded session directory. You will create a new copy of this notebook for each analysis session.\n",
    "\n",
    "You can use the cells below to ensure your sessions are found in this notebook, and the correct python is being used in your designated conda env (where your moseq2 tools are installed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample directory structure with your MoSeq2-Notebook:\n",
    "\n",
    "```\n",
    ".\n",
    "├── MoSeq2-Notebook.ipynb\n",
    "└── sample_session/\n",
    "    ├── depth.dat\n",
    "    ├── depth_ts.txt\n",
    "    └── metadata.json\n",
    "```\n",
    "\n",
    "For multiple sessions,\n",
    "\n",
    "```\n",
    ".\n",
    "├── MoSeq2-Notebook.ipynb\n",
    "├── session_1/\n",
    "    ├── depth.dat\n",
    "    ├── depth_ts.txt\n",
    "    └── metadata.json\n",
    "└── session_2/   \n",
    "    ├── depth.dat\n",
    "    ├── depth_ts.txt\n",
    "    └── metadata.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you are running the python version located in your corresponding conda environment.\n",
    "\n",
    "For example, if your anaconda environment is called moseq2, then your output would look like: ```/Users/username/anaconda3/envs/moseq2/bin/python```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Configuration Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the files and env are correct, run the cell below to generate a configuration file that will aid in modifying advanced analysis/extraction parameters throughout the MoSeq2 pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction/Analysis Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from moseq2_extract.gui import *\n",
    "\n",
    "base_dir = './' # \"./\" == directory where this notebook is located\n",
    "config_filepath = base_dir+'config.yaml'\n",
    "\n",
    "generate_config_command(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a Flip file\n",
    "In order to ensure your extraction is smooth and invariant to the mouse's orientation, we recommend using a flip-classifier to aid keeping the mouse oriented throughout the extraction.\n",
    "\n",
    "The flip file indices are as follows:\n",
    "* [0] - Large mice with fibers.\n",
    "* [1] - Adult male c57s.\n",
    "* [2] - Mice with Inscopix cables.\n",
    "\n",
    "Enter your desired index in the variable assignment below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import *\n",
    "\n",
    "selected_index = 1 # flip file index\n",
    "download_flip_command(base_dir, config_filepath, selected_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is done, the flip file will be automatically used in your following extractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Extraction Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction Road Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Extraction Data Quality Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing a full extraction on your recordings, follow the following steps to ensure your Regions of Interest (ROIs) are properly found. This will bring more clarity as to what to expect after a complete extraction of your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Test\n",
    "The following cell will extract the first frame, ROI, and background ROI for your reference before continuing into the extraction process. This ensures proper data quality going into the analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import find_roi_command\n",
    "\n",
    "sample_testdir_in = base_dir+'session_1/' # session directory to perform ROI testing\n",
    "sample_testfile = sample_testdir_in+'depth.dat' # depth file to perform ROI testing on\n",
    "sample_testdir_out = sample_testdir_in+'sample_proc/' # directory to save roi extraction results\n",
    "\n",
    "find_roi_command(sample_testfile, sample_testdir_out, config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to display your calculated ROI images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display extracted ROI\n",
    "from IPython.display import display, Image\n",
    "for infile in os.listdir(sample_testdir_out):\n",
    "    if infile[-3:] == 'png':\n",
    "        print(infile[:-4])\n",
    "        display(Image(sample_testdir_out+infile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Test Extraction \n",
    "Run the following cell to test your raw data extraction parameters before extracting all of your data to ensure the best data quality going into the PCA step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import sample_extract_command\n",
    "\n",
    "extract_testdir_out = sample_testdir_in+'test_proc/' # directory to save sample extraction\n",
    "nframes = 100 # number of frames to extract from raw to preview\n",
    "\n",
    "sample_extract_command(sample_testfile, extract_testdir_out, config_filepath, nframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Video, Image\n",
    "\n",
    "display(Video(extract_testdir_out+'results_00.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Session Extraction\n",
    "If you only have one session you would like to extract, run the following cell. Otherwise, run the Batch Extraction cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import extract_command\n",
    "\n",
    "session_path = 'session_1/' # session folder to extract\n",
    "input_filepath = session_path+'depth.dat' # specify depth filename to extract\n",
    "output_dir = 'proc/' # will output to session_path/proc/\n",
    "\n",
    "extract_command(input_filepath, output_dir, config_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, Image\n",
    "\n",
    "Video(session_path+output_dir+'results_00.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Session (Batch) Extraction\n",
    "Extract multiple recording sessions (directories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_batch.gui import extract_batch_command\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "filename = 'depth.dat' # depth files to recursively search for that have been partially extracted or not yet extracted \n",
    "cluster_type = 'local' # specify whether running on 'local' computer or 'slurm' cluster\n",
    "\n",
    "## advanced settings\n",
    "temp_storage = Path('tmp/') # path to temporarily store values\n",
    "partition = 'short' # slurm job partition specification\n",
    "prefix = '' # command to run before executing batch extraction\n",
    "skip_checks = False\n",
    "\n",
    "commands = extract_batch_command(base_dir, Path(config_filepath), filename, cluster_type, temp_storage,\n",
    "                  partition, prefix, skip_checks)\n",
    "\n",
    "with open('batch_extract.sh', 'w') as f:\n",
    "    f.write('#!/bin/bash\\n')\n",
    "    for cmd in commands:\n",
    "        cmd = cmd.strip(';')\n",
    "        f.write('%s\\n' % cmd)\n",
    "        print(cmd)\n",
    "\n",
    "os.system('chmod a+x batch_extract.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following bash command to execute your batch extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./batch_extract.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is done, aggregate all of your extraction results to consolidate all of your metadata and timestamp data in one folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_batch.gui import aggregate_extract_results_command\n",
    "\n",
    "recording_format = '{start_time}_{session_name}_{subject_name}' # filename formats for the extracted data\n",
    "aggregate_results_dir = 'aggregate_results/' # directory to save all metadata+extracted videos to with above respective name format\n",
    "mouse_threshold = 0\n",
    "\n",
    "aggregate_extract_results_command(base_dir, recording_format, aggregate_results_dir, mouse_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View your extracted videos by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Video\n",
    "\n",
    "for infile in os.listdir(aggregate_results_dir):\n",
    "    if infile[-3:] == 'mp4':\n",
    "        print(infile[:-4])\n",
    "        display(Video(aggregate_results_dir+infile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all your data is extracted and saved in your desired proc/ directory, you are now able to perform the PCA step.\n",
    "\n",
    "## Training\n",
    "\n",
    "To train your PCA on your extracted data results, run the following command. It will recursively search within your current directory structure for your extracted results_xx.h5 files. (If extraction results were aggregated, then all the loaded files will be from your `aggregate_results/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_pca.gui import train_pca_command\n",
    "\n",
    "pca_filename = 'pca' # Name of your PCA model h5 file to be saved\n",
    "pca_dirname = '_pca/' # Directory to save your computed PCA results\n",
    "\n",
    "train_pca_command(base_dir, config_filepath, pca_dirname, pca_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is finished, run the following cell to view the computed components and PCA Scree plot to determine the number of PCs to use in ARHMM modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "images = [pca_dir+'pca_components.png',pca_dir+'pca_scree.png']\n",
    "for im in images:\n",
    "    display(Image(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Principal Component Scores\n",
    "Once your PCA model has been trained, you can now apply your model using your extracted data amd computed principal components. To compute your PC Scores, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_pca.gui import apply_pca_command\n",
    "\n",
    "scores_filename = 'pca_scores' # name of the scores file to compute and save\n",
    "\n",
    "apply_pca_command(base_dir, config_filepath, pca_dirname, scores_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Computing Model-free Syllable Changepoints\n",
    "This is an optional step that is used to help determine the kappa parameter to use in the modeling step.\n",
    "\n",
    "To measure block duration distances between detected syllables using your PCA model or computed scores, you can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_pca.gui import compute_changepoints_command\n",
    "\n",
    "changepoints_filename = 'changepoints' # name of the changepoints images to generate\n",
    "\n",
    "compute_changepoints_command(base_dir, config_filepath, pca_dirname, changepoints_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(pca_dirname+changepoints_filename+'_dist.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ARHMM (Compute Locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling Roadmap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have computed your PCA Scores, you can now use this data as your input to train your Auto-Regressive Hidden Markov Model (ARHMM).\n",
    "If you have multiple groups (for example, a control and experimental group) that you would like to model separately using the same model, use the ```--separate-trans``` flag in the command below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Group Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_model.gui import learn_model_command\n",
    "import os\n",
    "\n",
    "scores_file = pca_dirname+scores_filename+'.h5' # path to input scores file to model\n",
    "model_path = './model.p' # path to save trained model\n",
    "index_file = \"\" # index file path (not necessary for single group training)\n",
    "hold_out = False # boolean to hold out data during the training process\n",
    "hold_out_seed = -1 # integer to standardize the held out folds during training\n",
    "nfolds = 5 # number of folds to hold out during training (if hold_out==True)\n",
    "num_iter = 10 # number of iterations to train model\n",
    "max_states = 50 # number of maximum states the ARHMM can end up with\n",
    "npcs = 10  # number of PCs being used\n",
    "kappa = 100000 # total number of frames\n",
    "gamma = 1e3\n",
    "alpha = 5.7\n",
    "separate_trans = False # separate group transition graphs\n",
    "robust = False # use robust-ARHMM with t-distribution\n",
    "checkpoint_freq = -1 # model saving freqency (in interations)\n",
    "\n",
    "\n",
    "learn_model_command(scores_file, model_path, config_filepath, index_file, hold_out, nfolds,\n",
    "                    num_iter, max_states, npcs, kappa, gamma, alpha, \n",
    "                    separate_trans, robust, checkpoint_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Group Training\n",
    "In order to model multiple groups separately in your model (e.g. control vs. experimental groups), you must generate an index file to point to all your relevant paths, as well as indicate use the separate transition graphs flag.\n",
    "\n",
    "Begin by generating your index file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate your index file\n",
    "This file will be used to point to all of your extracted data + metadata to aid in analysis visualization and group dictation/separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import generate_index_command\n",
    "\n",
    "index_filepath = 'moseq2-index.yaml' # index file containing all the path/metadata info about the groups + Subjects\n",
    "filter_tup = ()\n",
    "all_uuids = False\n",
    "\n",
    "generate_index_command(base_dir, scores_file, index_filepath, filter_tup, all_uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell you can view which groups your subjects are associated with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your subjects to groups in your index file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add your subjects to specific groups, simply indicate the correct keys, values and groups that correspond with a session parameter in your index file, and run the cell.\n",
    "\n",
    "This can be done to add many subjects to one group, or run multiple times to manually add different subjects to certain groups.\n",
    "\n",
    "Note: the groups can also be manually configured in your moseq2-index.yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import add_group_command\n",
    "\n",
    "key = 'SubjectName' # Name of index key (metadata variable id)\n",
    "value = 'dat01' # value of the corresponding key\n",
    "group = 'group1' # designated group name\n",
    "exact = False # Must be exact string match\n",
    "lowercase = False # change to lowercase\n",
    "negative = False # select opposite selection than key-value pair given\n",
    "\n",
    "add_group_command(index_filepath, key, value, group, exact, lowercase, negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view your current groups and their labeled subjects, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement view groups function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Multiple Groups\n",
    "Now you can train your model on multiple groups using your augmented index file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_model.gui import learn_model_command\n",
    "\n",
    "scores_file = pca_dir+scores_filename+'.h5' # path to input scores file to model\n",
    "model_path = base_dir+'model.p' # path to save trained model\n",
    "hold_out = False # boolean to hold out data during the training process\n",
    "hold_out_seed = -1 # integer to standardize the held out folds during training\n",
    "nfolds = 5 # number of folds to hold out during training (if hold_out==True)\n",
    "num_iter = 10 # number of iterations to train model\n",
    "max_states = 50 # number of maximum states the ARHMM can end up with\n",
    "npcs = 10  # number of PCs being used\n",
    "kappa = 100000 # total number of frames\n",
    "gamma = 1e3\n",
    "alpha = 5.7\n",
    "separate_trans = True # separate group transition graphs\n",
    "robust = False # use robust-ARHMM with t-distribution\n",
    "checkpoint_freq = -1 # model saving freqency (in interations)\n",
    "\n",
    "learn_model_command(scores_file, model_path, config_filepath, index_file, hold_out, nfolds,\n",
    "                    num_iter, max_states, npcs, kappa, gamma, alpha, \n",
    "                    separate_trans, robust, checkpoint_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viz Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a trained ARHMM, you can use the moseq2-viz module to produce crowd videos and a number of statistical analysis plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Ensure that you have a `moseq2-index.yaml` file generated using this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import generate_index_command\n",
    "\n",
    "index_filepath = base_dir+'moseq2-index.yaml' # index file containing all the path/metadata info about the groups + Subjects\n",
    "filter_tup = ()\n",
    "all_uuids = False # include all uuids in the index file?\n",
    "\n",
    "generate_index_command(base_dir, scores_file, index_filepath, filter_tup, all_uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Crowd Videos\n",
    "This tool allows you to create videos containing many overlayed clips of the mouse performing the same specified syllable at the moment a red dot appears on their body. The videos are sorted by most frequently expressed syllable to least.\n",
    "To create the crowd videos, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import make_crowd_movies_command\n",
    "\n",
    "crowd_dir = base_dir+'crowd_movies/' # output directory to save all movies in\n",
    "\n",
    "max_syllables, max_examples = 10, 10 # maximum number of syllables, and examples of each syllable in a video respectively\n",
    "\n",
    "make_crowd_movies_command(index_filepath, model_path, config_filepath, crowd_dir, max_syllables, max_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to view your generated crowd movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Video\n",
    "\n",
    "for infile in os.listdir(crowd_dir):\n",
    "    if infile[-3:] == 'mp4':\n",
    "        print(infile[:-4])\n",
    "        display(Video(crowd_dir+infile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Usage Plots\n",
    "Use this command to compute the model-detected syllables usages sorted in descending order of usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import plot_usages_command\n",
    "\n",
    "sort = True\n",
    "count = 'usage'\n",
    "max_syllable = 10\n",
    "group = ''\n",
    "output_file = 'usages'\n",
    "\n",
    "plot_usages_command(index_filepath, model_path, sort, count, max_syllable, group, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image('usages.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Scalar Summary and Tracking Plots\n",
    "Use the following command to compute some scalar summary information about your modeled groups, such as average velocity, height, etc.\n",
    "This command will also generate a tracking summary plot; depicting the path traveled by the mouse in your recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import plot_scalar_summary_command\n",
    "\n",
    "output_file = 'scalars' # prefix name of the saved scalar position and summary graphs\n",
    "\n",
    "plot_scalar_summary_command(index_filepath, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image('scalars_summary.png'))\n",
    "display(Image('scalars_position.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Syllable Transition Graph\n",
    "Use the following command to generate a syllable transition graph. The graph will be comprised of nodes labelled by syllable, and edges depicting a probable transition, with edge thickness depicting the weight of the transition edge.\n",
    "\n",
    "For multiple groups, there will be a transition graph for each group, as well as a unified graph with different colors to identify the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import plot_transition_graph_command\n",
    "\n",
    "max_syllable = 40 # Maximum number of nodes in the transition graph\n",
    "group = '' # Group to graph, default if empty str\n",
    "output_filename = 'transition' # name of the png file to be saved\n",
    "\n",
    "plot_transition_graph_command(index_filepath, model_path, config_filepath, max_syllable, group, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image('transition.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
