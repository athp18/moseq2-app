{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to MoSeq2-Notebook\n",
    "\n",
    "### Run all of the MoSeq2 tools in a containerized notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>MoSeq2 Introduction</h1></center>\n",
    "<img src=\"https://raw.githubusercontent.com/dattalab/moseq2-app/jupyter/media/Data_Pipeline.png?token=ACRN4H6FD7TUR7AI5K3GGAC5T6EDC\">\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoSeq2 software toolkit for unsupervised characterization of animal behavior. Moseq takes depth recordings of single behaving animals as input, and outputs a rich labeling of postural dynamics in terms of reused motifs or 'syllables'. This notebook begins with compressed depth recordings (see 'Data Acquisiting Overview' below) and transforms this data through the steps of:\n",
    "\n",
    "- **Extraction**: The animal is segmented from the background and its position and heading direction are aligned acros frames\n",
    "- **Compute PCA**: Raw video is de-noised and transformed to low-dimensional pose trajectories using principal component analysis (PCA)\n",
    "- **Train ARHMM**: Pose trajectories are modeled using an autoregressive hidden Markov model (ARHMM), producing a sequence of syllable labels\n",
    "- **Analysis**: Model output is reported through visualization and stiatical analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "Below are a list of publications and links to the individual github tool wikis for your convenience.\n",
    "- Publications\n",
    "    - [Mapping Sub-Second Structure in Mouse Behavior](http://datta.hms.harvard.edu/wp-content/uploads/2018/01/pub_23.pdf)\n",
    "    - [Composing Probabilistic Graphical Models and Variational Autoencoders](http://datta.hms.harvard.edu/wp-content/uploads/2018/01/pub_24.pdf)\n",
    "    - [Q&A: Understanding the composition of behavior](http://datta.hms.harvard.edu/wp-content/uploads/2019/06/Datta-QA.pdf)\n",
    "- Wikis\n",
    "    - [Extract](https://github.com/dattalab/moseq2-extract/wiki)\n",
    "    - [PCA](https://github.com/dattalab/moseq2-pca/wiki)\n",
    "    - [Model](https://github.com/dattalab/moseq2-model/wiki)\n",
    "    - [Viz](https://github.com/dattalab/moseq2-viz/wiki)\n",
    "    - [Batch](https://github.com/dattalab/moseq2-batch/wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moseq2 takes animal depth recordings as input. We we have developed a [data acquisition pipeline](https://github.com/dattalab/moseq2-docs/wiki/Setup:-acquisition-software) for the Xbox Kinect depth camera. We suggest following our [data acquisiting tutorial](https://github.com/dattalab/moseq2-docs/wiki/Acquisition) for doing recordings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data file organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moseq2 requires input data to have this following directory structure, where a single master directory contains one sub-directory for each recording session, and each of the sub-directories has depth data, metadata and optional timestamp data:\n",
    "\n",
    "```\n",
    "├── session_1/\n",
    "├   ├── depth.dat        # depth data\n",
    "├   ├── depth_ts.txt     # timestamps\n",
    "├   └── metadata.json    # metadata\n",
    "└── session_2/   \n",
    "├   ├── depth.dat\n",
    "├   ├── depth_ts.txt\n",
    "└── └── metadata.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Software Setup</h1></center>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the requirements for Moseq2 using the [setup notebook](http://localhost:8888/notebooks/MoSeq2_Step_0.ipynb) (if you have not done so already). Then copy the present notebook into the same directory as your depth recordings before proceeding with the following setup steps\n",
    "<img src=\"https://raw.githubusercontent.com/dattalab/moseq2-app/jupyter/media/Setup_Pipeline.png?token=ACRN4HZAR3Z7OMAYVFRZH725T6EGA\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure your session folders are found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import get_found_sessions\n",
    "\n",
    "found_sessions = get_found_sessions()\n",
    "\n",
    "print('number of found sessions to analyze:', found_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure you are running the python version located in your corresponding conda environment.\n",
    "\n",
    "For example, if your anaconda environment is called moseq2, then your output would look like: ```/Users/username/anaconda3/envs/moseq2/bin/python```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from moseq2_extract.gui import generate_config_command\n",
    "from moseq2_viz.gui import generate_index_command\n",
    "\n",
    "base_dir = './' # \"./\" == directory where this notebook is located\n",
    "config_filepath = base_dir+'config.yaml'\n",
    "\n",
    "generate_config_command(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A configuration file has been created in the same directory as your Notebook and session directories. The directory should now have the following contents\n",
    "\n",
    "```\n",
    ".\n",
    "├── MoSeq2-Notebook.ipynb\n",
    "├── config.yaml **\n",
    "├── session_1/\n",
    "└── session_2/   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a Flip File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure your extraction is smooth and invariant to the mouse's orientation, we recommend using a flip-classifier to aid keeping the mouse oriented throughout the extraction.\n",
    "\n",
    "Three pre-trained flip classifiers are available\n",
    "* [0] - Large mice with fibers.\n",
    "* [1] - Adult male c57s.\n",
    "* [2] - Mice with Inscopix cables.\n",
    "\n",
    "Enter the index corresponding to your preferred pretrained classifier below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import download_flip_command\n",
    "\n",
    "selected_index = 1 # flip file index\n",
    "download_flip_command(base_dir, config_filepath, selected_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional Advanced Setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the following options to match your current hardware working environment. This is particularly relevant for running the pipeline on a compute cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurable Parameter Descriptions\n",
    "- __CLUSTER_TYPE__: Indicates whether you are running on your local computer, or using a slurm scheduler.\n",
    "    Options: {'local', 'slurm'}\n",
    "- __CORES__: Number of cores to split the processes among.\n",
    "- __MEMORY__: Amount of memory to allocate to your operations.\n",
    "- __PROCESSES__: Number of processes to spawn that each will have a set of worker threads. (Choose 1 unless you are using a scheduler)\n",
    "- __NWORKERS__: Number of worker threads that are executing your operations. (Choose 1 unless you are using a scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "\n",
    "with open(config_filepath, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "f.close()\n",
    "\n",
    "config_data['cluster_type'] = 'local'\n",
    "config_data['cores'] = 4 # recommended n-1; where n = total number of cores/cpus available\n",
    "config_data['memory'] =  \"15GB\" # recommended n-2GB; where n = total GB of RAM\n",
    "config_data['nworkers'] =  1 # recommended 1 per core (for local cluster)\n",
    "config_data['processes'] =  1 # recommended 1 per core (for local cluster)\n",
    "\n",
    "\n",
    "with open(config_filepath, 'w') as f:\n",
    "    yaml.dump(config_data, f, Dumper=yaml.RoundTripDumper)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Raw Data Extraction</h1></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the MoSeq2-Extract module in order to convert your raw data files to human-readable/viewable formats such as mp4 videos, and YAML/HDF5 metadata files. These metadata files are used to then train your PCA model, while the mp4 file is primarily used to ensure that the session was extracted correctly with no defects or unwanted artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the extraction step, begin by testing your detected ROIs with the default parameters. If all goes well, continue into the to the test extraction step.\n",
    "\n",
    "The first two steps are meant to debug possible extraction errors you may encounter before performing an extraction on your full dataset.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dattalab/moseq2-app/jupyter/media/Extraction_Pipeline.png?token=ACRN4HYVSFGOOCOO4UCXWAS5T6EHM\">\n",
    "\n",
    "Once testing is done, you can then proceed to extract all the session files found by your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Extraction Data Quality Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing a full extraction on your recordings, follow the following steps to ensure your Regions of Interest (ROIs) are properly found. This will bring more clarity as to what to expect after a complete extraction of your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test ensures that your whole background area is properly captured without any artifacts that may interfere with the mouse video extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurable Parameter Descriptions\n",
    "- __BG_ROI_DILATE__: Mask size to capture the complete environment. Make both values equal for square or circular environments, and different for rectangular environment cases.\n",
    "- __BG_ROI_DEPTH_RANGE__: Height range for the extraction algorithm to use when estimating distance from the camera to the floor. (Ensure to take real-life measurements to guarantee proper depth capturing.\n",
    "- __BG_ROI_GRADIENT_FILTER__: Boolean value for whether you would like to extract the walls from your environment ROI.\n",
    "- __USE_PLANE_BGROUND__: Boolean value for whether you would like to use a geometric-plane fit during ROI analysis. (This is typically used when some regions of the environment may not be captured fully.)\n",
    "\n",
    "### Possible ROI Pathologies\n",
    "- __BG_ROI_DILATE__: If incorrect, you may end up with an incorrect representation for your environment.\n",
    "- __BG_ROI_DEPTH_RANGE__: If incorrect, the estimations for the mouse's height during the extraction will not be reliable.\n",
    "- __USE_PLANE_BGROUND__: Used for when regions of the bucket/environment floor may be too shiny, causing them to be omitted from the background region representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will extract the first frame, ROI, and background ROI for your reference before continuing into the extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "from moseq2_extract.gui import find_roi_command\n",
    "\n",
    "sample_testdir_in = base_dir+'session_1/' # session directory to perform ROI testing\n",
    "sample_roi_testfile = sample_testdir_in+'depth.dat' # depth file to perform ROI testing on\n",
    "sample_testdir_out = sample_testdir_in+'sample_proc/' # directory to save roi extraction results\n",
    "\n",
    "with open(config_filepath, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "f.close()\n",
    "\n",
    "# Relevant ROI parameters you may need to configure\n",
    "config_data['bg_roi_dilate'] = (10, 10) # Size of the mask dilation (to include environment walls)\n",
    "config_data['bg_roi_depth_range'] = (650, 750) # Range to search for floor of arena (in mm)\n",
    "config_data['bg_roi_gradient_filter'] = False # Exclude walls with gradient filtering\n",
    "config_data['use_plane_bground'] = False # Use plane fit for background\n",
    "\n",
    "with open(config_filepath, 'w') as f:\n",
    "    yaml.dump(config_data, f, Dumper=yaml.RoundTripDumper)\n",
    "f.close()\n",
    "\n",
    "find_roi_command(sample_roi_testfile, sample_testdir_out, config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, you can expect the following directory structure:\n",
    "\n",
    "```\n",
    ".\n",
    "├── config.yaml\n",
    "├── MoSeq2-Notebook.ipynb\n",
    "├── session_1/\n",
    "├   ├── sample_proc/ **\n",
    "├   ├   ├── bground.png **\n",
    "├   ├   ├── first_frame.png **\n",
    "├   ├   └── roi.png ** \n",
    "├   ├── depth.dat\n",
    "├   ├── depth_ts.txt\n",
    "├   └── metadata.json\n",
    "└── session_2/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display your calculated ROI images below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "for infile in os.listdir(sample_testdir_out):\n",
    "    if infile[-3:] == 'png':\n",
    "        print(infile[:-4])\n",
    "        display(Image(sample_testdir_out+infile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Test Extraction \n",
    "Run the following cell to test your raw data extraction parameters before extracting all of your data to ensure the best data quality going into the PCA step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurable Parameter Descriptions\n",
    "- __MIN_HEIGHT__: The shortest possible height that the mouse can be in your recordings.\n",
    "- __MAX_HEIGHT__: The tallest possible height your mouse can be in your recordings.\n",
    "- __SPATIAL_FILTER_SIZE__: Median filter applied to the raw video as it is extracted in order to get crisp mp4 files. The larger the kernel, the more granular your video will become.\n",
    "- __USE_TRACKING_MODEL__: Boolean value to decide whether to use Expectation Maximization (EM) Tracking on mice with inscopix cables, helps improve the processed representation of the mice poses.\n",
    "- __CABLE_FILTER_ITERS__: Number of times to iterate over mouse cable during frame cleaning process. (Only occurs if use_tracking_model is true).\n",
    "\n",
    "### Possible Extraction Pathologies\n",
    "- __MIN_HEIGHT__: Important factor to include in order to properly estimate the minimum depth value during video construction.\n",
    "- __MAX_HEIGHT__: Important factor to include in order to properly estimate the maximum depth value during video construction.\n",
    "- __SPATIAL_FILTER_SIZE__: Careful not to set it too high as to not lose video clarity. (Must be ODD)\n",
    "- __USE_TRACKING_MODEL__: Not recommended to use for regular mice without any cable obstructions in the video.\n",
    "- __CABLE_FILTER_ITERS__: We recommend starting with 5 filter iterations at first and then modifying it based on how much of the cable has been cleaned from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import sample_extract_command\n",
    "\n",
    "sample_testfile = sample_testdir_in + 'depth.dat'\n",
    "extract_testdir_out = 'test_proc/' # directory to save sample extraction\n",
    "nframes = 200 # number of frames to extract from raw to preview\n",
    "\n",
    "with open(config_filepath, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "f.close()\n",
    "\n",
    "# Extraction parameters you may need to configure\n",
    "config_data['min_height'] = 10 # Min mouse height from floor (mm)\n",
    "config_data['max_height'] = 100 # Max mouse height from floor (mm)\n",
    "\n",
    "# Use an expectation-maximization style model to aid mouse tracking. Useful for data with cables\n",
    "config_data['use_tracking_model'] = False \n",
    "config_data['cable_filter_iters'] = 0 # Number of cable filter iterations\n",
    "\n",
    "with open(config_filepath, 'w') as f:\n",
    "    yaml.dump(config_data, f, Dumper=yaml.RoundTripDumper)\n",
    "f.close()\n",
    "\n",
    "sample_extract_command(sample_testfile, extract_testdir_out, config_filepath, nframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After an extraction, you can expect the following directory structure:\n",
    "\n",
    "```\n",
    ".\n",
    "├── config.yaml\n",
    "├── MoSeq2-Notebook.ipynb\n",
    "├── session_1/\n",
    "├   ├── test_proc/ **\n",
    "├   ├   ├── bground.tiff **\n",
    "├   ├   ├── first_frame.tiff **\n",
    "├   ├   ├── results_00.mp4 **\n",
    "├   ├   ├── results_00.h5 **\n",
    "├   ├   ├── results_00.yaml **\n",
    "├   ├   └── roi.tiff ** \n",
    "├   ├── depth.dat\n",
    "├   ├── depth_ts.txt\n",
    "├   └── metadata.json\n",
    "└── session_2/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view your sample extraction below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Video, Image\n",
    "\n",
    "display(Video(sample_testdir_in+extract_testdir_out+'results_00.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are happy with your sample extraction, continue to extracting your full dataset. Otherwise, consider adjusting some of your ROI or extraction parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Session(s)\n",
    "\n",
    "Run the following cells to create a shell script that will extract all of your found `depth.dat` files, and subsequently execute the shell script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_batch.gui import extract_batch_command\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "filename = 'depth.dat' # depth files to recursively search for that have been partially extracted or not yet extracted \n",
    "\n",
    "## advanced settings\n",
    "skip_checks = False # check whether the session directory has been previously extracted\n",
    "partition = 'short' # slurm job partition specification // skip if running on local cluster\n",
    "\n",
    "\n",
    "commands = extract_batch_command(base_dir, Path(config_filepath), filename, partition, skip_checks)\n",
    "\n",
    "with open('batch_extract.sh', 'w') as f:\n",
    "    f.write('#!/bin/bash\\n')\n",
    "    for cmd in commands:\n",
    "        cmd = cmd.strip(';')\n",
    "        f.write('%s\\n' % cmd)\n",
    "        print(cmd)\n",
    "\n",
    "os.system('chmod a+x batch_extract.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./batch_extract.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shell command is simply peforming a extract command for each of your sessions, (this is best utilized using a Slurm scheduler). \n",
    "\n",
    "This is what your directory structure should look like once the process is complete:\n",
    "\n",
    "```\n",
    ".\n",
    "├── MoSeq2-Notebook.ipynb\n",
    "├── config.yaml\n",
    "├── session_1/\n",
    "├   ...\n",
    "├   └── proc/ **\n",
    "├   ├   ├── roi.tiff\n",
    "├   ├   ...\n",
    "├   └   └── results.h5 **\n",
    "└── session_2/\n",
    "├   ...\n",
    "├   └── proc/ **\n",
    "├   ├   ├── roi.tiff\n",
    "├   ├   ...\n",
    "└   └   └── results.h5 **\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is done, aggregate all of your extraction results to consolidate all of your unique session metadata and timestamp information in one folder, and generate your index file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_batch.gui import aggregate_extract_results_command\n",
    "\n",
    "recording_format = '{start_time}_{session_name}_{subject_name}' # filename formats for the extracted data\n",
    "aggregate_results_dir = 'aggregate_results/' # directory to save all metadata+extracted videos to with above respective name format\n",
    "\n",
    "aggregate_extract_results_command(base_dir, recording_format, aggregate_results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting in the following directory (sample) structure:\n",
    "\n",
    "```\n",
    ".\n",
    "├── aggregate_results/ **\n",
    "├   ├── session_1_results.h5 **\n",
    "├   ├── session_1_results.yaml **\n",
    "├   ├── session_1_results.mp4 **\n",
    "├   ├── session_2_results.h5 **\n",
    "├   ├── session_2_results.yaml **\n",
    "├   └── session_2_results.mp4 **\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml **\n",
    "├── MoSeq2-Notebook.ipynb\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n",
    "\n",
    "__Notice your index file has also been generated in your base directory.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View all of your extracted videos below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Video\n",
    "\n",
    "for infile in os.listdir(aggregate_results_dir):\n",
    "    if infile[-3:] == 'mp4':\n",
    "        print(infile[:-4])\n",
    "        display(Video(aggregate_results_dir+infile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Principal Component Analysis (PCA)</h1></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been extracted, you can now implement a Principal Component Analysis on your metadata (specifically h5 files) in order to compute the principal components of your mouse's body in order to classify its behavior in the ARHMM model.\n",
    "\n",
    "The pipeline below depicts the flow of operations to prepare your data for the ARHMM Modeling step.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dattalab/moseq2-app/jupyter/media/PCA_Pipeline.png?token=ACRN4H7IOJUK6RD7AB3NATK5T6EJA\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will find your extracted metadata and use it to train a PCA model, and save it to your desired directory for later use.\n",
    "\n",
    "__A good example of what you should expect from your PCA Components and Scree plot are shown below:__\n",
    "\n",
    "<center>Components</center> | <center>Scree Plot</center>\n",
    "- | - \n",
    "<img src=\"https://raw.githubusercontent.com/dattalab/moseq2-app/jupyter/media/Components_Ex.png?token=ACRN4H7DJOXB3CIJL5DBHOK5T6EKA\" width=400 height=400> | <img src=\"https://raw.githubusercontent.com/dattalab/moseq2-app/jupyter/media/Scree_Ex.png?token=ACRN4H62HBD5WPYABA7AQC25T6ELI\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurable Parameter Descriptions\n",
    "- __GAUSSFILTER_SPACE__: Kernel size for performing a gaussian filter on your processed mouse video before performing PCA. This helps identify crisper, more informative principal components.\n",
    "- __MEDFILTER_SPACE__: Same as gauss filter kernel but uses Median Filtering instead. (Typically use one or the other)\n",
    "- __MISSING_DATA__: If you have missing/dropped frames in your videos, set this to true.\n",
    "- __MISSING_DATA_ITERS__: Number of times to iterate over missing data during PCA to fill in missing gaps appropriately.\n",
    "- __RECON_PCS__: Number of principal components to reconstruct from missing data.\n",
    "\n",
    "### Possible PCA Pathologies\n",
    "- __GAUSSFILTER_SPACE__ & __MEDFILTER_SPACE__: Used for when the principal components do not appear to have crisp boundaries, or are all too similar to each other to be considered reliable components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_pca.gui import train_pca_command\n",
    "\n",
    "pca_filename = 'pca' # Name of your PCA model h5 file to be saved\n",
    "pca_dirname = '_pca/' # Directory to save your computed PCA results\n",
    "\n",
    "with open(config_filepath, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "f.close()\n",
    "\n",
    "# PCA parameters you may need to configure\n",
    "config_data['gaussfilter_space'] = (1.5, 1) # Spatial filter for data (Gaussian)\n",
    "config_data['medfilter_space'] = [0] # Median spatial filter\n",
    "config_data['recon_pcs'] = 10 # Number of PCs to use for missing data reconstruction\n",
    "config_data['missing_data'] = True # Use missing data PCA\n",
    "config_data['missing_data_iters'] = 10 # Number of times to iterate over missing data during PCA\n",
    "\n",
    "\n",
    "with open(config_filepath, 'w') as f:\n",
    "    yaml.dump(config_data, f, Dumper=yaml.RoundTripDumper)\n",
    "f.close()\n",
    "\n",
    "train_pca_command(base_dir, config_filepath, pca_dirname, pca_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, you can expect your relative directory structure to look something like this:\n",
    "```\n",
    ".\n",
    "├── _pca/ **\n",
    "├   ├── pca.h5 **\n",
    "├   ├── pca.yaml  **\n",
    "├   ├── pca_components.png **\n",
    "├   └── pca_scree.png **\n",
    "├── aggregate_results/\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── MoSeq2-Notebook.ipynb\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "\n",
    "```\n",
    "\n",
    "You can now view your `computed components` and `scree plot` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "images = [pca_dirname+'pca_components.png',pca_dirname+'pca_scree.png']\n",
    "for im in images:\n",
    "    display(Image(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Principal Component Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your PCA model has been trained, you can now apply your model using your extracted data amd computed principal components. To compute your PC Scores, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_pca.gui import apply_pca_command\n",
    "\n",
    "scores_filename = 'pca_scores' # name of the scores file to compute and save\n",
    "\n",
    "apply_pca_command(base_dir, config_filepath, pca_dirname, scores_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, you will have a pca_scores file saved in your pca directory. (Example shown below)\n",
    "```\n",
    ".\n",
    "├── _pca/\n",
    "├   ├── pca.h5\n",
    "├   ├── pca.yaml\n",
    "├   ├── pca_scores.h5  **\n",
    "├   ├── pca_components.png\n",
    "├   └── pca_scree.png\n",
    "├── aggregate_results/\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── MoSeq2-Notebook.ipynb\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Computing Model-Free Syllable Changepoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an optional step used to aid in determining model-free syllable lengths; which are general approximations of the duration of respective body language syllables. Computing Model-Free Changepoints can be useful for determining the prior variable for syllable duration, denoted as `kappa`, in the ARHMM modeling step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A good example of a Changepoints Distance plot is shown below__\n",
    "<img src=\"https://raw.githubusercontent.com/dattalab/moseq2-app/jupyter/media/CP_Ex.png?token=ACRN4HYHGNQVJMBDZQFQ4BK5T6EM4\" width=400 height=400>\n",
    "\n",
    "\n",
    "Measure syllable block duration distances between detected syllables using your PCA model or computed scores below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Warning: These parameters have been hard-coded to accomodate for C57 Mice, and those of the like. Therefore, we do not recommend changing the changepoint calculation parameters. However, if you decide to do so, it is at your own risk.__\n",
    "\n",
    "### Configurable Parameter Descriptions\n",
    "- __THRESHOLD__: Computed value used to determine the \"peak\"/transition point from one syllable to the other\n",
    "- __DIMS__: Number of random projections to use in order to compare the computed principal components with, and determine a distribution for the block durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_pca.gui import compute_changepoints_command\n",
    "import ruamel.yaml as yaml\n",
    "changepoints_filename = 'changepoints' # name of the changepoints images to generate\n",
    "\n",
    "with open(config_filepath, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "f.close()\n",
    "\n",
    "# Changepoint computation parameters you may want to configure\n",
    "config_data['threshold'] = 0.5 # Peak threshold to use for changepoints\n",
    "config_data['dims'] = 300 # Number of random projections to use\n",
    "\n",
    "with open(config_filepath, 'w') as f:\n",
    "    yaml.dump(config_data, f, Dumper=yaml.RoundTripDumper)\n",
    "f.close()\n",
    "\n",
    "compute_changepoints_command(base_dir, config_filepath, pca_dirname, changepoints_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changepoints plot will be generated and saved in the pca directory (example below).\n",
    "\n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├   ├── pca.h5\n",
    "├   ├── pca_scores.h5\n",
    "├   ...\n",
    "├   └── changepoints_dist.png **\n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── MoSeq2-Notebook.ipynb\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View your changepoints distance plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(pca_dirname+changepoints_filename+'_dist.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>ARHMM Modeling</h1></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train your ARHMM (Auto-Regressive Hidden Markov Model), you will use your computed PC scores as your input data, and specify whether you are modeling a single experimental group for observational research, or modeling multiple different groups (e.g. control vs. experimental groups) for comparative analysis.\n",
    "\n",
    "The pipeline below shows the flow of operations in order to train your ARHMM.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dattalab/moseq2-app/jupyter/media/Model_Pipeline.png?token=ACRN4H3AFJJ3MZIMRL3LA7C5T6EOK\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Specify Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoSeq using groups in the `moseq2-index.yaml` file to indicate whether your collected sessions are representing a single experimental group, or many different groups that you would like to compare while modeling and visuslizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, all the session recordings have the same group title: `'default'`. If you do not have 2 sessions that are different enough to separate to different groups for later comparison, you can skip this step.\n",
    "\n",
    "Otherwise, there are 3 ways you are able to specify your groups:\n",
    "1. Specify group by SessionName\n",
    "2. Specify group by SubjectName\n",
    "3. Manually edit index file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Indexed Sessions\n",
    "Use this cell to view your sessions' information regarding their SessionNames, SubjectNames, and Groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import get_groups_command\n",
    "\n",
    "index_filepath = base_dir+'moseq2-index.yaml'\n",
    "\n",
    "get_groups_command(index_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Specify Group by Session Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import add_group_command\n",
    "\n",
    "value = 'wednesday' # value of the corresponding key\n",
    "group = 'group1' # designated group name\n",
    "exact = False # Must be exact key-value match\n",
    "lowercase = False # change to lowercase\n",
    "negative = False # select opposite selection than key-value pair given\n",
    "\n",
    "add_group_by_session(index_filepath, value, group, exact, lowercase, negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Specify Group by Subject Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import add_group_command\n",
    "\n",
    "value = 'mouse1' # value of the corresponding key\n",
    "group = 'group1' # designated group name\n",
    "exact = False # Must be exact key-value match\n",
    "lowercase = False # change to lowercase\n",
    "negative = False # select opposite selection than key-value pair given\n",
    "\n",
    "add_group_by_subject(index_filepath, value, group, exact, lowercase, negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Manually Edit Index File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply navigate to your `moseq2-index.yaml` file in your jupyter notebook homepage and edit the group names to your desired values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ARHMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurable Parameter Descriptions\n",
    "- __HOLD_OUT__: Boolean for whether to hold out data during the training process.\n",
    "- __HOLD_OUT_SEED__: Integer used to reproduce the same hold out set for repeated testing.\n",
    "- __NFOLDS__: Number of data folds to hold out during training. (If used, nfolds <= nsessions)\n",
    "- __NPCS__: Number of selected principal components, chosen in order as shown in the PC Components plot.\n",
    "- __NUM_INTER__: Number of time the model will iterate over your dataset, we recommend at least 100 starting out.\n",
    "- __MAX_STATES__: Maximum number of states the ARHMM that the ARHMM can end up with at the end of training. \n",
    "- __SEPARATE_TRANS__: Boolean for whether to separate the modeling process for different groups. (Must set to true if number of unique groups > 1)\n",
    "- __KAPPA__: Prior probability variable used to indicate average syllable length. Setting kappa to the number of frames is a good starting point to determining the proper expressed syllable durations.\n",
    "- __CHECKPOINT_FREQ__: Value indicating when to save model checkpoints per number of iterations passed. (If -1, do not checkpoint)\n",
    "\n",
    "### Possible ARHMM Pathologies\n",
    "- __KAPPA__: If kappa is too low, syllables will appear to be too short, and vice versa.\n",
    "- __NPCS__: If too few or too many PCs are selected, the ARHMM predictions will become unreliable.\n",
    "- __NUM_INTER__: This is modeling regularization parameter to ensure that your model is fitting appropriately to its given dataset.\n",
    "- __MAX_STATES__: This is modeling regularization parameter that indicates the complexity of the transitions that may be happening in your dataset. Therefore, if there are too few the model may not learn the actual behavior, and if there are too many, then the model will overfit to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_model.gui import learn_model_command\n",
    "import os\n",
    "scores_file = pca_dirname+scores_filename+'.h5' # path to input PC scores file to model\n",
    "model_path = base_dir+'model.p' # path to save trained model\n",
    "index_filepath = base_dir+'moseq2-index.yaml' # path to your auto-generated (possibly modified) index file\n",
    "\n",
    "# Advanced modeling parameters\n",
    "hold_out = False # boolean to hold out data during the training process\n",
    "hold_out_seed = -1 # integer to standardize the held out folds during training\n",
    "nfolds = 5 # number of folds to hold out during training (if hold_out==True)\n",
    "npcs = 10  # number of PCs being used\n",
    "\n",
    "num_iter = 50 # number of iterations to train model\n",
    "max_states = 100 # number of maximum states the ARHMM can end up with\n",
    "kappa = 100000 # syllable length prior\n",
    "robust = False # use robust-ARHMM with t-distribution\n",
    "\n",
    "separate_trans = False # separate group transition graphs; set to True if ngroups > 1\n",
    "\n",
    "checkpoint_freq = -1 # model saving freqency (in interations)\n",
    "\n",
    "#OMIT THESE TWO\n",
    "gamma = 1e3 # Weight value on syllables with higher number of usages\n",
    "alpha = 5.7 # Transition probability rate\n",
    "\n",
    "learn_model_command(scores_file, model_path, config_filepath, index_filepath, hold_out, nfolds,\n",
    "                    num_iter, max_states, npcs, kappa, gamma, alpha, \n",
    "                    separate_trans, robust, checkpoint_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, your model will be saved in your base directory (shown below) and you are ready to use the moseq2-viz module to produce crowd videos and a number of statistical analysis plots.\n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── model.p **\n",
    "├── moseq2-index.yaml/\n",
    "├── MoSeq2-Notebook.ipynb\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Visualize Analysis Results</h1></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a trained ARHMM, you can use it generate informative graphs and videos regarding the behavior syllables found, their usage frequency, and transition probabilities.\n",
    "\n",
    "The graph below shows the 4 operations that the MoSeq2-Viz module currently affords. They can also be computed in any order at this point in the notebook.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dattalab/moseq2-app/jupyter/media/Viz_Pipeline.png?token=ACRN4H7NCTTVXJ6ULHZLAVK5T6EP2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Crowd Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool allows you to create videos containing many overlayed clips of the mouse performing the same specified syllable at the moment a red dot appears on their body. The videos are sorted by most frequently expressed syllable to least.\n",
    "To create the crowd videos, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import make_crowd_movies_command\n",
    "\n",
    "crowd_dir = base_dir+'crowd_movies/' # output directory to save all movies in\n",
    "\n",
    "max_syllables, max_examples = 10, 10 # maximum number of syllables, and examples of each syllable in a video respectively\n",
    "\n",
    "make_crowd_movies_command(index_filepath, model_path, config_filepath, crowd_dir, max_syllables, max_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once completed, you can find your crowd movies along with a metadata YAML file in your corresponding crowd directory. The metadata `info.yaml` file will contain model information pertaining to how these crowd videos were produced.\n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── crowd_movies/ **\n",
    "├   ├── info.yaml **\n",
    "├   ├── syllable_sorted_44 (usage).mp4 **\n",
    "├   ...\n",
    "├   └── syllable_sorted_12 (usage).mp4 **\n",
    "├── model.p \n",
    "├── moseq2-index.yaml\n",
    "├── MoSeq2-Notebook.ipynb\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View your generated crowd movies below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Video\n",
    "\n",
    "for infile in os.listdir(crowd_dir):\n",
    "    if infile[-3:] == 'mp4':\n",
    "        print(infile[:-4])\n",
    "        display(Video(crowd_dir+infile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Usage Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this command to compute the model-detected syllables usages sorted in descending order of usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import plot_usages_command\n",
    "\n",
    "sort = True\n",
    "count = 'usage'\n",
    "max_used_syllable = max_syllables - 1 \n",
    "group = ''\n",
    "output_file = 'usages'\n",
    "\n",
    "plot_usages_command(index_filepath, model_path, sort, count, max_syllable, group, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Usage Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image('usages.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Scalar Summary and Tracking Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following command to compute some scalar summary information about your modeled groups, such as average velocity, height, etc.\n",
    "This command will also generate a tracking summary plot; depicting the path traveled by the mouse in your recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import plot_scalar_summary_command\n",
    "\n",
    "output_file = 'scalars' # prefix name of the saved scalar position and summary graphs\n",
    "\n",
    "plot_scalar_summary_command(index_filepath, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image('scalars_summary.png'))\n",
    "display(Image('scalars_position.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Syllable Transition Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following command to generate a syllable transition graph. The graph will be comprised of nodes labelled by syllable, and edges depicting a probable transition, with edge thickness depicting the weight of the transition edge.\n",
    "\n",
    "For multiple groups, there will be a transition graph for each group, as well as a unified graph with different colors to identify the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_viz.gui import plot_transition_graph_command\n",
    "\n",
    "max_syllable = 40 # Maximum number of nodes in the transition graph\n",
    "group = '' # Group to graph, default if empty str\n",
    "output_filename = 'transition' # name of the png file to be saved\n",
    "\n",
    "plot_transition_graph_command(index_filepath, model_path, config_filepath, max_syllable, group, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot your syllable transition graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image('transition.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Notebook End</h1></center>\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
