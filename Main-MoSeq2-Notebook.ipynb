{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moseq2 App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1jCpb4AQKBasne2SoXw_S1kwU7R9esAF6\">\n",
    "\n",
    "MoSeq2 software toolkit for unsupervised modeling and characterization of animal behavior. Moseq transforms depth recordings of animals into a rich description of behavior as a series of reused and stereotyped motifs, also known as 'syllables'. \n",
    "\n",
    "This notebook begins with depth recordings (see the [data acquisition overview](#Data-Acquisition-Overview) below) and transforms this data through the steps of: \n",
    "\n",
    "- **[Extraction](#Raw-Data-Extraction)**: The animal is segmented from the background and its position and heading direction are aligned across frames.\n",
    "- **[Dimensionality reduction](#Principal-Component-Analysis-(PCA))**: Raw video is de-noised and transformed to low-dimensional pose trajectories using principal component analysis (PCA).\n",
    "- **[Model training](#ARHMM-Modeling)**: Pose trajectories are modeled using an autoregressive hidden Markov model (AR-HMM), producing a sequence of syllable labels.\n",
    "- **[Analysis](#Visualize-Analysis-Results)**: Model output is reported through visualization and statistical analysis.\n",
    "\n",
    "These are notebook shortcuts you can click on to navigate to that part of the notebook.\n",
    "\n",
    "### Resources\n",
    "We've provided links to the MoSeq documentation and recent publications that have used this software.\n",
    "- Documentation of all MoSeq functions (links to pdfs):\n",
    "    - [Wiki](https://github.com/dattalab/moseq2-app/wiki)\n",
    "    - [App](https://github.com/dattalab/moseq2-app/blob/release/Documentation.pdf)\n",
    "    - [Extract](https://github.com/dattalab/moseq2-extract/blob/release/Documentation.pdf)\n",
    "    - [PCA](https://github.com/dattalab/moseq2-pca/blob/release/Documentation.pdf)\n",
    "    - [Model](https://github.com/dattalab/moseq2-model/blob/release/Documentation.pdf)\n",
    "    - [Viz](https://github.com/dattalab/moseq2-viz/blob/release/Documentation.pdf)\n",
    "- Publications\n",
    "    - [Mapping Sub-Second Structure in Mouse Behavior](http://datta.hms.harvard.edu/wp-content/uploads/2018/01/pub_23.pdf)\n",
    "    - [The Striatum Organizes 3D Behavior via Moment-to-Moment Action Selection](http://datta.hms.harvard.edu/wp-content/uploads/2019/06/Markowitz.final_.pdf)\n",
    "    - [Revealing the structure of pharmacobehavioral space through motion sequencing](https://www.nature.com/articles/s41593-020-00706-3)\n",
    "    - [Q&A: Understanding the composition of behavior](http://datta.hms.harvard.edu/wp-content/uploads/2019/06/Datta-QA.pdf)\n",
    "    \n",
    "### Feedback\n",
    "\n",
    "If you would like to leave us feedback on how you liked or disliked this notebook,\n",
    "or if you want specific and new features, please fill out [this survey](https://forms.gle/FbtEN8E382y8jF3p6).\n",
    "    \n",
    "### Data Acquisition Overview\n",
    "MoSeq2 takes animal depth recordings as input. We we have developed a [data acquisition pipeline](https://github.com/dattalab/moseq2-app/wiki/Setup:-acquisition-software) for the second generation `Xbox Kinect` depth camera. We suggest following our [data acquisition tutorial](https://github.com/dattalab/moseq2-app/wiki/Acquisition) for doing recordings. \n",
    "\n",
    "MoSeq2 also accepts depth recordings from an `Azure Kinect` camera as well as the `Intel RealSense` depth camera. These cameras have their own means of acquiring data that is built-in to their respective Development Kits.\n",
    "\n",
    "**We recommend recording more than 10 hours of depth video (~1 million frames at 30 frames per second) to ensure quality MoSeq models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Zkd0tATi8r2ENHvN8OczIrEf4K8PFmhM\">\n",
    "\n",
    "### Check to see if you're running python from the correct conda enviornment\n",
    "\n",
    "If you performed the recommended installation, you should see the `sys.executable` path point to the python path within the `moseq2-app` environment. I.e.,\n",
    "```python\n",
    "import sys\n",
    "print(sys.executable)\n",
    "# /Users/wgillis/miniconda3/envs/moseq2-app/bin/python\n",
    "```\n",
    "\n",
    "### Check if the dependencies are found\n",
    "\n",
    "Run the following cell to check if `moseq2-app` is installed in your current conda kernel. The latest working version number is `0.1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import moseq2_app\n",
    "\n",
    "print('Python path:', sys.executable)\n",
    "print('MoSeq2 app version:', moseq2_app.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data file organization\n",
    "\n",
    "The currently accepted depth data extensions are `.dat`, `.tar.gz`, `.avi` and `.mkv`. `.dat` files are generated from our kinect2 data acquisition software, `.tar.gz` files are compressed depth files using the data acquisition software, `.avi` files are compressed `.dat` files using `moseq2-extract`, and `.mkv` files are generated from Microsoft's recording software for the Azure Kinect.\n",
    "\n",
    "After performing data acquisition, store all of your recording folders in the same folder (shown below). We recommend that you save a copy of this notebook in the same folder as your data to: (1) access them in this notebook, (2) have a unique MoSeq pipeline notebook for each project, and (3) enable the videos this notebook generates to load. \n",
    "\n",
    "```\n",
    ".\n",
    "└── Data_Directory/\n",
    "    ├── Main-MoSeq2-Notebook.ipynb (running)\n",
    "    ├── session_1/ ** - the folder containing all of a single session's data\n",
    "    ├   ├── depth.dat        # depth data - the recording itself\n",
    "    ├   ├── depth_ts.txt     # timestamps - csv/txt file of the frame timestamps\n",
    "    ├   └── metadata.json    # metadata - json file that contains the rodent's info (group, subjectName, etc.)\n",
    "    ...\n",
    "    ├── session_2/ **\n",
    "    ├   ├── depth.dat\n",
    "    ├   ├── depth_ts.txt\n",
    "    └── └── metadata.json\n",
    "\n",
    "```\n",
    "\n",
    "__Note: if your data was acquired using an Azure Kinect or Intel RealSense depth camera, you will not have `depth_ts.txt` or `metadata.json` in your session directories. Before extraction you need to manually create a `metadata.json` file if you wish to identify sessions based on the session name or mouse ID.__\n",
    "\n",
    "### Notebook Progress File\n",
    "\n",
    "This notebook generates a `progress.yaml` file that stores the filepaths to data generated from this notebook. For example it will contain paths to:\n",
    "- aggregated extractions\n",
    "- PC scores of the extractions\n",
    "- model results\n",
    "\n",
    "In the case that your notebook kernel is shutdown for any reason, you can load the progress file to 'restore' your progress. The progress file does **not** track MoSeq pipeline operations that were executed outside of this notebook (for example, if you were to run PCA using the CLI). You can manually modify the paths in the progress file or loaded dictionary to record filepaths for these external operations.\n",
    "\n",
    "__To restore previously computed variables, look for the cells following the `Restore Progress Variables` label.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Progress Variables\n",
    "\n",
    "- Use this cell to load your notebook analysis progress. We recommend running this notebook from the folder where your data is located so the generated media will display properly. In that case, you can specify the `base_dir` as `./` (or the current folder).\n",
    "\n",
    "`check_progress` will print progress bars for each pipeline step in the notebook. \n",
    "- The extraction progress bar indicates total the number of extracted sessions detected in the provided `base_dir` path.\n",
    "- It prints the session names that haven't been extracted. __Note: the progress does not reflect the contents of the aggregate_results/ folder.__\n",
    "- The remainder of the progress bars are derived from reading the paths in the `progress_paths` dictionary, filling up the bar if the included paths are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import check_progress\n",
    "\n",
    "base_dir = './' # Add the path to your data folder here.\n",
    "# We recommend that you run this notebook in the same folder as your data. In that case, you don't have to change base_dir\n",
    "progress_filepath = join(base_dir, 'progress.yaml')\n",
    "\n",
    "progress_paths = check_progress(base_dir, progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Configuration Files\n",
    "\n",
    "The `config.yaml` will be used to hold all configurable parameters for all steps in the MoSeq pipeline. The parameters used will be added to this file as you progress through the notebook. You can then use it to run an identical pipeline in future analyses, or directly configure parameters from there when debugging cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_extract.gui import generate_config_command\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "config_filepath = join(base_dir, 'config.yaml')\n",
    "\n",
    "print(f'generating file in path: {config_filepath}')\n",
    "generate_config_command(config_filepath)\n",
    "progress_paths = update_progress(progress_filepath, 'config_file', config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A configuration file has been created in the base directory (depicted below).\n",
    "\n",
    "```\n",
    ".\n",
    "└── Data_Directory/\n",
    "    ├── config.yaml **\n",
    "    ├── session_1/ \n",
    "    ├   ├── depth.dat        \n",
    "    ├   ├── depth_ts.txt     \n",
    "    ├   └── metadata.json    \n",
    "    ...\n",
    "    ├── session_2/ \n",
    "    ├   ├── depth.dat\n",
    "    ├   ├── depth_ts.txt\n",
    "    └── └── metadata.json\n",
    "```\n",
    "\n",
    "### Download a Flip File\n",
    "\n",
    "MoSeq2 uses a flip classifier to guarantee that the mouse is always oriented facing east in the extractions. The flip classifiers we provide __are trained for experiments run with C57BL/6 mice using with Kinect v2 depth cameras__.\n",
    "\n",
    "If your dataset does not work with these flip classifiers, consider training your own. Click [this link](https://github.com/dattalab/moseq2-app/tree/jupyter/) to view the flip-classifier training notebooks. Once you have it trained, add the path to the `config.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import download_flip_command\n",
    "# selection=0 - large mice with fibers (default)\n",
    "# selection=1 - adult male C57s\n",
    "# selection=2 - mice with Inscopix cables\n",
    "download_flip_command(base_dir, config_filepath, selection=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Extraction\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1vH3fs0Xu5c4gyzFChzV6r_5JGi6JReW2\">\n",
    "\n",
    "The MoSeq2-Extract module is used to segment the mouse from the background and create data files for dimensionality reduction and modeling. The resulting `.h5` and `.yaml` data files stored in a `proc` subfolder created in the session's folder by default.`.mp4` videos are also generated and primarily used for quality assurance after extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive ROI Detection Tool\n",
    "\n",
    "Use this interactive tool to define the extraction parameters prior to extracting all of your data. Most of the parameters are related to detecting the region the mouse occupies (the ROI). This tool can also be used to catch possibly corrupted or inconsistent sessions, and to diagnose ROI detection/extraction errors.\n",
    "\n",
    "### Widget Guide\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1kgMrz-Py4x1k-WwyoHS95cbPRMdJuwvy\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"width: 45%;\">\n",
    "            <ol>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Click on any row in the session selector to load that session's data to the view.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    If the indicator next to the session's name is green, then the session is considered ready for extraction. A red indicator can either mean the session has not been checked yet, or its extraction parameter set is incorrect.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Adjust the Depth Range Selector to include the depth range of the detected bucket floor distance (which can be found by hovering over the Background image with your mouse). You can also manually enter slider values by clicking on the numbers.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    If the mouse seems to be cropped when at the bucket edge, increase the dilate iterations to enlarge the size of the included floor area.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Use the Rodent Height Threshold Slider to remove any noise/speckle from the bucket floor or walls. \n",
    "                    <ul>\n",
    "                        <li style=\"text-align:left; font-size:14px\">\n",
    "                            Ensure the min height parameter is small enough to only filter out floor reflections.\n",
    "                        </li>\n",
    "                        <li style=\"text-align:left; font-size:14px\">\n",
    "                            Ensure the max height parameter is large enough to include the largest possible mouse height, (i.e., when the mouse is rearing). A reasonable value is around 100 mm for Kinect v2 recordings.\n",
    "                        </li>\n",
    "                        <li style=\"text-align:left; font-size:14px\">\n",
    "                            To explore the session's mouse heights, hover over either of the bottom 2 plots to view the mouse height.\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Use the current frame selector slider to change the displayed session frame in the bottom 2 plots.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Change the frame range slider values to adjust the segments of the video to extract, then click the \"Extract Sample\" button to trigger an extraction and view the results.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Once you have found a parameter set that is satisfactory, click the \"Check All Sessions\" button to test the parameters on all of the found sessions, flagging any outliers.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    In the case that no sessions were flagged, click the \"Save Parameters\" button to save the currently displayed and configured parameters to the inputted config file, as well as the individual session config file.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Otherwise, if a session is flagged, click on the session in the Session Selector, view the text indicator for the the error details, then adjust the parameters until the session passes and save the parameters.\n",
    "                    <ul>\n",
    "                        <li style=\"text-align:left; font-size:14px\">\n",
    "                            If a session appears to have a passing ROI and extraction but is flagged, you can use the \"Mark Passing\" button to manually accept the session's parameter set. This is the case when a session's ROI area pixel count is outside of the acceptance threshold when compared with the latest passing session's pixel count.\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ol>\n",
    "            <p style=\"text-align:left; font-size:14px\">\n",
    "                    You can manually edit the slider values by clicking on the numbers.\n",
    "            </p>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img style=\"display:contents\" src=\"https://drive.google.com/uc?export=view&id=1jwLb1Tzpx0iAl89RF7z9bnI-sjFftTDh\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "__Notice: if cell seems to be running out of memory after first use, set `compute_all_bgs=False` to reduce the memory pressure.__\n",
    "\n",
    "If you are using an alternative `flip_classifier`, `crop_size`,  or would like to edit other extraction preprocessing parameters, use the following format:\n",
    "```\n",
    "# Read in the config file\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "    \n",
    "# Edit its contents\n",
    "config_data['crop_size'] = (100, 100) # new crop size\n",
    "config_data['flip_classifier'] = './alternative-flip-classifier.pkl' # updated flip classifier path\n",
    "...\n",
    "config_data['gaussfilter_space'] = [2.5, 2] # new spatial filtering kernel size\n",
    "config_data['medfilter_time'] = (3,) # new temporal filtering kernel size\n",
    "\n",
    "# Filtering out head-fixed cables?\n",
    "config_data['cable_filter_iters'] = 3 # new number of cable filtering iterations\n",
    "config_data['cable_filter_size'] = (7, 7) # new spatial filter kernel size\n",
    "\n",
    "# Write the changes back to the config file before running the interactive_roi_detector command.\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "```\n",
    "\n",
    "You can see all the parameters and their current values: `print(config_data)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import ruamel.yaml as yaml\n",
    "from moseq2_app.main import interactive_roi_detector\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "session_config_path = join(progress_paths['base_dir'], 'session_config.yaml')\n",
    "progress_paths = update_progress(progress_filepath, 'session_config', session_config_path)\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "config_data['camera_type'] = 'kinect' # 'kinect', 'azure' or 'realsense'\n",
    "config_data['crop_size'] = (80, 80)\n",
    "config_data['output_dir'] = 'proc' # the subfolder extracted data is saved to\n",
    "\n",
    "# if using azure or realsense, increase the noise_tolerance for ROI detection\n",
    "config_data['noise_tolerance'] = 30\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "interactive_roi_detector(base_dir, progress_paths, compute_all_bgs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Progress Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "base_dir = './' # User-defined absolute path\n",
    "\n",
    "progress_filepath = join(base_dir, 'progress.yaml')\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Session(s)\n",
    "\n",
    "- If `extract_all=False`, the cell will prompt you to choose whether you would like to extract individual sessions, (empty string to extract all of them). Enter your selection, and then wait for the extraction to complete to preview them.\n",
    "- If `skip_extracted=True`, the function will only search for and extract sessions that have not been previously extracted.\n",
    "\n",
    "__Note: If sessions are not listed when running the cell, ensure your selected extension matches that of your depth files.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import extract_found_sessions\n",
    "\n",
    "# include the file extensions for the depth files you would like to search for and extract.\n",
    "extensions = ['.avi', '.dat'] # .avi, .dat, and/or .mkv\n",
    "\n",
    "extract_found_sessions(base_dir, progress_paths['config_file'], extensions, extract_all=True, skip_extracted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what your directory structure should look like once the process is complete:\n",
    "\n",
    "```\n",
    ".\n",
    "├── config.yaml\n",
    "├── session_1/\n",
    "├   ...\n",
    "├   └── proc/ **\n",
    "├   ├   ├── roi.tiff\n",
    "├   ├   ...\n",
    "├   ├   ├── results_00.yaml ** (.yaml file storing extraction parameters)\n",
    "├   ├   ├── results_00.h5 ** (.h5 file storing extraction)\n",
    "├   └   └── results_00.mp4 ** (extracted video)\n",
    "└── session_2/\n",
    "├   ...\n",
    "├   └── proc/ **\n",
    "├   ├   ├── roi.tiff\n",
    "├   ├   ...\n",
    "├   ├   ├── results_00.yaml **\n",
    "├   ├   ├── results_00.h5 **\n",
    "└   └   └── results_00.mp4 **\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Extraction Validation Tests\n",
    "\n",
    "Once all the extractions are complete, run this cell to run some data validation tests. The tests will either emit an error or a warning. \n",
    "- An __error__ indicates that the session is corrupted in some way and should not be included in the following pipeline steps.\n",
    "- A __warning__ indicates some data strays farther from the mean than expected. The outlier sessions, along with the metrics computed will be displayed, allowing you to make decision on whether to include data in the subsequent steps.\n",
    "  - Depending on the experimental conditions, a warning can indicate that the session may need to be inspected prior to continuing into the PCA step. \n",
    "  - There are quite a few situations where the warnings aren't informative, such as comparing the effects of stimulating drugs to a control group. In this situation, you can ignore warnings (such as a warning related to velocity in the stimulant experiment) based on your best judgement.\n",
    "\n",
    "Error raising tests: \n",
    "- Count Dropped Frames: an error is raised if a session is missing >5% of the frames based on the timestamps, if the session's timestamp file exists.\n",
    "- Missing Mouse Check: raises an error if a mouse is missing from the video for any reason for >5% of the session's total frames.\n",
    "- Scalar Anomaly: raises an error if >5% of a session's computed scalar values are NaN.\n",
    "\n",
    "Warning raising tests:\n",
    "- Size Anomaly: Warning is raised when a mouse's captured body size is less than 2 standard deviations from the mean size throughout the session.\n",
    "- Scalar Anomaly: Warning is raised with a list of specified scalars if some of the mean scalar values are outside of the 1st through 3rd quartile range. \n",
    "- Position Anomaly: There are two cases that raise warnings:\n",
    "    1. Mouse is stationary for >5% of the session.\n",
    "    2. Mouse's position PDF is at least 2 standard deviations away from the mean of all the sessions, measured using Kullback–Leibler divergence.\n",
    "     - This anomaly can indicate that a mouse has explored a much larger or smaller region of the arena compared to the remainder of the dataset.\n",
    "\n",
    "To diagnose certain scalar anomalies, use the [Scalar Summary Cell](#Compute-Scalar-Summary) below to graph any desired scalar value.\n",
    "   \n",
    "The following cell will run the tests and emit the warnings and errors if any are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import validate_extractions\n",
    "\n",
    "validate_extractions(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Extractions\n",
    "\n",
    "Run the following cell to launch an interactive session selection widget to preview an extracted session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import preview_extractions\n",
    "\n",
    "preview_extractions(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate your results into one folder and generate an index file.\n",
    "\n",
    "The following cell will search through your base directory for the `proc/` subfolders within each session folder and copy them all in a single `aggregate_results/` folder. \n",
    "\n",
    "Then it will generate a `moseq2-index.yaml` file, consolidating the metadata for all extracted sessions in the `aggregate_results/` folder. This file contains the mappings between the experimental groups and the extracted sessions. The index file will initially assign the `default` group to each session. Later in the notebook, you can re-assign group labels to each session.\n",
    "\n",
    "The `aggregate_results/` folder contains all the data you need to run the rest of the pipeline. The PCA and modeling step will use data in this folder.\n",
    "\n",
    "__Important Note: The index file contains UUIDs to map each session to a specific extraction. These UUIDs are referenced throughout the pipeline, so if you re-extract a session and re-aggregate your data, ensure all the UUIDs in the index file are up-to-date BEFORE running the PCA step.__ Not updating the index file will likely cause `KeyError`s to occur in the PCA and modeling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "from moseq2_extract.gui import aggregate_extract_results_command\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "recording_format = '{start_time}_{session_name}_{subject_name}' # filename formats for the copied extracted data files\n",
    "\n",
    "# directory NAME to save all metadata+extracted videos to with above respective name format\n",
    "aggregate_results_dirname = 'aggregate_results/'\n",
    "\n",
    "train_data_dir = join(base_dir, aggregate_results_dirname)\n",
    "update_progress(progress_filepath, 'train_data_dir', train_data_dir)\n",
    "\n",
    "# the subpath indicates to only aggregate extracted session paths with that subpath, only change if aggregating data from a different location\n",
    "index_filepath = aggregate_extract_results_command(base_dir, recording_format, aggregate_results_dirname)\n",
    "progress_paths = update_progress(progress_filepath, 'index_file', index_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregate results folder will be saved in your base directory,\n",
    "resulting in the following directory (sample) structure where the base directory contains the notebook:\n",
    "\n",
    "```\n",
    ".\n",
    "├── aggregate_results/ **\n",
    "├   ├── session_1_results_00.h5 ** # session 1 compressed extraction + metadata \n",
    "├   ├── session_1_results_00.yaml **\n",
    "├   ├── session_1_results_00.mp4 ** # session 1 extracted video\n",
    "├   ├── session_2_results_00.h5 ** # session 2 compressed extraction + metadata \n",
    "├   ├── session_2_results_00.yaml **\n",
    "├   └── session_2_results_00.mp4 ** # session 2 extracted video\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml ** # index file\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n",
    "\n",
    "__Notice the index file has been generated in your base directory, and it's general initial structure is shown below.__\n",
    "\n",
    "```\n",
    "files:\n",
    "- group: default\n",
    "  metadata:\n",
    "      ...\n",
    "      SubjectName: control_mouse_1\n",
    "      SessionName: day1\n",
    "  path: [/path/to/results_00.h5, /path/to/results_00.yaml]\n",
    "  uuid: 11dc6c26-0de6-4145-9bcc-a9ec200b667e\n",
    "- group: default\n",
    "  metadata:\n",
    "      ...\n",
    "      SubjectName: drug_mouse_1\n",
    "      SessionName: day1\n",
    "  path: [/path/to/results_00.h5, /path/to/results_00.yaml]\n",
    "  uuid: 16d76d24-35c3-4ca8-aedc-c12456abb4c4\n",
    "...\n",
    "pca_path: ./_pca/pca_scores.h5\n",
    "```\n",
    "\n",
    "## Specify Groups\n",
    "\n",
    "MoSeq using groups in the moseq2-index.yaml file to indicate whether your collected sessions are representing a single experimental group, or many different groups that you would like to compare while modeling and visualizing.\n",
    "\n",
    "Specifying groups also helps distinguishing the data points in the scalar and heatmap plots.\n",
    "\n",
    "The index file requires that all your sessions have a metadata.json file in order to successfully assign each recorded subject or session to a group.\n",
    "\n",
    "Use this GUI to input the group names associated with all the sessions. \n",
    "- You can click on the column names to sort the index file.\n",
    "- You can use your keyboard to select multiple rows.\n",
    "- Enter your desired group name in the text input and click `Set Group` to update all the associated session rows.\n",
    "- Once all your groups are set, click the `Update Index File` button to update the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import interactive_group_setting\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "interactive_group_setting(progress_paths['index_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Scalar Summary\n",
    "\n",
    "Use the following command to compute some scalar summary information about your modeled groups, such as average velocity, height, etc.\n",
    "\n",
    "This graph is meant to give you an idea of whether your extractions were consistent throughout the sessions. If you have a large standard deviation in mouse length/width when your mice are all the same size in reality, then there may have been an error in the extraction or acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "from moseq2_viz.gui import plot_scalar_summary_command\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "# Prefix name of the saved scalar position and summary graphs\n",
    "output_file = join(progress_paths['plot_path'], 'scalars') \n",
    "\n",
    "# Scalars to display\n",
    "show_scalars = ['velocity_2d_mm', 'velocity_3d_mm',\n",
    "                'height_ave_mm', 'width_mm', 'length_mm']\n",
    "\n",
    "colors = None # None for default colors; otherwise use list\n",
    "\n",
    "scalar_df = plot_scalar_summary_command(progress_paths['index_file'], output_file, \n",
    "                                        show_scalars=show_scalars, \n",
    "                                        colors=colors)\n",
    "\n",
    "# Graph the output\n",
    "display(Image(output_file+'_summary.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Position Heatmaps For Each Session\n",
    "Each heatmap will be titled with the session's subject name and group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "from moseq2_viz.gui import plot_verbose_position_heatmaps\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'session_heatmaps') \n",
    "plot_verbose_position_heatmaps(progress_paths['index_file'], output_file)\n",
    "\n",
    "# Graph the output\n",
    "display(Image(output_file+'.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Group Mean Position Summary\n",
    "\n",
    "These plots will give you a good idea of the general captured hyperactivity level and amount of area exploration in each of your experimental groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "from moseq2_viz.gui import plot_mean_group_position_heatmaps_command\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'group_heatmaps') \n",
    "plot_mean_group_position_heatmaps_command(progress_paths['index_file'], output_file)\n",
    "\n",
    "# Graph the output\n",
    "display(Image(output_file+'.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "Once the data have been extracted, perform Principal Components Analysis (PCA) to reduce the dimensionality of the data, making it easier to model.\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1I1WcfEwzpfwIxNYStX7swLAIvjQEVApy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Progress Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "base_dir = './' # User-defined absolute path\n",
    "\n",
    "progress_filepath = join(base_dir, 'progress.yaml')\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training PCA\n",
    "\n",
    "Perform PCA on your extracted data to acquire the principal components (PCs) that explain the largest possible variance in your dataset. The PCs should look smooth and well defined like the examples below. The PCs should explain >90% of the variance in the dataset using around 10 PCs. If this isn't the case, consult the [pathologies below](#Possible-PCA-Pathologies) to solve any issues.\n",
    "\n",
    "- You can check progress while the PCA operations are taking place on the [dask server](https://localhost:8787/). You can only use this link if you're running PCA locally and is meant for optimization and debugging.\n",
    "- If there are occlusions over the rodent in your extractions (shown below), set `config_data['missing_data'] = True` to recompute using a modified PCA algorithm to handle missing data.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1y9_aRzrE3PS34GC2LJe3zuEXvms04S90\">\n",
    "\n",
    "__Using SLURM?__ Add and edit the following config parameters to spawn dask workers according to your specifications:\n",
    "```\n",
    "# Read in the config file\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "    \n",
    "# Edit its contents\n",
    "config_data['cluster_type'] = 'slurm'\n",
    "\n",
    "config_data['nworkers'] = 8 # number of spawned jobs\n",
    "config_data['queue'] = 'short' # partition\n",
    "config_data['memory'] = '40GB' # amount of memory per worker\n",
    "config_data['cores'] = 1 # number of cores per worker\n",
    "config_data['wall_time'] = '01:00:00' # worker time limit\n",
    "\n",
    "# Write the changes back to the config file before running the interactive_roi_detector command.\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import ruamel.yaml as yaml\n",
    "from moseq2_pca.gui import train_pca_command\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "pca_filename = 'pca' # Name of your PCA model h5 file to be saved\n",
    "pca_dirname = '_pca/' # Directory to save your computed PCA results\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "# PCA parameters you may need to configure\n",
    "config_data['gaussfilter_space'] = (1.5, 1) # Spatial filter for data (Gaussian)\n",
    "config_data['medfilter_space'] = [0] # Median spatial filter\n",
    "config_data['medfilter_time'] = [0] # Median temporal filter\n",
    "\n",
    "# If dataset includes head-attached cables, set missing_data=True\n",
    "config_data['missing_data'] = False # Set True for dataset with missing/dropped frames to reconstruct respective PCs.\n",
    "config_data['missing_data_iters'] = 10 # Number of times to iterate over missing data during PCA\n",
    "config_data['recon_pcs'] = 10 # Number of PCs to use for missing data reconstruction\n",
    "\n",
    "# Dask Configuration\n",
    "config_data['dask_port'] = '8787' # port to access Dask Dashboard\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'pca_dirname', join(base_dir, pca_dirname))\n",
    "\n",
    "# will train on data in aggregate_results/\n",
    "train_pca_command(progress_paths, pca_dirname, pca_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, a new directory titled `_pca` will be created containing all your PCA data.\n",
    "```\n",
    ".\n",
    "├── _pca/ **\n",
    "├   ├── pca.h5 ** # pca model compressed file\n",
    "├   ├── pca.yaml  ** # pca model YAML metadata file\n",
    "├   ├── pca_components.png **\n",
    "├   └── pca_scree.png **\n",
    "├── aggregate_results/\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "\n",
    "```\n",
    "\n",
    "View your `computed PCs` and `scree plot` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "images = [join(progress_paths['pca_dirname'], 'pca_components.png'), \n",
    "          join(progress_paths['pca_dirname'], 'pca_scree.png')]\n",
    "for im in images:\n",
    "    display(Image(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible PCA Pathologies\n",
    "\n",
    "<table style=\"width: 100%;\">\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th></th>\n",
    "      <th>Good PCA Output Examples</th>\n",
    "      <th style=\"text-align:center;\">Bad Scree Plot Example</th>\n",
    "      <th style=\"text-align:center;\">Bad Principal Components Example</th>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Pathology Description</th>\n",
    "      <th style=\"text-align:center;\"></th>\n",
    "      <td style=\"text-align:center;\">Cannot achieve a explained variance of over 90% from less than 15 Principal Components (PCs).</td>\n",
    "      <td style=\"text-align:center;\">PCs look noisy, or are not representative of realistic mouse body regions.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Reference Examples</th>\n",
    "      <th style=\"text-align:center;\">\n",
    "        <ul>\n",
    "            <li>Components<br>\n",
    "                <img src=\"https://drive.google.com/uc?export=view&id=1dX5Gpd3PKL4vfVviLeP0CqBrz9PW37Au\" width=350 height=350></li><br><br>\n",
    "            <li>Scree Plot<br>\n",
    "                <img src=\"https://drive.google.com/uc?export=view&id=12uqsBYuWCjpUQ6QrAjo35MnwYDzHqnge\" width=350 height=350>\n",
    "            <br>\"90.65% in 7 PCs\"</li>\n",
    "        </ul>\n",
    "      </th>\n",
    "      <td><img src=\"https://drive.google.com/uc?export=view&id=14OwThgsf2GXnrl3-9TXEMvF3PDxmRsHE\" width=350 height=350></td>\n",
    "      <td><img src=\"https://drive.google.com/uc?export=view&id=1d35zKWiT7bkWbNNAon_JdSjKyVgcHHzi\" width=350 height=350></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Image Analysis Solutions</th>\n",
    "      <th style=\"text-align:center;\"></th>\n",
    "      <td>\n",
    "        <ul>\n",
    "          <li style=\"text-align:left;\">Check if the crop size is too large, if so, decrease it and re-extract your data.</li>\n",
    "          <li style=\"text-align:left;\">Try (incrementally) adjusting the spatial and temporal filtering kernel sizes in the PCA step. Generally, increasing temporal smoothing will aid in increasing explained variance, but can potentially throw out data.</li>\n",
    "        </ul>\n",
    "      </td>\n",
    "      <td>\n",
    "          <ul>\n",
    "              <li style=\"text-align:left;\">Ensure that an appropriate amount of spatial and temporal filtering is applied.</li>\n",
    "              <li style=\"text-align:left;\">If you set missing_data=True, adjust spatial and temporal filtering, and try adjusting the amount of PCs used for reconstruction (the recon_pcs parameter).</li>\n",
    "          </ul>\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">General Solutions</th>\n",
    "      <th style=\"text-align:center;\"></th>\n",
    "      <td style=\"text-align:center;\">\n",
    "          <ul>\n",
    "          <li style=\"text-align:left;\">If there are cable occlusions, try setting missing_data=True. Using an iterative PCA to reconstruct the PCs can aid in increasing the explained variance ratio.</li>\n",
    "          <li style=\"text-align:left;\">Increase the size of your dataset. If your dataset is too small, it may contribute to overfit PCs.</li>\n",
    "        </ul>\n",
    "      </td> <!-- G -->\n",
    "      <td style=\"text-align:center;\">Acquire and extract more data, then try again.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Principal Component Scores\n",
    "\n",
    "Apply the computed PCs to the extractions to compute PC scores. They define a \"pose trajectory\" in PC space, and these are the values that the MoSeq model (ARHMM) will train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_pca.gui import apply_pca_command\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "scores_filename = 'pca_scores' # name of the scores file to compute and save\n",
    "\n",
    "scores_file = join(progress_paths['pca_dirname'], scores_filename + '.h5') # path to input PC scores file to model\n",
    "progress_paths = update_progress(progress_filepath, 'scores_path', scores_file)\n",
    "\n",
    "apply_pca_command(progress_paths, scores_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, you will have a pca_scores file saved in your pca directory. (Example shown below)\n",
    "```\n",
    ".\n",
    "├── _pca/\n",
    "├   ├── pca.h5\n",
    "├   ├── pca.yaml\n",
    "├   ├── pca_scores.h5  ** # scores file\n",
    "├   ├── pca_components.png\n",
    "├   └── pca_scree.png\n",
    "├── aggregate_results/\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "\n",
    "```\n",
    "\n",
    "## (Optional) Computing Model-Free Syllable Changepoints\n",
    "\n",
    "This is an optional step used to aid in determining model-free changepoint \"block\" durations, and are used as a general approximation of syllable duration. Although this step is optional, it is important to use the changepoint duration distribution to find the best MoSeq model fits. The changepoint distribution can help determine the accuracy of the PCA fit.\n",
    "\n",
    "A good changepoint graph should show a smooth left-skewed histogram distribution of changepoint durations. If that is not the case, consult the [below pathologies](#Possible-Model-Free-Changepoints-Pathologies).\n",
    "\n",
    "Ideally, the changepoint duration mode should be roughly 0.3 seconds (300 ms).\n",
    "\n",
    "__Note: the parameters below have been preconfigured for C57 mouse data, and have not been tested for other strains/species. Configure them at your own risk.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "from moseq2_pca.gui import compute_changepoints_command\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "changepoints_filename = 'changepoints' # name of the changepoints images to generate\n",
    "\n",
    "# Changepoint computation parameters you may want to configure\n",
    "config_data['threshold'] = 0.5 # Peak threshold to use for changepoints\n",
    "config_data['dims'] = 300 # Number of random projections to compare the computed principal components with\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'changepoints_path', changepoints_filename)\n",
    "compute_changepoints_command(progress_paths['train_data_dir'], progress_paths, changepoints_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changepoints plot will be generated and saved in the pca directory (example below).\n",
    "\n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├   ├── pca.h5\n",
    "├   ├── pca_scores.h5\n",
    "├   ...\n",
    "├   └── changepoints_dist.png **\n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n",
    "\n",
    "View your changepoints distance plot (if the text is too small, check the pdf file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(join(progress_paths['pca_dirname'], progress_paths['changepoints_path'] + '_dist.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Model-Free Changepoints Pathologies\n",
    "\n",
    "<table style=\"width: 100%;\">\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th></th>\n",
    "      <th style=\"text-align:center;\">Good Changepoint Analysis Example</th>\n",
    "      <th style=\"text-align:center;\">Poor Changepoints Analysis Example</th>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Pathology Description</th>\n",
    "      <td style=\"text-align:center;\"></td>\n",
    "      <td style=\"text-align:center;\">Model-free syllable changepoint distances distribution is incorrectly skewed/too sparse and/or changepoint mode duration is less than 0.2s</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Reference Example</th>\n",
    "      <td><img src=\"https://drive.google.com/uc?export=view&id=1sMkSB34bGbOimumN6Gg1-zV2Hk98v2Zy\" width=350 height=350></td>\n",
    "      <td><img src=\"https://drive.google.com/uc?export=view&id=1S-ALkPmb8sBZGkKmJ7Q3-RdxAbfS0PWV\" width=350 height=350></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">General Solutions</th>\n",
    "      <td style=\"text-align:center;\"></td>\n",
    "      <td>\n",
    "          <ul>\n",
    "              <li style=\"text-align:left;\">Try retraining the PCA with adjusted spatial and temporal filtering kernel sizes.</li>\n",
    "              <li style=\"text-align:left;\">Ensure your extracted data is correct with minimal flips. If the extraction version of the mouse is too noisy, then the PC trajectories cannot be accurately applied to the data.</li>\n",
    "              <li style=\"text-align:left;\">Get more data and try again.</li>\n",
    "          </ul>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARHMM Modeling\n",
    "\n",
    "In order to train an ARHMM (Auto-Regressive Hidden Markov Model), use the computed PC scores as input and specify whether you are modeling a single experimental group or modeling multiple different groups (e.g. control vs. experimental groups) for comparative analysis.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=15uoeWDTn8fbcau2jErJEVFUt2iJyadmo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Progress Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "base_dir = './' # User-defined absolute path\n",
    "\n",
    "progress_filepath = join(base_dir, 'progress.yaml')\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ARHMM\n",
    "\n",
    "__Note: if loading a model checkpoint, ensure the modeling parameters (especially the selected groups) are identical to that of the checkpoint. Otherwise the model will fail.__\n",
    "\n",
    "In order to fit an ARHMM to your data, you should perform a scan over a single hyperparameter: `kappa`. In a nutshell, `kappa` sets the syllable duration distribution. A higher `kappa` leads to longer syllable durations. The goal is to find a kappa that best matches the changepoint duration distribution. In order to save time, run these models for ~100-200 iterations, which will give you enough information to select the best `kappa`.\n",
    "\n",
    "Once you have an optimal `kappa`, run a final model with the desired `kappa` value for ~1000 iterations to \"bake-in\" in the model parameters.\n",
    "\n",
    "If you want to test out a model to get a rough idea if the data is modelable without going through a `kappa` scan, you can set the `kappa` variable below to `None`, and `kappa` will be set to the total number of recorded frames in your dataset. We've found this default is a reasonable heuristic. If `kappa` is set to `'scan'`, the code will perform a hyperparameter scan over an array of candidate values. Be warned — these scans take quite a while to perform, especially on a local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import ruamel.yaml as yaml\n",
    "from moseq2_model.gui import learn_model_command\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "modeling_session_path = 'saline-amphetamine/'\n",
    "model_name = 'model.p'\n",
    "\n",
    "session_path = join(base_dir, modeling_session_path)\n",
    "model_path = join(session_path, model_name) # path to save trained model\n",
    "\n",
    "select_groups = False # select specific groups to model; if False, will model all data as is in moseq2-index.yaml\n",
    "\n",
    "# model saving freqency (in interations); will create a checkpoints/ directory containing checkpointed models\n",
    "checkpoint_freq = -1\n",
    "use_checkpoint = False # resume training from latest saved checkpoint\n",
    "\n",
    "# Advanced modeling parameters\n",
    "hold_out = False # boolean to hold out data subset during the training process\n",
    "nfolds = 2 # (if hold_out==True): number of folds to hold out during training; 1 fold per session\n",
    "\n",
    "npcs = 10  # number of PCs being used\n",
    "max_states = 100 # number of maximum states the ARHMM can end up with\n",
    "\n",
    "# use robust-ARHMM with t-distribution -> able to tolerate more noise\n",
    "robust = True \n",
    "\n",
    "# separate group transition graphs; set to True if you want to compare multiple groups\n",
    "separate_trans = True \n",
    "\n",
    "num_iter = 100 # number of iterations to train model\n",
    "\n",
    "# syllable length probability distribution prior; (None, int or 'scan'); if None, kappa=nframes\n",
    "kappa = None \n",
    "\n",
    "# if kappa == 'scan', optionally set bounds to scan kappa values between, in either a linear or log-scale.\n",
    "scan_scale = 'log' # or linear\n",
    "min_kappa = None\n",
    "max_kappa = None\n",
    "\n",
    "# total number of models to spool\n",
    "n_models = 15\n",
    "\n",
    "# Select platform to run models on\n",
    "cluster_type = 'local' # currently supported cluster_types = 'local' or 'slurm'\n",
    "run_cmd = False # if True, runs the commands via os.system(...)\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'model_path', model_path)\n",
    "progress_paths = update_progress(progress_filepath, 'model_session_path', session_path)\n",
    "\n",
    "learn_model_command(progress_paths, hold_out=hold_out, nfolds=nfolds, num_iter=num_iter, max_states=max_states,\n",
    "                    npcs=npcs, kappa=kappa, separate_trans=separate_trans, robust=robust,\n",
    "                    checkpoint_freq=checkpoint_freq, use_checkpoint=use_checkpoint, select_groups=select_groups,\n",
    "                    cluster_type=cluster_type, min_kappa=min_kappa, scan_scale=scan_scale,\n",
    "                    max_kappa=max_kappa, n_models=n_models, run_cmd=run_cmd, output_dir=modeling_session_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, your model will be saved in your base directory (shown below). \n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── modeling_session/ ***\n",
    "├   └── model.p ***\n",
    "├── moseq2-index.yaml/\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Notebook Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "base_dir = './' # User-defined absolute path\n",
    "\n",
    "progress_filepath = join(base_dir, 'progress.yaml')\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Best Model Fit\n",
    "\n",
    "Use this feature to determine whether the trained model has captured median syllable durations that match the principal components changepoints.\n",
    "\n",
    "This feature can also return the best model from a list of models found in the `progress_paths['model_session_path']`.\n",
    "\n",
    "Below are examples of some comparative distributions that you can expect when using this tool:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img height=400 width=400 src=\"https://drive.google.com/uc?export=view&id=1ENQVOFcM7moN_k6G_hVysIAaH-smRnEd\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img height=400 width=400 src=\"https://drive.google.com/uc?export=view&id=1rtfzkBGISuu8fpGNLNOTt9881Hgg_rXC\">\n",
    "        </td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "from moseq2_viz.gui import get_best_fit_model\n",
    "from moseq2_app.gui.progress import update_progress, restore_progress_vars\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'model_vs_pc_changepoints')\n",
    "\n",
    "best_model_fit = get_best_fit_model(progress_paths, plot_all=True)\n",
    "progress_paths = update_progress(progress_filepath, 'model_path', best_model_fit)\n",
    "display(Image(output_file + '.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End and User Survey\n",
    "\n",
    "Please take some time to tell us your thoughts about this notebook:\n",
    "**[user feedback survey](https://forms.gle/FbtEN8E382y8jF3p6)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.993px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
